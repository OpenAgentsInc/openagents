---
id: d-004
title: Continual Constant Improvement of Autopilot
status: active
priority: urgent
created: 2025-12-20
updated: 2026-01-10T17:00:00Z
---

## Goal

Build a self-improving autopilot system that continuously measures, analyzes, and optimizes its own performance across all dimensions. Every run should contribute to making future runs better.

## Background

Autopilot is the autonomous agent that powers OpenAgents development. It runs Claude Code in autonomous loops, claiming issues, implementing solutions, and committing code. To date, autopilot has accumulated hundreds of trajectory logs in `docs/logs/` representing thousands of tool calls and decisions.

These trajectories are a goldmine of improvement signals:
- Which patterns lead to successful task completion?
- What causes tool errors and how can they be prevented?
- Where is time/tokens being wasted?
- Which instructions are being ignored?

Currently this data sits unused. This directive establishes infrastructure for **continual constant improvement** - a flywheel where every autopilot run feeds back into making autopilot better.

Reference: `docs/autopilot/IMPROVEMENT-DIMENSIONS.md` defines 50+ measurable dimensions across 10 categories.

## Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│                    CONTINUAL IMPROVEMENT LOOP                        │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│   ┌──────────┐     ┌──────────┐     ┌──────────┐     ┌──────────┐  │
│   │ AUTOPILOT│     │ COLLECT  │     │ ANALYZE  │     │  APPLY   │  │
│   │   RUN    │────►│ METRICS  │────►│ PATTERNS │────►│ LEARNING │  │
│   └──────────┘     └──────────┘     └──────────┘     └──────────┘  │
│        ▲                                                    │       │
│        └────────────────────────────────────────────────────┘       │
│                                                                      │
│   Per-Run:                                                          │
│   • Trajectory logged (.rlog + .json)                               │
│   • Metrics extracted and stored                                    │
│   • Anomalies flagged for review                                    │
│                                                                      │
│   Aggregate:                                                        │
│   • Trend analysis across runs                                      │
│   • Regression detection                                            │
│   • Improvement opportunities ranked                                │
│                                                                      │
│   Applied:                                                          │
│   • System prompt refinements                                       │
│   • Hook improvements                                               │
│   • New guardrails                                                  │
│   • Issue creation for problems                                     │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### Data Flow

```
docs/logs/YYYYMMDD/*.rlog     →  metrics.db (SQLite)
docs/logs/YYYYMMDD/*.json     →  │
                                  │
                                  ▼
                           ┌─────────────┐
                           │  Analyzer   │
                           │  Pipeline   │
                           └─────────────┘
                                  │
                    ┌─────────────┴─────────────┐
                    ▼                           ▼
            ┌─────────────┐             ┌─────────────┐
            │   Reports   │             │   Issues    │
            │ (dashboard) │             │ (autopilot) │
            └─────────────┘             └─────────────┘
```

## Success Criteria

### Phase 1: Metrics Collection Infrastructure
- [x] Create `crates/autopilot/src/metrics.rs` module
- [x] Define MetricEvent struct capturing all 50+ dimensions
- [x] Add metrics.db SQLite database for persistent storage
- [ ] Emit metrics at end of each autopilot run
- [ ] Backfill metrics from existing trajectory logs
- [x] CLI command: `cargo autopilot metrics import <log-dir>`

### Phase 2: Real-Time Analysis Hooks
- [ ] PostRun hook to automatically extract metrics
- [ ] Detect anomalies (>2 std dev from baseline)
- [ ] Flag sessions with high error rates for review
- [ ] Track metric trends per-issue, per-directive
- [x] CLI command: `cargo autopilot metrics show <session-id>`

### Phase 3: Aggregate Analysis
- [x] CLI command: `cargo autopilot analyze` - comprehensive report
- [x] Calculate baselines for each metric dimension
- [ ] Detect regressions (metric worse than baseline)
- [ ] Identify improvement opportunities (high-impact, low-effort)
- [ ] Weekly trend reports (automated)
- [ ] CLI command: `cargo autopilot analyze --compare <date1> <date2>`

### Phase 4: Automated Issue Creation
- [ ] When a pattern of failures is detected, auto-create improvement issue
- [ ] Issue includes: evidence from trajectories, proposed fix, priority
- [ ] Link improvement issues to d-004 directive
- [ ] Track which improvements came from automated detection
- [ ] Example: "Tool error rate >10% this week - investigate Read tool EISDIR errors"

### Phase 5: Learning Application
- [ ] Refine compaction instructions based on what context is lost
- [ ] Tune model selection rules based on task complexity outcomes
- [ ] Update system prompts based on instruction adherence failures
- [ ] Improve guardrails based on safety violations
- [x] Document learnings in `docs/autopilot/LEARNINGS.md`

### Phase 6: Dashboard & Visibility
- [x] WGPUI metrics dashboard pane (Autopilot Control Room)
- [x] Real-time session monitoring
- [x] Historical trend visualization
- [x] Drill-down from aggregate to individual sessions
- [x] **DSPy callbacks for real-time LLM visibility** (2026-01-10) - via d-009 Coder integration
- [ ] Alert configuration for critical regressions

### Phase 7: Benchmark Suite
- [x] Define standard benchmark tasks (known solutions)
- [ ] Run benchmarks on each autopilot code change
- [ ] Compare performance across versions
- [ ] Gate releases on benchmark regressions
- [x] CLI command: `cargo autopilot benchmark`

### Phase 8: Self-Improvement Automation
- [ ] Autopilot can propose CLAUDE.md updates based on patterns
- [ ] Autopilot can propose hook modifications
- [ ] Autopilot can tune its own parameters (with human approval)
- [ ] Track improvement velocity over time
- [ ] Celebrate wins: log when metrics improve

### Phase 9: Repo Onboarding (Launch Priority)

Connect external GitHub repos to Autopilot for the demo/funnel flow.

- [ ] GitHub OAuth flow for repo connection
- [ ] Repository picker (user's accessible repos)
- [ ] Validate required permissions (read code, write PRs, read/write issues)
- [ ] Auto-detect language and tooling (package.json, Cargo.toml, etc.)
- [ ] First-run health check (clone, detect tests, run sample)
- [ ] Store connected repo metadata in local DB
- [ ] CLI: `openagents autopilot connect <repo-url>`

### Phase 10: GitHub Issue → PR Flow (Launch Priority)

Full GitHub workflow from issue to merged PR.

**Issue Claiming:**
- [ ] Detect issues labeled for autopilot (e.g., `autopilot`, `good-first-issue`)
- [ ] Post "claiming" comment with agent identity
- [ ] Add `in-progress` label
- [ ] Update issue assignment

**Branch + Commits:**
- [ ] Create feature branch: `autopilot/<issue-number>-<slug>`
- [ ] Consistent commit message format: `[#123] <type>: <description>`
- [ ] Include co-author footer in commits
- [ ] Push to remote after each logical change

**Pull Request:**
- [ ] Open PR with structured template
- [ ] Link to original issue
- [ ] Include receipts link (tests run, costs)
- [ ] Post replay link as PR comment
- [ ] Handle review feedback (re-run on request changes)

### Phase 11: CI Integration (Launch Priority)

Integrate with repository CI/CD for verified completions.

- [ ] Detect CI system (GitHub Actions, CircleCI, etc.)
- [ ] Run local test suite before opening PR
- [ ] Poll CI status after PR opened
- [ ] Auto-fix CI failures (up to N attempts)
- [ ] Stop and escalate after N consecutive failures
- [ ] Include CI status in receipts

### Phase 12: Replay Artifacts (Launch Priority)

Produce publishable replay bundles for demos.

- [ ] Generate Run Bundle on session completion:
  - rlog (full trajectory)
  - metadata (timing, costs, model)
  - diffs (files changed)
  - receipts (test results, CI status)
- [ ] Deterministic timeline for replay scrubbing
- [ ] Sign bundle with agent identity
- [ ] Redact secrets before external publish
- [ ] Store bundles locally with retention policy
- [ ] CLI: `openagents autopilot export <session-id>`

### Phase 13: Notifications & Escalation

Keep humans informed when needed.

- [ ] Desktop notifications for completed runs
- [ ] Email summary for overnight runs
- [ ] Escalation on repeated failures
- [ ] Configurable notification preferences
- [ ] Slack/Discord webhook integration

## Key Metrics (from IMPROVEMENT-DIMENSIONS.md)

### Priority 0 (Critical)
| Metric | Current | Target | Notes |
|--------|---------|--------|-------|
| Tool Error Rate | ~15% | <5% | EISDIR, file not read, invalid paths |
| Task Completion Rate | ~80% | >95% | Issues claimed vs completed |
| Unsafe Operation Prevention | ~90% | 100% | sqlite3 writes, git force push |

### Priority 1 (High)
| Metric | Current | Target | Notes |
|--------|---------|--------|-------|
| Log Completeness | ~85% | 100% | Missing session_id, token counts |
| Read-Before-Edit Enforcement | ~90% | 100% | "File has not been read" errors |
| Git Workflow Adherence | ~70% | 100% | Branch → PR → merge pattern |
| Cache Hit Rate | ~95% | >90% | Already good, maintain |

### Priority 2 (Medium)
| Metric | Current | Target | Notes |
|--------|---------|--------|-------|
| Parallelization Rate | ~60% | >80% | Independent tool calls in parallel |
| First-Try Success Rate | ~70% | >80% | Tasks completed without prompt refinement |
| Tokens Per Task | varies | -10% QoQ | Efficiency improvement target |

## Key Files to Create/Modify

| File | Purpose |
|------|---------|
| `crates/autopilot/src/metrics.rs` | **NEW** - Metrics collection and storage |
| `crates/autopilot/src/analyze.rs` | **NEW** - Analysis pipeline |
| `crates/autopilot/src/benchmark.rs` | **NEW** - Benchmark runner |
| `crates/autopilot-gui/src/views/dashboard.rs` | **NEW** - Metrics dashboard pane |
| `crates/autopilot/src/main.rs` | Add CLI commands |
| `autopilot-metrics.db` | **NEW** - Metrics database |
| `docs/autopilot/LEARNINGS.md` | **NEW** - Documented improvements |
| `docs/autopilot/BASELINES.md` | **NEW** - Baseline metrics per version |

## Database Schema

```sql
-- Session-level metrics
CREATE TABLE sessions (
    id TEXT PRIMARY KEY,
    timestamp DATETIME,
    model TEXT,
    prompt TEXT,
    duration_seconds REAL,
    tokens_in INTEGER,
    tokens_out INTEGER,
    tokens_cached INTEGER,
    cost_usd REAL,
    issues_claimed INTEGER,
    issues_completed INTEGER,
    tool_calls INTEGER,
    tool_errors INTEGER,
    final_status TEXT  -- completed, crashed, budget_exhausted
);

-- Per-tool-call metrics
CREATE TABLE tool_calls (
    id INTEGER PRIMARY KEY,
    session_id TEXT REFERENCES sessions(id),
    timestamp DATETIME,
    tool_name TEXT,
    duration_ms INTEGER,
    success BOOLEAN,
    error_type TEXT,  -- NULL if success
    tokens_in INTEGER,
    tokens_out INTEGER
);

-- Detected anomalies
CREATE TABLE anomalies (
    id INTEGER PRIMARY KEY,
    session_id TEXT REFERENCES sessions(id),
    dimension TEXT,
    expected_value REAL,
    actual_value REAL,
    severity TEXT,  -- warning, error, critical
    investigated BOOLEAN DEFAULT FALSE,
    issue_number INTEGER  -- NULL if no issue created
);

-- Aggregate baselines (updated weekly)
CREATE TABLE baselines (
    dimension TEXT PRIMARY KEY,
    mean REAL,
    stddev REAL,
    p50 REAL,
    p90 REAL,
    p99 REAL,
    sample_count INTEGER,
    updated_at DATETIME
);
```

## Hooks Integration

### PostRun Hook
```rust
// After each run, extract and store metrics
fn post_run_hook(trajectory: &Trajectory) -> Result<()> {
    let metrics = extract_metrics(trajectory)?;
    store_metrics(&metrics)?;

    let anomalies = detect_anomalies(&metrics)?;
    if !anomalies.is_empty() {
        log_anomalies(&anomalies)?;
        maybe_create_issues(&anomalies)?;
    }

    Ok(())
}
```

### PrePromptSubmit Hook
```rust
// Before each prompt, inject learnings
fn pre_prompt_hook(prompt: &str, context: &Context) -> String {
    let learnings = get_relevant_learnings(prompt)?;
    inject_learnings(prompt, learnings)
}
```

## Example Improvement Cycle

1. **Detection**: Analyze shows 20% tool error rate, mostly EISDIR
2. **Investigation**: Grep trajectories for EISDIR, find pattern: agents try to Read directories
3. **Issue Creation**: Auto-create issue "Add directory detection before Read tool calls"
4. **Implementation**: Autopilot claims issue, adds check before Read calls
5. **Validation**: Next week's metrics show EISDIR rate dropped to 2%
6. **Documentation**: Update LEARNINGS.md with this pattern

## Dependencies

```toml
[dependencies]
rusqlite = { version = "0.31", features = ["bundled"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
chrono = { version = "0.4", features = ["serde"] }
statrs = "0.17"  # Statistical functions
```

## Testing Strategy

1. **Unit tests** - Metrics extraction from sample trajectories
2. **Integration tests** - Full analyze pipeline on test data
3. **Regression tests** - Metrics don't break on valid trajectories
4. **Benchmark tests** - Analysis performance on large trajectory sets
5. **Accuracy tests** - Verify anomaly detection catches known issues

## Notes

This directive is unique because it's **self-referential**: autopilot will work on improving itself. This creates interesting dynamics:

1. Improvements to autopilot immediately benefit future d-004 work
2. Metrics from d-004 issues feed back into the system
3. The directive can never truly be "complete" - it's a perpetual flywheel

The goal is not perfection but **constant improvement**: every week should be measurably better than the last.

Key principles:
- **Measure everything** - Can't improve what you don't measure
- **Automate detection** - Humans shouldn't hunt for problems
- **Close the loop** - Every detection should lead to action
- **Celebrate wins** - Track and acknowledge improvements
- **Stay humble** - Autopilot will always have room to improve

## Related Directives

- All other directives benefit from autopilot improvements
- d-005 (GitAfter) trajectories are a special case - agent collaboration metrics

## Open Questions

1. Should metrics be stored per-issue or per-session?
   → Both - session for raw data, issue for outcome correlation

2. How to handle multi-agent sessions (subagents)?
   → Track parent-child relationships, aggregate to parent

3. What's the right cadence for baseline updates?
   → Weekly, with manual override for major changes

4. Should improvements require human approval?
   → Yes for prompt/hook changes, no for issue creation
