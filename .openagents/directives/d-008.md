---
id: d-008
title: Unified Data/Compute/Skills Marketplace
status: active
priority: urgent
created: 2025-12-21
updated: 2025-12-25T14:20:56Z
---

## Goal

Build a unified marketplace for data, compute, and skills — the economic layer that enables agents to buy capabilities, sell services, and transact with each other. This is the connective tissue of the agent economy, built CLI-first for immediate functionality, then with a full WGPUI GUI launched via the unified `openagents` binary.

## Background

From THE_SYNTHESIS:
> "We are building one global connected market of all AI agents, all AI services in one market."

The marketplace unifies three interconnected markets:

1. **Compute Marketplace** — Buy/sell inference capacity (NIP-90 DVMs)
2. **Skills Marketplace** — Agent capabilities as purchasable products (NIP-SA)
3. **Data Marketplace** — Datasets, embeddings, knowledge, and **crowdsourced developer trajectories**

### The Untapped Goldmine: Developer Trajectory Data

Every developer using Codex Code, Cursor, Codex, or any AI coding assistant has **valuable data sitting on their computer**. This includes:

- **Agent interaction traces** — Full transcripts of agent thinking, tool calls, and message blocks
- **Human-computer interaction patterns** — Typing, tab completion, cmd+k usage, composer interactions
- **Task trajectories** — The complete path from initial state to successful completion
- **Environment snapshots** — Git commits as checkpoints, CI/CD results as rewards

**Why this matters**: Everyone else is grinding on synthetic environment generation pipelines, replication training, and artificial task construction. That's the hard way. Real developer trajectories captured from actual coding sessions eliminate all of that:

- **Initial state** = a git commit (real environment, no simulation needed)
- **Task trajectory** = tracking of typing, completions, agent tool calls, thinking blocks
- **Reward signal** = next commit + CI/CD pipeline passed (build succeeds, tests green)
- **Task instructions** = inferred from commit messages or auto-generated

No synthetic environments. No artificial task pipelines. No simulated rewards. Just real developers doing real work, captured and contributed.

**There's no dataset that accounts for real human-computer interactions.** Browser interactions, developer workflows, the actual back-and-forth between humans and agents — this data exists on everyone's machines but nobody is collecting it openly.

**The core insight**: Your work is generating things that are valuable. You should get paid for it.

Instead of big labs hoarding this data and maybe helping you someday, you can:
1. **Contribute** your trajectory data (anonymized, secrets redacted)
2. **Get paid** in Bitcoin for valuable training signal
3. **Benefit** from models trained on crowdsourced real-world data

Open-source redaction scripts run locally before any data leaves your machine. You control what gets shared. This is the open-source alternative to proprietary data moats.

These markets share:
- **Discovery** via Nostr (NIP-89 social graph, NIP-90 competitive bids)
- **Payments** via Bitcoin Lightning (instant micropayments)
- **Identity** via Nostr keypairs (unified with wallet)
- **Flow of Funds** — transparent revenue splits to all contributors

### Why One Market Wins

Labs fight each other (OpenAI vs OpenAI vs Google). Each builds siloed marketplaces.

We're neutral:
- Work with ALL models and frameworks
- Skills for Codex Code work on our platform
- Skills for Cursor work on our platform
- One liquidity pool, exponential network effects

**Reed's Law (V ∝ 2^N)**: Our unified market enables 2^N possible coalitions across ALL participants. Fragmented competitors are exponentially disadvantaged.

### Flow of Funds Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    USER PAYMENT                              │
│                         │                                    │
│                         ▼                                    │
│  ┌─────────────────────────────────────────────────────────┐│
│  │                   FLOW OF FUNDS                          ││
│  │                                                          ││
│  │  Skill/Data Creator:     50-60%                         ││
│  │  Compute Provider:       20-30%                         ││
│  │  Platform (OpenAgents):  10-15%                         ││
│  │  Referrer:               5-10%                          ││
│  └─────────────────────────────────────────────────────────┘│
│                         │                                    │
│                         ▼                                    │
│              Instant Bitcoin Lightning                       │
└─────────────────────────────────────────────────────────────┘
```

## Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         UNIFIED MARKETPLACE                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐         │
│  │     COMPUTE     │    │     SKILLS      │    │      DATA       │         │
│  │   (NIP-90 DVM)  │    │   (NIP-SA)      │    │   (NIP-94/95)   │         │
│  └────────┬────────┘    └────────┬────────┘    └────────┬────────┘         │
│           │                      │                      │                   │
│           └──────────────────────┼──────────────────────┘                   │
│                                  │                                          │
│                    ┌─────────────▼─────────────┐                           │
│                    │      MARKETPLACE CORE      │                           │
│                    │                            │                           │
│                    │  • Discovery (NIP-89)      │                           │
│                    │  • Bidding (NIP-90)        │                           │
│                    │  • Licensing (39220/21)    │                           │
│                    │  • Reputation (NIP-32)     │                           │
│                    │  • Payments (Lightning)    │                           │
│                    └─────────────┬──────────────┘                           │
│                                  │                                          │
│           ┌──────────────────────┼──────────────────────┐                   │
│           │                      │                      │                   │
│  ┌────────▼────────┐   ┌────────▼────────┐   ┌────────▼────────┐          │
│  │   CLI (cargo    │   │   GUI (WGPUI)   │   │   API (library) │          │
│  │   marketplace)  │   │                │   │                 │          │
│  └─────────────────┘   └─────────────────┘   └─────────────────┘          │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Binary Structure

```
crates/marketplace/
├── Cargo.toml
├── src/
│   ├── main.rs              # CLI entry point
│   ├── lib.rs               # Core library exports
│   │
│   ├── core/                # Shared marketplace logic
│   │   ├── mod.rs
│   │   ├── discovery.rs     # NIP-89 provider discovery
│   │   ├── bidding.rs       # NIP-90 job request/response
│   │   ├── reputation.rs    # NIP-32 reputation labels
│   │   ├── licensing.rs     # NIP-SA 39220/39221 skill licenses
│   │   └── payments.rs      # Lightning Flow of Funds
│   │
│   ├── compute/             # Compute marketplace
│   │   ├── mod.rs
│   │   ├── provider.rs      # Become a compute provider
│   │   ├── consumer.rs      # Submit inference jobs
│   │   ├── pricing.rs       # Per-token pricing models
│   │   └── jobs.rs          # Job lifecycle management
│   │
│   ├── skills/              # Skills marketplace
│   │   ├── mod.rs
│   │   ├── publish.rs       # Publish skills (kind:39220)
│   │   ├── install.rs       # Install skills
│   │   ├── invoke.rs        # Metered skill execution
│   │   └── license.rs       # License enforcement
│   │
│   ├── data/                # Data marketplace
│   │   ├── mod.rs
│   │   ├── publish.rs       # Publish datasets (NIP-94/95)
│   │   ├── discover.rs      # Search and filter data
│   │   ├── purchase.rs      # Buy data access
│   │   └── serve.rs         # Serve data to purchasers
│   │
│   ├── trajectories/        # Developer trajectory contribution
│   │   ├── mod.rs
│   │   ├── collect.rs       # Collect local trajectory data
│   │   ├── redact.rs        # Open-source secret redaction
│   │   ├── anonymize.rs     # PII and identifying info removal
│   │   ├── contribute.rs    # Submit to marketplace
│   │   ├── validate.rs      # Trajectory quality validation
│   │   └── rewards.rs       # Payment for contributions
│   │
│   ├── cli/                 # CLI commands
│   │   ├── mod.rs
│   │   ├── compute.rs       # cargo marketplace compute ...
│   │   ├── skills.rs        # cargo marketplace skills ...
│   │   ├── data.rs          # cargo marketplace data ...
│   │   ├── trajectories.rs  # cargo marketplace trajectories ...
│   │   ├── earnings.rs      # cargo marketplace earnings ...
│   │   └── provider.rs      # cargo marketplace provider ...
│   │
│   ├── gui/                 # WGPUI application
│   │   ├── mod.rs
│   │   ├── app.rs           # WGPUI app shell
│   │   ├── state.rs         # UI state + view models
│   │   └── backend.rs       # In-process integration
│   │
│   └── views/               # WGPUI panes
│       ├── mod.rs
│       ├── dashboard.rs     # Main dashboard
│       ├── compute.rs       # Compute marketplace views
│       ├── skills.rs        # Skills marketplace views
│       ├── data.rs          # Data marketplace views
│       ├── earnings.rs      # Earnings/payouts views
│       └── provider.rs      # Provider management views
```

## Success Criteria

### Phase 1: Core Infrastructure (CLI-First)
- [x] Create `crates/marketplace/` workspace crate
- [x] Implement NIP-90 job request/result types (kinds 5000-5999, 6000-6999, 7000)
- [x] Implement NIP-89 discovery (application handler events)
- [ ] Implement provider capability advertisement
- [x] Implement job submission and streaming results
- [x] Add `cargo marketplace` CLI entry point
- [ ] Integrate with Spark (d-001) for Lightning payments

### Phase 2: Compute Marketplace CLI
- [x] `cargo marketplace compute providers` — list available compute providers
- [x] `cargo marketplace compute submit` — submit inference job
- [x] `cargo marketplace compute status` — check job status
- [x] `cargo marketplace compute history` — view job history
- [x] `cargo marketplace provider online` — go online as provider
- [x] `cargo marketplace provider offline` — go offline
- [x] `cargo marketplace provider config` — configure pricing/capabilities
- [x] `cargo marketplace provider earnings` — view earnings
- [ ] Implement automatic local→swarm fallback

### Phase 3: Skills Marketplace CLI
- [x] `cargo marketplace skills browse` — browse available skills
- [x] `cargo marketplace skills search <query>` — search skills
- [x] `cargo marketplace skills show <id>` — show skill details
- [x] `cargo marketplace skills install <id>` — install skill
- [x] `cargo marketplace skills uninstall <id>` — uninstall skill
- [x] `cargo marketplace skills list` — list installed skills
- [ ] `cargo marketplace skills publish` — publish new skill
- [x] `cargo marketplace skills update` — update existing skill
- [ ] `cargo marketplace skills deprecate` — deprecate skill version
- [ ] Implement NIP-SA licensing (kinds 39220, 39221)
- [ ] Implement metered skill execution with Lightning payments

### Phase 4: Data Marketplace CLI
- [ ] `cargo marketplace data browse` — browse available datasets
- [ ] `cargo marketplace data search <query>` — search datasets
- [ ] `cargo marketplace data show <id>` — show dataset details
- [ ] `cargo marketplace data purchase <id>` — purchase dataset access
- [ ] `cargo marketplace data download <id>` — download purchased data
- [ ] `cargo marketplace data publish` — publish dataset
- [ ] `cargo marketplace data serve` — serve data to purchasers
- [ ] Implement NIP-94/95 file metadata and storage
- [ ] Implement access control via NIP-44 encryption

### Phase 4b: Trajectory Contribution System
- [x] `cargo marketplace trajectories scan` — scan local trajectory data sources
- [x] `cargo marketplace trajectories preview` — preview what would be contributed
- [x] `cargo marketplace trajectories redact` — run redaction on trajectory data
- [x] `cargo marketplace trajectories contribute` — submit redacted trajectories
- [x] `cargo marketplace trajectories status` — check contribution status
- [x] `cargo marketplace trajectories earnings` — view trajectory contribution earnings
- [x] Implement trajectory collectors for Codex Code, Cursor, Codex logs
- [x] Implement open-source secret redaction (API keys, tokens, passwords)
- [x] Implement PII anonymization (names, emails, paths)
- [x] Implement trajectory validation (quality scoring, completeness)
- [x] Implement reward calculation based on trajectory value
- [x] Implement git commit correlation (initial state → reward signal)

### Phase 5: Earnings & Payouts CLI
- [x] `cargo marketplace earnings dashboard` — view earnings dashboard
- [x] `cargo marketplace earnings history` — detailed history
- [x] `cargo marketplace earnings withdraw` — withdraw to Lightning
- [x] `cargo marketplace earnings export` — export for accounting
- [ ] Implement Flow of Funds revenue splitting
- [ ] Implement real-time earnings tracking
- [ ] Implement minute-level revenue buckets

### Phase 6: Reputation System
- [ ] Implement NIP-32 labels for reputation
- [ ] Track job success rates per provider
- [ ] Track skill quality metrics (usage, ratings)
- [ ] Implement trust tiers (New → Established → Trusted → Expert)
- [ ] Implement reputation-weighted discovery
- [ ] Add reputation CLI commands

### Phase 7: Marketplace GUI (WGPUI)
- [ ] Create WGPUI app shell (same architecture as autopilot-gui)
- [ ] In-process UI (no local HTTP server)
- [ ] Implement dashboard view with all three markets
- [ ] Implement compute marketplace browse/submit UI
- [ ] Implement skills marketplace browse/install UI
- [ ] Implement data marketplace browse/purchase UI
- [ ] Implement provider management UI (go online, pricing, earnings)
- [ ] Implement earnings dashboard with charts
- [ ] Implement real-time updates via backend channels

### Phase 8: Agent Integration
- [ ] Expose marketplace as MCP tools for agents
- [ ] Implement automatic skill acquisition by agents
- [ ] Implement automatic compute scaling for agents
- [ ] Implement agent-to-agent commerce
- [ ] Integrate with NIP-SA agent state (budget constraints)
- [ ] Implement coalition payment pools for multi-agent jobs

### Phase 9: v1 Compute Units (Launch Priority)

This phase defines the first two verifiable compute unit types that Autopilot can purchase from the marketplace.

#### CU-1: SandboxRun (kind 5930/6930)

Run shell commands against a repo snapshot in an isolated sandbox. Fully verifiable via exit codes and artifact hashes.

**Job Request (kind 5930)**
```json
{
  "kind": 5930,
  "tags": [
    ["p", "<provider-pubkey>"],
    ["repo", "https://github.com/owner/repo.git"],
    ["commit", "abc123def456"],
    ["cmd", "cargo test"],
    ["timeout", "300"],
    ["budget", "5000"]
  ],
  "content": ""
}
```

**Job Result (kind 6930)**
```json
{
  "kind": 6930,
  "tags": [
    ["e", "<request-event-id>"],
    ["status", "success"],
    ["exit_code", "0"],
    ["duration_ms", "45000"],
    ["cpu_seconds", "120"],
    ["memory_gb_minutes", "8"],
    ["egress_mb", "50"],
    ["log_hash", "sha256:abc123..."],
    ["artifact_hash", "sha256:def456..."]
  ],
  "content": "<stdout+stderr logs>"
}
```

**Success Criteria:**
- [ ] Define SandboxRun request/result event types
- [ ] Implement sandbox execution engine (Docker/Firecracker)
- [ ] Capture exit code, timing, resource usage
- [ ] Hash logs and artifacts for verification
- [ ] Implement pay-after-verify settlement
- [ ] CLI: `cargo marketplace compute sandbox-run <repo> <cmd>`

#### CU-2: RepoIndex (kind 5931/6931)

Produce embeddings, symbols, or digests for a repository. Semi-verifiable via schema validation and spot checks.

**Job Request (kind 5931)**
```json
{
  "kind": 5931,
  "tags": [
    ["p", "<provider-pubkey>"],
    ["repo", "https://github.com/owner/repo.git"],
    ["commit", "abc123def456"],
    ["output", "embeddings"],
    ["model", "text-embedding-3-small"],
    ["budget", "10000"]
  ],
  "content": ""
}
```

**Job Result (kind 6931)**
```json
{
  "kind": 6931,
  "tags": [
    ["e", "<request-event-id>"],
    ["status", "success"],
    ["token_count", "150000"],
    ["file_count", "42"],
    ["blob_hash", "sha256:ghi789..."],
    ["blob_url", "https://cdn.provider.com/blobs/..."]
  ],
  "content": ""
}
```

**Success Criteria:**
- [ ] Define RepoIndex request/result event types
- [ ] Implement embeddings generation (OpenAI, local, etc.)
- [ ] Implement symbols extraction (tree-sitter)
- [ ] Implement digests/summaries generation
- [ ] Schema validation for output format
- [ ] Spot-check verification (sample embeddings)
- [ ] CLI: `cargo marketplace compute repo-index <repo> <output-type>`

#### v1 Pricing Model (Price Book)

Fixed prices for v1; dynamic bidding in v2.

**SandboxRun Pricing (configurable defaults)**
```
BASE          = 200 sats         (minimum per job)
CPU_RATE      = 0.5 sats/cpu-second
RAM_RATE      = 0.05 sats/gb-minute
EGRESS_RATE   = 0.02 sats/mb
CAP_PER_RUN   = 20,000 sats      (max per job)
```

**RepoIndex Pricing (configurable defaults)**
```
EMBEDDINGS    = 8 sats/1k tokens
SYMBOLS       = 2 sats/1k tokens
DIGESTS       = 1 sat/1k tokens
FETCH_RATE    = 1 sat/mb fetched
EGRESS_RATE   = 2 sats/mb egressed
CAP_PER_JOB   = 100,000 sats     (max per job)
```

**Success Criteria:**
- [ ] Implement price book configuration
- [ ] Calculate job cost from resource usage
- [ ] Enforce per-job caps
- [ ] Display estimated cost before job submission

#### Autopilot as Buyer

Autopilot is the first consumer of the compute marketplace.

**Flow:**
1. Autopilot needs to run tests → posts SandboxRun job request
2. Provider accepts job → runs in sandbox → returns result
3. Autopilot verifies result (exit code 0, hashes match)
4. Autopilot releases payment via Lightning

**Success Criteria:**
- [ ] Autopilot can post SandboxRun job requests
- [ ] Autopilot can select provider (price × latency × reputation)
- [ ] Autopilot can verify results before payment
- [ ] Autopilot falls back to reserve pool if no provider bids
- [ ] Autopilot respects budget constraints

#### Provider Tiers

Tiered access for compute providers:

| Tier | Name | Requirements | Quotas |
|------|------|--------------|--------|
| 0 | New | Pass qualification suite | 5 jobs/hour |
| 1 | Verified | 100 jobs, >95% success | 50 jobs/hour |
| 2 | Trusted | 1000 jobs, >98% success | 500 jobs/hour |
| 3 | Elite | Invitation + audit | Unlimited |

**Success Criteria:**
- [ ] Implement provider tier system
- [ ] Create qualification suite (test jobs)
- [ ] Track provider success rates
- [ ] Enforce quotas per tier
- [ ] Promotion/demotion logic

#### Pay-After-Verify Settlement

Jobs are paid only after successful verification:

```
1. Consumer submits job with budget hold
2. Provider executes job
3. Provider posts result with hashes
4. Consumer verifies:
   - Exit code matches expected (SandboxRun)
   - Hashes match claimed values
   - Output schema valid (RepoIndex)
5. Consumer releases payment OR disputes
6. Dispute resolution via reputation penalty
```

**Success Criteria:**
- [ ] Implement budget hold mechanism
- [ ] Implement verification checks
- [ ] Implement payment release
- [ ] Implement dispute flow
- [ ] Track disputes in reputation

#### Reserve Provider Pool

Fallback compute capacity for when no providers bid:

- OpenAgents operates reserve capacity
- Higher prices (2x market rate)
- Guarantees jobs can always run
- Revenue funds provider incentives

**Success Criteria:**
- [ ] Configure reserve provider
- [ ] Implement fallback routing
- [ ] Track reserve usage metrics

## CLI Commands Reference

### Compute Commands
```bash
# Discovery
cargo marketplace compute providers                    # List all providers
cargo marketplace compute providers --model llama3    # Filter by model
cargo marketplace compute providers --region us-west  # Filter by region

# Jobs
cargo marketplace compute submit --model llama3 --prompt "..." --budget 1000
cargo marketplace compute submit --file prompt.txt --stream
cargo marketplace compute status <job-id>
cargo marketplace compute cancel <job-id>
cargo marketplace compute history
cargo marketplace compute history --json

# Provider Mode
cargo marketplace provider online
cargo marketplace provider offline
cargo marketplace provider config --price-input 10 --price-output 20  # sats/1k tokens
cargo marketplace provider config --models llama3,mistral
cargo marketplace provider config --schedule "9am-5pm"
cargo marketplace provider status
cargo marketplace provider earnings
cargo marketplace provider earnings --today
cargo marketplace provider earnings --withdraw 10000  # sats
```

### Skills Commands
```bash
# Discovery
cargo marketplace skills browse
cargo marketplace skills browse --category dev-tools
cargo marketplace skills search "code review"
cargo marketplace skills show <skill-id>
cargo marketplace skills show <skill-id> --versions

# Installation
cargo marketplace skills install <skill-id>
cargo marketplace skills install <skill-id>@1.2.0
cargo marketplace skills uninstall <skill-id>
cargo marketplace skills list
cargo marketplace skills update <skill-id>
cargo marketplace skills update --all

# Publishing
cargo marketplace skills publish ./skill.toml
cargo marketplace skills publish --name "PR Reviewer" --price 50
cargo marketplace skills update <skill-id> --version 1.3.0
cargo marketplace skills deprecate <skill-id>@1.0.0 --message "Use v2.0"
```

### Data Commands
```bash
# Discovery
cargo marketplace data browse
cargo marketplace data browse --category embeddings
cargo marketplace data search "rust codebase"
cargo marketplace data show <dataset-id>

# Purchase & Download
cargo marketplace data purchase <dataset-id>
cargo marketplace data download <dataset-id> --output ./data/
cargo marketplace data list  # List purchased datasets

# Publishing
cargo marketplace data publish ./dataset/ --name "Rust Repos" --price 5000
cargo marketplace data serve  # Start serving data
cargo marketplace data serve --port 8080
```

### Trajectory Commands
```bash
# Scan for local trajectory data
cargo marketplace trajectories scan                    # Find all trajectory sources
cargo marketplace trajectories scan --source codex   # Codex Code only
cargo marketplace trajectories scan --source cursor   # Cursor only
cargo marketplace trajectories scan --source codex    # Codex only
cargo marketplace trajectories scan --since 7d        # Last 7 days

# Preview and redact
cargo marketplace trajectories preview                 # Show what would be contributed
cargo marketplace trajectories preview --verbose      # Detailed preview
cargo marketplace trajectories redact                  # Run redaction (secrets, PII)
cargo marketplace trajectories redact --dry-run       # Preview redaction
cargo marketplace trajectories redact --strict        # Aggressive redaction

# Contribute
cargo marketplace trajectories contribute              # Submit redacted trajectories
cargo marketplace trajectories contribute --batch     # Batch mode (non-interactive)
cargo marketplace trajectories contribute --review    # Review each before submit

# Status and earnings
cargo marketplace trajectories status                  # View pending/accepted/rejected
cargo marketplace trajectories earnings               # View trajectory earnings
cargo marketplace trajectories earnings --detail     # Per-trajectory breakdown

# Configuration
cargo marketplace trajectories config                  # Show current config
cargo marketplace trajectories config --auto          # Enable auto-contribution
cargo marketplace trajectories config --sources codex,cursor
cargo marketplace trajectories config --redaction strict
```

### Earnings Commands
```bash
cargo marketplace earnings                    # Summary dashboard
cargo marketplace earnings --compute          # Compute earnings only
cargo marketplace earnings --skills           # Skills earnings only
cargo marketplace earnings --data             # Data earnings only
cargo marketplace earnings --trajectories     # Trajectory contribution earnings
cargo marketplace earnings history            # Full history
cargo marketplace earnings history --period month
cargo marketplace earnings withdraw 50000     # Withdraw sats
cargo marketplace earnings export --format csv --output earnings.csv
```

## NIP Event Kinds

### Compute (NIP-90 DVMs)

| Kind | Name | Description |
|------|------|-------------|
| 5050 | Text Generation Request | Request LLM inference |
| 5100 | Image Generation Request | Request image generation |
| 5200 | Speech-to-Text Request | Request transcription |
| 5xxx | Other Job Types | Extensible job types |
| 6050 | Text Generation Result | Response to 5050 |
| 6100 | Image Generation Result | Response to 5100 |
| 6200 | Speech-to-Text Result | Response to 5200 |
| 7000 | Job Feedback | Status updates, payments |

### Skills (NIP-SA)

| Kind | Name | Description |
|------|------|-------------|
| 39220 | Skill License | Marketplace-issued license |
| 39221 | Skill Delivery | Gift-wrapped skill content |

### Data (NIP-94/95)

| Kind | Name | Description |
|------|------|-------------|
| 1063 | File Metadata | Dataset file info |
| 1064 | Binary Blob | Actual file content |

### Reputation (NIP-32)

| Kind | Name | Description |
|------|------|-------------|
| 1985 | Label | Reputation labels |

## Database Schema

```sql
-- Providers (for local tracking)
CREATE TABLE providers (
    id TEXT PRIMARY KEY,
    pubkey TEXT NOT NULL,
    name TEXT,
    capabilities JSONB NOT NULL,  -- models, regions, etc
    pricing JSONB NOT NULL,       -- per_1k_input, per_1k_output
    reputation_score REAL DEFAULT 0.0,
    jobs_completed INTEGER DEFAULT 0,
    success_rate REAL DEFAULT 0.0,
    last_seen_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Jobs (compute/skills/data)
CREATE TABLE jobs (
    id TEXT PRIMARY KEY,
    type TEXT NOT NULL,           -- compute, skill, data
    provider_id TEXT REFERENCES providers(id),
    request_event_id TEXT,        -- Nostr event ID
    result_event_id TEXT,
    status TEXT NOT NULL,         -- pending, processing, completed, failed
    input_tokens INTEGER,
    output_tokens INTEGER,
    cost_sats INTEGER,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    completed_at TIMESTAMPTZ
);

-- Skills (installed)
CREATE TABLE installed_skills (
    id TEXT PRIMARY KEY,
    skill_id TEXT NOT NULL,
    version TEXT NOT NULL,
    name TEXT NOT NULL,
    creator_pubkey TEXT NOT NULL,
    price_per_call INTEGER,
    license_event_id TEXT,
    installed_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    UNIQUE(skill_id)
);

-- Skill Usage
CREATE TABLE skill_usage (
    id TEXT PRIMARY KEY,
    skill_id TEXT NOT NULL REFERENCES installed_skills(skill_id),
    credits_used INTEGER NOT NULL,
    input_tokens INTEGER,
    output_tokens INTEGER,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Purchased Data
CREATE TABLE purchased_data (
    id TEXT PRIMARY KEY,
    dataset_id TEXT NOT NULL,
    name TEXT NOT NULL,
    creator_pubkey TEXT NOT NULL,
    price_sats INTEGER NOT NULL,
    access_key TEXT,              -- NIP-44 decryption key
    purchased_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    expires_at TIMESTAMPTZ,
    UNIQUE(dataset_id)
);

-- Trajectory Contributions
CREATE TABLE trajectory_contributions (
    id TEXT PRIMARY KEY,
    source TEXT NOT NULL,         -- codex, cursor, codex, etc
    session_id TEXT,              -- original session identifier
    initial_commit TEXT,          -- git commit hash (initial state)
    final_commit TEXT,            -- git commit hash (reward signal)
    ci_passed BOOLEAN,            -- CI/CD result as reward
    trajectory_hash TEXT NOT NULL, -- hash of redacted trajectory
    token_count INTEGER,          -- size metric
    tool_calls INTEGER,           -- complexity metric
    quality_score REAL,           -- validation score 0-1
    status TEXT NOT NULL,         -- pending, accepted, rejected
    reward_sats INTEGER,          -- payment for contribution
    redaction_version TEXT,       -- version of redaction script used
    contributed_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    processed_at TIMESTAMPTZ
);

-- Trajectory Sources (local scan cache)
CREATE TABLE trajectory_sources (
    id TEXT PRIMARY KEY,
    source TEXT NOT NULL,         -- codex, cursor, codex
    path TEXT NOT NULL,           -- local file path
    session_count INTEGER,        -- number of sessions found
    last_scanned_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    auto_contribute BOOLEAN DEFAULT FALSE
);

-- Earnings (for providers/creators)
CREATE TABLE earnings (
    id TEXT PRIMARY KEY,
    type TEXT NOT NULL,           -- compute, skill, data, trajectory
    item_id TEXT NOT NULL,        -- job_id, skill_id, dataset_id, trajectory_id
    gross_sats INTEGER NOT NULL,
    platform_fee_sats INTEGER NOT NULL,
    net_sats INTEGER NOT NULL,
    period_start TIMESTAMPTZ NOT NULL,
    period_end TIMESTAMPTZ NOT NULL,
    paid_out BOOLEAN DEFAULT FALSE,
    payment_preimage TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Revenue Buckets (minute-level)
CREATE TABLE revenue_buckets (
    id TEXT PRIMARY KEY,
    bucket_minute TIMESTAMPTZ NOT NULL,
    type TEXT NOT NULL,
    item_id TEXT NOT NULL,
    gross_sats INTEGER NOT NULL,
    creator_sats INTEGER NOT NULL,
    compute_sats INTEGER NOT NULL,
    platform_sats INTEGER NOT NULL,
    referrer_sats INTEGER,
    split_version INTEGER NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    UNIQUE(bucket_minute, type, item_id)
);
```

## Configuration

```toml
# ~/.openagents/marketplace.toml

[general]
# Default relays for marketplace discovery
relays = [
    "wss://relay.damus.io",
    "wss://nos.lol",
    "wss://relay.nostr.band"
]

[compute]
# Provider configuration
enabled = false
models = ["llama3", "mistral", "phi"]
regions = ["us-west"]
pricing.per_1k_input_sats = 10
pricing.per_1k_output_sats = 20
pricing.minimum_sats = 100
schedule = "always"  # or "9am-5pm" etc
max_concurrent_jobs = 3

# Consumer preferences
preferred_providers = []
max_price_per_1k = 50
fallback_to_swarm = true

[skills]
# Publishing defaults
default_visibility = "public"
default_price_per_call = 50

# Installation
auto_update = false
require_approval = true

[data]
# Publishing
default_price_sats = 1000
storage_path = "~/.openagents/marketplace/data"

# Download
download_path = "~/.openagents/marketplace/downloads"

[trajectories]
# Data sources to scan
sources = ["codex", "cursor", "codex"]

# Source paths (auto-detected if not specified)
# codex_path = "~/.codex/logs"
# cursor_path = "~/.cursor/logs"
# codex_path = "~/.codex/logs"

# Contribution settings
auto_contribute = false           # Require manual approval
min_quality_score = 0.5           # Minimum quality to contribute
require_ci_signal = false         # Only contribute if CI/CD result available

# Redaction settings
redaction_level = "standard"      # standard, strict, paranoid
redact_file_paths = true          # Replace absolute paths
redact_usernames = true           # Remove system usernames
redact_repo_names = false         # Keep repo names (useful context)
custom_patterns = []              # Additional regex patterns to redact

# Reward preferences
min_reward_sats = 10              # Minimum reward to bother contributing

[payments]
# Flow of Funds splits (percentages)
creator_share = 55
compute_share = 25
platform_share = 12
referrer_share = 8

# Lightning
auto_withdraw = false
withdraw_threshold_sats = 100000
lightning_address = ""

[reputation]
# Trust thresholds
new_threshold = 0
established_threshold = 10
trusted_threshold = 50
expert_threshold = 100
```

## Dependencies

### External Crates
```toml
[dependencies]
# Nostr
nostr-core = { path = "../nostr/core" }
nostr-client = { path = "../nostr/client" }

# Payments
spark = { path = "../spark" }

# CLI
clap = { version = "4", features = ["derive"] }

# GUI
wgpui = { path = "../wgpui", features = ["desktop"] }
wgpu = "24.0"
winit = "0.30"
pollster = "0.4"

# Database
rusqlite = { version = "0.32", features = ["bundled"] }

# Async
tokio = { version = "1", features = ["full"] }

# Serialization
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Utilities
chrono = "0.4"
uuid = { version = "1", features = ["v4"] }
tracing = "0.1"
```

### Internal Dependencies
- **d-001** (Spark) — Lightning payments for marketplace transactions
- **d-002** (Nostr) — NIP-89, NIP-90, NIP-94, NIP-95, NIP-32 implementations
- **d-003** (Wallet) — UnifiedIdentity for marketplace identity
- **d-006** (NIP-SA) — Skill licensing (39220, 39221), agent integration
- **d-007** (FROSTR) — Threshold signing for protected skill delivery

## Testing Strategy

1. **Unit tests** — Each module (discovery, bidding, payments, licensing)
2. **Integration tests** — Full job lifecycle with mock relays
3. **CLI tests** — All commands with expected outputs
4. **Payment tests** — Lightning payment flows (mock/regtest)
5. **E2E tests** — Full marketplace flow: publish → discover → purchase → deliver
6. **Load tests** — Multiple concurrent providers and consumers
7. **Interop tests** — Compatibility with other NIP-90 implementations

## Security Considerations

### Payment Security
- Hold invoices for job prepayment
- Verify preimage before delivering results
- Idempotent payment references (no double-pay)
- Budget enforcement with hard caps

### Content Security
- Sandboxed skill execution
- No raw sensitive data in logs
- NIP-44 encryption for private data
- License verification before skill delivery

### Provider Security
- Capability verification before advertising
- Resource isolation for jobs
- Rate limiting on job acceptance
- Reputation penalties for failures

### Privacy
- No PII in marketplace events
- Encrypted data delivery
- Optional pseudonymous participation
- Data residency compliance

### Trajectory Data Security
- **Local redaction first** — All secrets removed before data leaves machine
- **Open-source redaction** — Redaction scripts are fully auditable
- **Pattern-based detection** — API keys, tokens, passwords, private keys
- **Path anonymization** — Absolute paths replaced with relative
- **PII removal** — Usernames, emails, identifying information stripped
- **User control** — Review and approve each contribution
- **Opt-in only** — Never auto-contribute without explicit consent
- **Deletion rights** — Request removal of contributed trajectories

## GUI Architecture

Same pattern as d-009 (Autopilot GUI):

```
┌─────────────────────────────────────────────────────────────────┐
│                    Marketplace Desktop App                       │
├─────────────────────────────────────────────────────────────────┤
│  winit EventLoop + wgpu                                         │
│  WGPUI renderer + layout                                        │
│  App state + panes (dashboard/compute/skills/data/etc.)          │
│  Backend channels (commands/events)                             │
└─────────────────────────────────────────────────────────────────┘
```

## Notes

This directive implements the marketplace layer from THE_SYNTHESIS vision:

> "We are building one global connected market of all AI agents, all AI services in one market."

The marketplace is not just a feature — it's the economic foundation that makes autonomous agents viable. Agents need to:
- **Buy compute** to think
- **Acquire skills** to act
- **Access data** to learn
- **Earn revenue** to survive

This creates natural economic alignment: agents that create value earn Bitcoin, agents that don't create value can't afford to run. This is safer than centralized control which can be captured.

### Trajectory Data: The Open Alternative

The trajectory contribution system represents a fundamental shift in how AI training data is sourced:

**Current model (Labs):**
- Users generate valuable interaction data
- Labs collect it silently, maybe improve their models
- Users see no direct benefit
- Data stays siloed in proprietary systems

**Our model (Open Marketplace):**
- Users generate valuable interaction data
- Users **choose** to contribute (after local redaction)
- Users **get paid** in Bitcoin for valuable contributions
- Data flows to open training efforts
- Everyone benefits from better models

This isn't just about fairness — it's about **unlocking data that would otherwise never be used**. Most developers don't have auto-delete enabled, so months or years of valuable coding trajectories are sitting on their machines. That's real-world task data with real reward signals (CI passing, tests green, builds succeeding) that eliminates the need for synthetic environment generation.

**We're going to pay you. Your work is generating things that are valuable.**

### Why CLI-First

CLI-first ensures:
1. **Immediate functionality** — No waiting for UI
2. **Scriptability** — Integrate with automation
3. **Agent-native** — Agents use CLI, not GUI
4. **Testability** — Easy to test all functionality
5. **Foundation** — GUI is just a wrapper around CLI

### Reed's Law Moat

The unified marketplace captures Reed's Law advantages:
- **2^N possible coalitions** across all participants
- **Cross-lab, cross-framework** skill and compute pooling
- **Coalition payment pools** via Lightning MPP
- **First-mover advantage** amplified exponentially

Labs can't replicate this because they fight each other. We build the neutral connective tissue.

## Related Directives

- **d-001** (Spark SDK) — Lightning payment infrastructure
- **d-002** (Nostr Protocol) — NIP-89, NIP-90, NIP-94, NIP-95, NIP-32
- **d-003** (Wallet) — Identity and payment foundation
- **d-004** (Autopilot) — Agent execution (marketplace consumer)
- **d-005** (GitAfter) — Bounty system (marketplace for development work)
- **d-006** (NIP-SA) — Skill licensing, agent economics
- **d-007** (FROSTR) — Threshold signatures for secure skill delivery

## Related Documents

- `reference/writing/THE_SYNTHESIS.md` — Strategic vision
- `reference/writing/SYNTHESIS_MARKETPLACE.md` — Marketplace economics
- `reference/writing/SYNTHESIS_REEDS_LAW.md` — Coalition economics
- `reference/planning/14-marketplace.md` — EPIC-13 detailed stories
- `reference/planning/21-swarm-compute.md` — EPIC-20 compute network
