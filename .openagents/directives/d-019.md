---
id: d-019
title: "GPT-OSS Local Inference Integration"
status: active
priority: high
created: 2025-12-23
updated: 2025-12-23
---

# d-019: GPT-OSS Local Inference Integration

## Goal

Add GPT-OSS (OpenAI's open-weight models) as a first-class local inference backend, establishing a unified abstraction layer (`LocalModelBackend` trait) that both GPT-OSS and the existing fm-bridge implement. This enables compile-time type-safe swapping between local model providers throughout the application.

## Background

OpenAgents currently steers Claude Code and Codex via external CLI tools. For local inference:
- `crates/fm-bridge/` provides Apple Foundation Model support (macOS only)
- `crates/compute/` has a stubbed OllamaService (not implemented)

GPT-OSS provides:
- **gpt-oss-120b** — 117B params (5.1B active), fits single 80GB GPU
- **gpt-oss-20b** — 21B params (3.6B active), for local/lower latency
- Reference Responses API server with browser/python tools built-in
- Apache 2.0 license, fine-tunable, full chain-of-thought access
- Configurable reasoning effort (low/medium/high)

The Responses API server in gpt-oss provides an OpenAI-compatible interface that aligns with our existing fm-bridge patterns, making integration straightforward.

### Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│                    crates/local-inference/                          │
│  ┌───────────────────────────────────────────────────────────────┐  │
│  │  pub trait LocalModelBackend: Send + Sync {                   │  │
│  │      async fn is_available(&self) -> bool;                    │  │
│  │      async fn list_models(&self) -> Result<Vec<ModelInfo>>;   │  │
│  │      async fn complete(&self, req: CompletionRequest)         │  │
│  │          -> Result<CompletionResponse>;                       │  │
│  │      async fn stream(&self, req: CompletionRequest)           │  │
│  │          -> Result<impl Stream<Item=Result<StreamChunk>>>;    │  │
│  │  }                                                            │  │
│  └───────────────────────────────────────────────────────────────┘  │
│                              ▲                                      │
│              ┌───────────────┼───────────────┐                      │
│              │               │               │                      │
└──────────────┼───────────────┼───────────────┼──────────────────────┘
               │               │               │
     ┌─────────┴─────┐ ┌───────┴───────┐ ┌─────┴─────────┐
     │ fm-bridge     │ │ gpt-oss       │ │ future:       │
     │ (Apple FM)    │ │ (Responses    │ │ llama.cpp,    │
     │               │ │  API Server)  │ │ MLX, etc.     │
     └───────────────┘ └───────────────┘ └───────────────┘
```

### Key Decisions

1. **Responses API Server** — Use GPT-OSS's reference server (`python -m gpt_oss.responses_api.serve`), not Ollama. Provides browser/python tools out of the box.

2. **Shared Trait** — Create `LocalModelBackend` trait that both fm-bridge and gpt-oss implement. Compile-time type safety via generics.

3. **Builder Pattern** — Follow fm-bridge's `FMClientBuilder` pattern for GPT-OSS client configuration.

4. **Layered Design** — Start with inference primitive, then build agent wrapper for full autopilot integration.

## Success Criteria

### Phase 1: LocalModelBackend Trait (v0.1)
- [ ] Create `crates/local-inference/` with shared types
- [ ] Define `LocalModelBackend` trait with core methods
- [ ] Define shared types: `CompletionRequest`, `CompletionResponse`, `StreamChunk`, `ModelInfo`
- [ ] Add `LocalModelError` error type with thiserror

### Phase 2: GPT-OSS Client (v0.2)
- [ ] Create `crates/gpt-oss/` crate
- [ ] Implement `GptOssClient` with builder pattern
- [ ] Implement `LocalModelBackend` for `GptOssClient`
- [ ] Add Responses API compatibility (tool use, reasoning effort)
- [ ] Support streaming via SSE
- [ ] Add health check and model listing

### Phase 3: Refactor fm-bridge (v0.3)
- [ ] Add `local-inference` as dependency to fm-bridge
- [ ] Implement `LocalModelBackend` for `FMClient`
- [ ] Maintain backwards compatibility with existing API
- [ ] Update types to use shared definitions

### Phase 4: Agent Wrapper (v0.4)
- [ ] Create `crates/gpt-oss-agent/` for agent-level abstraction
- [ ] Implement tool handling (browser, python, apply_patch)
- [ ] Add trajectory recording support
- [ ] Integrate with `acp-adapter` pattern

### Phase 5: GUI & Autopilot Integration (v0.5)
- [ ] Add "gpt-oss" option to agent selection in GUI
- [ ] Add local model configuration UI (`/api/local-inference/config`)
- [ ] Support in autopilot CLI (`--agent gpt-oss --model 20b`)
- [ ] Add model download/status endpoints

### Phase 6: Documentation & Testing (v0.6)
- [ ] Integration tests with mock Responses API
- [ ] E2E test with real GPT-OSS server
- [ ] Usage documentation
- [ ] Performance benchmarks

## Key Files to Create/Modify

| File | Purpose |
|------|---------|
| `crates/local-inference/Cargo.toml` | **NEW** - Shared trait crate |
| `crates/local-inference/src/lib.rs` | **NEW** - Trait and type exports |
| `crates/local-inference/src/types.rs` | **NEW** - Shared request/response types |
| `crates/local-inference/src/error.rs` | **NEW** - LocalModelError |
| `crates/gpt-oss/Cargo.toml` | **NEW** - GPT-OSS client crate |
| `crates/gpt-oss/src/lib.rs` | **NEW** - Public exports |
| `crates/gpt-oss/src/client.rs` | **NEW** - GptOssClient with builder |
| `crates/gpt-oss/src/responses_api.rs` | **NEW** - Responses API types |
| `crates/gpt-oss/src/tools.rs` | **NEW** - Tool handling (browser, python) |
| `crates/gpt-oss-agent/Cargo.toml` | **NEW** - Agent wrapper crate |
| `crates/gpt-oss-agent/src/lib.rs` | **NEW** - Agent session management |
| `crates/fm-bridge/Cargo.toml` | Add local-inference dependency |
| `crates/fm-bridge/src/lib.rs` | Implement LocalModelBackend |
| `src/gui/routes/agents.rs` | Add gpt-oss to agent list |
| `src/gui/routes/local_inference.rs` | **NEW** - Config/status endpoints |
| `crates/autopilot/src/main.rs` | Add --agent gpt-oss option |
| `Cargo.toml` | Add new crates to workspace |

## Dependencies

**External (new)**:
- None beyond what's already in workspace (reqwest, tokio, serde, async-trait)

**Internal**:
- `local-inference` → no deps (pure trait crate)
- `gpt-oss` → `local-inference`, reqwest, tokio, serde
- `gpt-oss-agent` → `gpt-oss`, `acp-adapter`, `recorder`
- `fm-bridge` → adds `local-inference`

## API Design

### LocalModelBackend Trait

```rust
// crates/local-inference/src/lib.rs
use async_trait::async_trait;

#[async_trait]
pub trait LocalModelBackend: Send + Sync {
    /// Check if the backend is available and responding
    async fn is_available(&self) -> bool;

    /// List available models
    async fn list_models(&self) -> Result<Vec<ModelInfo>, LocalModelError>;

    /// Complete a prompt (non-streaming)
    async fn complete(
        &self,
        request: CompletionRequest,
    ) -> Result<CompletionResponse, LocalModelError>;

    /// Stream a completion
    async fn stream(
        &self,
        request: CompletionRequest,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<StreamChunk, LocalModelError>> + Send>>, LocalModelError>;

    /// Backend name for logging/display
    fn name(&self) -> &str;

    /// Default model if none specified
    fn default_model(&self) -> &str;
}
```

### GptOssClient

```rust
// crates/gpt-oss/src/client.rs
pub struct GptOssClient {
    base_url: String,
    http_client: Client,
    default_model: String,
    reasoning_effort: ReasoningEffort,
}

pub struct GptOssClientBuilder {
    base_url: String,
    default_model: String,
    timeout: Duration,
    reasoning_effort: ReasoningEffort,
}

impl GptOssClientBuilder {
    pub fn new() -> Self;
    pub fn base_url(self, url: impl Into<String>) -> Self;
    pub fn model(self, model: impl Into<String>) -> Self;
    pub fn reasoning_effort(self, effort: ReasoningEffort) -> Self;
    pub fn timeout(self, timeout: Duration) -> Self;
    pub fn build(self) -> Result<GptOssClient, LocalModelError>;
}

#[derive(Clone, Copy)]
pub enum ReasoningEffort {
    Low,
    Medium,
    High,
}
```

### Usage Example

```rust
use local_inference::LocalModelBackend;
use gpt_oss::GptOssClient;
use fm_bridge::FMClient;

// Generic function works with any local model
async fn run_inference<B: LocalModelBackend>(backend: &B, prompt: &str) -> Result<String> {
    let request = CompletionRequest {
        messages: vec![ChatMessage::user(prompt)],
        ..Default::default()
    };

    let response = backend.complete(request).await?;
    Ok(response.content())
}

// Use with GPT-OSS
let gpt_oss = GptOssClient::builder()
    .base_url("http://localhost:8000")
    .model("gpt-oss-20b")
    .reasoning_effort(ReasoningEffort::Medium)
    .build()?;

let result = run_inference(&gpt_oss, "Explain FROST signatures").await?;

// Use with Apple FM
let fm = FMClient::new()?;
let result = run_inference(&fm, "Explain FROST signatures").await?;
```

## Testing Strategy

1. **Unit Tests** - Mock HTTP responses for client tests
2. **Integration Tests** - Spin up mock Responses API server
3. **Trait Compliance** - Test both backends against same test suite
4. **E2E Tests** - Real GPT-OSS server (requires GPU, optional)

## Sub-Issues

| ID | Title | Description | Phase |
|----|-------|-------------|-------|
| gpt-oss-001 | Create local-inference trait crate | New crate with LocalModelBackend trait and shared types | v0.1 |
| gpt-oss-002 | Implement GptOssClient | HTTP client with builder pattern for Responses API | v0.2 |
| gpt-oss-003 | Add streaming support | SSE streaming for GPT-OSS completions | v0.2 |
| gpt-oss-004 | Implement LocalModelBackend for GptOssClient | Wire up trait implementation | v0.2 |
| gpt-oss-005 | Refactor fm-bridge to use shared types | Import from local-inference, implement trait | v0.3 |
| gpt-oss-006 | Add reasoning effort support | Low/medium/high effort configuration | v0.2 |
| gpt-oss-007 | Create gpt-oss-agent wrapper | Agent-level abstraction with tool handling | v0.4 |
| gpt-oss-008 | Add browser tool support | Integrate browser tool from gpt-oss | v0.4 |
| gpt-oss-009 | Add python tool support | Integrate python tool from gpt-oss | v0.4 |
| gpt-oss-010 | GUI agent selection | Add gpt-oss to agent dropdown | v0.5 |
| gpt-oss-011 | Local inference config UI | Settings page for model/server config | v0.5 |
| gpt-oss-012 | Autopilot CLI integration | --agent gpt-oss flag | v0.5 |
| gpt-oss-013 | Integration tests | Mock server tests for both backends | v0.6 |
| gpt-oss-014 | Documentation | Usage guide and API docs | v0.6 |

## Notes

### Running GPT-OSS Locally

```bash
# Install gpt-oss
pip install gpt-oss

# Start Responses API server (uses local checkpoint)
python -m gpt_oss.responses_api.serve \
  --checkpoint gpt-oss-20b/ \
  --port 8000 \
  --inference-backend triton

# Or with Ollama (simpler but less features)
ollama run gpt-oss:20b
```

### Environment Variables

```bash
# GPT-OSS server URL (default: http://localhost:8000)
GPT_OSS_URL=http://localhost:8000

# Default model (default: gpt-oss-20b)
GPT_OSS_MODEL=gpt-oss-20b

# Reasoning effort (default: medium)
GPT_OSS_REASONING_EFFORT=medium
```

### Harmony Format

GPT-OSS models require the Harmony response format. The Responses API server handles this automatically, but direct inference requires the `openai-harmony` package for prompt encoding.

## Related Directives

- d-017: ACP Integration (agent protocol patterns)
- d-009: Autopilot GUI (UI integration)
- d-012: No Stubs (all implementations must be complete)

## References

- [GPT-OSS Repository](https://github.com/openai/gpt-oss)
- [Harmony Format](https://github.com/openai/harmony)
- [Model Card](https://arxiv.org/abs/2508.10925)
- Local reference: `~/code/gpt-oss/README.md`
