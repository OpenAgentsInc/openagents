---
id: d-019
title: "GPT-OSS Local Inference Integration"
status: active
priority: high
created: 2025-12-23
updated: 2025-12-25T14:24:33Z
---

# d-019: GPT-OSS Local Inference Integration

## Goal

Add GPT-OSS (OpenAI's open-weight models) as a first-class local inference backend, establishing a unified abstraction layer (`LocalModelBackend` trait) that both GPT-OSS and the existing fm-bridge implement. This enables compile-time type-safe swapping between local model providers throughout the application.

## Background

OpenAgents currently steers Codex Code and Codex via external CLI tools. For local inference:
- `crates/fm-bridge/` provides Apple Foundation Model support (macOS only)
- `crates/compute/` has a stubbed OllamaService (not implemented)

GPT-OSS provides:
- **gpt-oss-120b** — 117B params (5.1B active), fits single 80GB GPU
- **gpt-oss-20b** — 21B params (3.6B active), for local/lower latency
- GGUF quantized models for efficient inference via llama.cpp
- Apache 2.0 license, fine-tunable, full chain-of-thought access
- Harmony format for reasoning with tool use

We use llama.cpp (via llama-server) for inference — 5-10x faster than Ollama. Tools are implemented natively in Rust, learning from GPT-OSS's patterns but without Python dependencies.

### Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│                    crates/local-inference/                          │
│  ┌───────────────────────────────────────────────────────────────┐  │
│  │  pub trait LocalModelBackend: Send + Sync {                   │  │
│  │      async fn is_available(&self) -> bool;                    │  │
│  │      async fn list_models(&self) -> Result<Vec<ModelInfo>>;   │  │
│  │      async fn complete(&self, req: CompletionRequest)         │  │
│  │          -> Result<CompletionResponse>;                       │  │
│  │      async fn stream(&self, req: CompletionRequest)           │  │
│  │          -> Result<impl Stream<Item=Result<StreamChunk>>>;    │  │
│  │  }                                                            │  │
│  └───────────────────────────────────────────────────────────────┘  │
│                              ▲                                      │
│          ┌───────────────────┼───────────────────┐                  │
│          │                   │                   │                  │
└──────────┼───────────────────┼───────────────────┼──────────────────┘
           │                   │                   │
     ┌─────┴─────┐       ┌─────┴─────┐       ┌─────┴─────┐
     │ fm-bridge │       │ gpt-oss   │       │ future:   │
     │ (Apple FM)│       │ (llama.cpp│       │ MLX, etc. │
     └───────────┘       │  server)  │       └───────────┘
                         └─────┬─────┘
                               │
                    ┌──────────┴──────────┐
                    │  gpt-oss-agent      │
                    │  (Rust tools)       │
                    │  ├── browser/       │
                    │  ├── python/        │
                    │  └── apply_patch/   │
                    └─────────────────────┘
```

### Key Decisions

1. **llama.cpp Backend Only** — Direct GGUF inference via llama-server. 5-10x faster than Ollama. See `docs/local-inference.md` for benchmarks (104 t/s on 20b, 15.9 t/s on 120b). No Python dependencies.

2. **Native Rust Tools** — Build browser and python tool support directly in Rust, learning from GPT-OSS's patterns but not using their Python implementation. Tools live in `crates/gpt-oss-agent/src/tools/`.

3. **Shared Trait** — Create `LocalModelBackend` trait that fm-bridge and gpt-oss implement. Compile-time type safety via generics.

4. **Builder Pattern** — Follow fm-bridge's `FMClientBuilder` pattern for GPT-OSS client configuration.

5. **Layered Design** — Inference primitive → Agent wrapper with tools → Autopilot integration.

## Success Criteria

### Phase 1: LocalModelBackend Trait (v0.1)
- [x] Create `crates/local-inference/` with shared types
- [x] Define `LocalModelBackend` trait with core methods
- [x] Define shared types: `CompletionRequest`, `CompletionResponse`, `StreamChunk`, `ModelInfo`
- [x] Add `LocalModelError` error type with thiserror

### Phase 2: GPT-OSS Client (v0.2)
- [x] Create `crates/gpt-oss/` crate
- [x] Implement `GptOssClient` with builder pattern
- [x] Implement `LocalModelBackend` for `GptOssClient`
- [x] Add Responses API compatibility (tool use, reasoning effort)
- [x] Integrate Harmony prompt formatting/parsing (`openai-harmony`)
- [x] Support streaming via SSE
- [x] Add health check and model listing

### Phase 3: Refactor fm-bridge (v0.3)
- [x] Add `local-inference` as dependency to fm-bridge
- [x] Implement `LocalModelBackend` for `FMClient`
- [x] Maintain backwards compatibility with existing API
- [x] Update types to use shared definitions

### Phase 4: Agent Wrapper (v0.4)
- [x] Create `crates/gpt-oss-agent/` for agent-level abstraction
- [x] Implement tool handling (browser, python, apply_patch)
- [x] Wire Harmony tool calls into the session execution loop
- [x] Add trajectory recording support
- [x] Integrate with `acp-adapter` pattern

### Phase 4b: Cross-Backend Runner (v0.4)
- [x] Create `crates/fm-bridge-agent/` wrapper with tools + trajectories
- [x] Add `local-infer` runner for GPT-OSS or FM bridge

### Phase 5: GUI & Autopilot Integration (v0.5)
- [ ] Add "gpt-oss" option to WGPUI agent selection
- [ ] Add local model configuration pane (WGPUI + backend channels)
- [x] Support in autopilot CLI (`--agent gpt-oss --model 20b`, tool loop + trajectories)
- [ ] Add model download/status endpoints

### Phase 6: Documentation & Testing (v0.6)
- [x] Integration tests with mock Responses API
- [x] E2E test with real GPT-OSS server (`crates/gpt-oss/tests/real_server_e2e.rs`, ignored)
- [x] Usage documentation
- [x] Usage examples in `docs/gpt-oss/examples/`
- [x] API documentation (`docs/gpt-oss/API.md`)
- [x] Performance benchmarks (`crates/local-inference/benches/backend_overhead.rs`)

## Key Files to Create/Modify

| File | Purpose |
|------|---------|
| `crates/local-inference/Cargo.toml` | **NEW** - Shared trait crate |
| `crates/local-inference/src/lib.rs` | **NEW** - Trait and type exports |
| `crates/local-inference/src/types.rs` | **NEW** - Shared request/response types |
| `crates/local-inference/src/error.rs` | **NEW** - LocalModelError |
| `crates/gpt-oss/Cargo.toml` | **NEW** - GPT-OSS client crate |
| `crates/gpt-oss/src/lib.rs` | **NEW** - Public exports |
| `crates/gpt-oss/src/client.rs` | **NEW** - GptOssClient with builder |
| `crates/gpt-oss/src/responses_api.rs` | **NEW** - Responses API types |
| `crates/gpt-oss/src/tools.rs` | **NEW** - Tool handling (browser, python) |
| `crates/gpt-oss-agent/Cargo.toml` | **NEW** - Agent wrapper crate |
| `crates/gpt-oss-agent/src/lib.rs` | **NEW** - Agent session management |
| `crates/fm-bridge-agent/` | **NEW** - Agent wrapper for Apple FM bridge |
| `src/bin/local-infer.rs` | **NEW** - Unified local inference CLI |
| `scripts/local-infer.sh` | **NEW** - Local inference wrapper script |
| `crates/fm-bridge/Cargo.toml` | Add local-inference dependency |
| `crates/fm-bridge/src/lib.rs` | Implement LocalModelBackend |
| `src/gui/routes/agents.rs` | Add gpt-oss to agent list |
| `src/gui/routes/local_inference.rs` | **NEW** - Config/status endpoints |
| `crates/autopilot/src/main.rs` | Add --agent gpt-oss option |
| `Cargo.toml` | Add new crates to workspace |

## Dependencies

**External (new)**:
- `openai-harmony` — Official Rust crate for Harmony format (at `~/code/harmony/`)

**Internal**:
- `local-inference` → async-trait, thiserror (pure trait crate)
- `gpt-oss` → `local-inference`, `openai-harmony`, reqwest, tokio, serde
- `gpt-oss-agent` → `gpt-oss`, `acp-adapter`, `recorder`
- `fm-bridge` → adds `local-inference`

## API Design

### LocalModelBackend Trait

```rust
// crates/local-inference/src/lib.rs
use async_trait::async_trait;

#[async_trait]
pub trait LocalModelBackend: Send + Sync {
    /// Check if the backend is available and responding
    async fn is_available(&self) -> bool;

    /// List available models
    async fn list_models(&self) -> Result<Vec<ModelInfo>, LocalModelError>;

    /// Complete a prompt (non-streaming)
    async fn complete(
        &self,
        request: CompletionRequest,
    ) -> Result<CompletionResponse, LocalModelError>;

    /// Stream a completion
    async fn stream(
        &self,
        request: CompletionRequest,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<StreamChunk, LocalModelError>> + Send>>, LocalModelError>;

    /// Backend name for logging/display
    fn name(&self) -> &str;

    /// Default model if none specified
    fn default_model(&self) -> &str;
}
```

### GptOssClient

```rust
// crates/gpt-oss/src/client.rs
pub struct GptOssClient {
    base_url: String,
    http_client: Client,
    default_model: String,
    reasoning_effort: ReasoningEffort,
}

pub struct GptOssClientBuilder {
    base_url: String,
    default_model: String,
    timeout: Duration,
    reasoning_effort: ReasoningEffort,
}

impl GptOssClientBuilder {
    pub fn new() -> Self;
    pub fn base_url(self, url: impl Into<String>) -> Self;
    pub fn model(self, model: impl Into<String>) -> Self;
    pub fn reasoning_effort(self, effort: ReasoningEffort) -> Self;
    pub fn timeout(self, timeout: Duration) -> Self;
    pub fn build(self) -> Result<GptOssClient, LocalModelError>;
}

#[derive(Clone, Copy)]
pub enum ReasoningEffort {
    Low,
    Medium,
    High,
}
```

### Usage Example

```rust
use local_inference::LocalModelBackend;
use gpt_oss::GptOssClient;
use fm_bridge::FMClient;

// Generic function works with any local model
async fn run_inference<B: LocalModelBackend>(backend: &B, prompt: &str) -> Result<String> {
    let request = CompletionRequest {
        messages: vec![ChatMessage::user(prompt)],
        ..Default::default()
    };

    let response = backend.complete(request).await?;
    Ok(response.content())
}

// Use with GPT-OSS
let gpt_oss = GptOssClient::builder()
    .base_url("http://localhost:8000")
    .model("gpt-oss-20b")
    .reasoning_effort(ReasoningEffort::Medium)
    .build()?;

let result = run_inference(&gpt_oss, "Explain FROST signatures").await?;

// Use with Apple FM
let fm = FMClient::new()?;
let result = run_inference(&fm, "Explain FROST signatures").await?;
```

## Testing Strategy

1. **Unit Tests** - Mock HTTP responses for client tests
2. **Integration Tests** - Spin up mock Responses API server
3. **Trait Compliance** - Test both backends against same test suite
4. **E2E Tests** - Real GPT-OSS server (requires GPU, optional)

## Sub-Issues

| ID | Title | Description | Phase |
|----|-------|-------------|-------|
| gpt-oss-001 | Create local-inference trait crate | New crate with LocalModelBackend trait and shared types | v0.1 |
| gpt-oss-002 | Implement GptOssClient | HTTP client with builder pattern for Responses API | v0.2 |
| gpt-oss-003 | Add streaming support | SSE streaming for GPT-OSS completions | v0.2 |
| gpt-oss-004 | Implement LocalModelBackend for GptOssClient | Wire up trait implementation | v0.2 |
| gpt-oss-005 | Refactor fm-bridge to use shared types | Import from local-inference, implement trait | v0.3 |
| gpt-oss-006 | Add reasoning effort support | Low/medium/high effort configuration | v0.2 |
| gpt-oss-007 | Create gpt-oss-agent wrapper | Agent-level abstraction with tool handling | v0.4 |
| gpt-oss-008 | Add browser tool support | Integrate browser tool from gpt-oss | v0.4 |
| gpt-oss-009 | Add python tool support | Integrate python tool from gpt-oss | v0.4 |
| gpt-oss-010 | GUI agent selection | Add gpt-oss to agent dropdown | v0.5 |
| gpt-oss-011 | Local inference config UI | Settings page for model/server config | v0.5 |
| gpt-oss-012 | Autopilot CLI integration | --agent gpt-oss flag | v0.5 |
| gpt-oss-013 | Integration tests | Mock server tests for both backends | v0.6 |
| gpt-oss-014 | Documentation | Usage guide and API docs | v0.6 |

## Notes

### Running GPT-OSS Locally

See `docs/local-inference.md` for detailed benchmarks. Summary:

| Model | Backend | Generation Speed |
|-------|---------|------------------|
| gpt-oss-20b | llama.cpp (GPU) | 104 t/s |
| gpt-oss-120b | llama.cpp (GPU) | 15.9 t/s |
| gpt-oss-120b | Ollama | ~2-3 t/s |

**Use llama.cpp** (5-10x faster than Ollama, no Python dependencies)

```bash
# Start llama-server
# Models already downloaded at ~/models/gpt-oss/
~/code/llama.cpp/build/bin/llama-server \
  -m ~/models/gpt-oss/gpt-oss-20b-mxfp4.gguf \
  --port 8080

# Or with OpenAI-compatible API
~/code/llama.cpp/build/bin/llama-server \
  -m ~/models/gpt-oss/gpt-oss-20b-mxfp4.gguf \
  --port 8080 \
  --api-key "" \
  --chat-template chatml
```

### Environment Variables

```bash
# llama.cpp server URL (default: http://localhost:8080)
LLAMACPP_URL=http://localhost:8080

# Default model path
GPT_OSS_MODEL_PATH=~/models/gpt-oss/gpt-oss-20b-mxfp4.gguf
```

### Harmony Format

GPT-OSS models use the Harmony response format for chain-of-thought. **We have the official Rust crate locally at `~/code/harmony/`** (`openai-harmony` crate). Use it as a git dependency:

```toml
[dependencies]
openai-harmony = { git = "https://github.com/openai/harmony" }
# Or local path during development:
# openai-harmony = { path = "../../../harmony" }
```

Usage:
```rust
use openai_harmony::chat::{Message, Role, Conversation};
use openai_harmony::{HarmonyEncodingName, load_harmony_encoding};

let enc = load_harmony_encoding(HarmonyEncodingName::HarmonyGptOss)?;
let convo = Conversation::from_messages([
    Message::from_role_and_content(Role::User, "Hello!")
]);
let tokens = enc.render_conversation_for_completion(&convo, Role::Assistant, None)?;
// Send tokens to llama-server, get response tokens back
let parsed = enc.parse_messages_from_completion_tokens(response_tokens, Role::Assistant)?;
```

### Native Rust Tools

Tools are implemented in Rust, not using GPT-OSS's Python implementations:

- **browser** — HTTP client with search/open/find, inspired by GPT-OSS's `SimpleBrowserTool`
- **python** — Docker-based code execution sandbox, similar to GPT-OSS's `PythonTool`
- **apply_patch** — File modification tool (already have this pattern in Codex Code)

## Related Directives

- d-017: ACP Integration (agent protocol patterns)
- d-009: Autopilot GUI (UI integration)
- d-012: No Stubs (all implementations must be complete)

## References

- [GPT-OSS Repository](https://github.com/openai/gpt-oss)
- [Harmony Format](https://github.com/openai/harmony)
- [Model Card](https://arxiv.org/abs/2508.10925)
- Local reference: `~/code/gpt-oss/README.md`
