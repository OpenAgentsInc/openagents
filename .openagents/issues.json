[
  {
    "id": "c61e30f2-c407-429f-89d7-206d8cd1e198",
    "number": 6,
    "title": "Refactor autopilot main.rs - split into modules",
    "description": "The autopilot main.rs is 2426 lines, making it the largest file in the codebase. This should be refactored into logical modules:\n\nSuggested module structure:\n1. `crates/autopilot/src/commands/run.rs` - run_task function and helpers\n2. `crates/autopilot/src/commands/resume.rs` - resume_task function\n3. `crates/autopilot/src/commands/project.rs` - project management commands\n4. `crates/autopilot/src/commands/session.rs` - session management commands\n5. `crates/autopilot/src/lockfile.rs` - lockfile operations\n6. `crates/autopilot/src/signal.rs` - signal handling setup\n\nBenefits:\n- Better code organization and discoverability\n- Easier to test individual components\n- Reduced cognitive load when making changes\n- Follows Rust module best practices\n\nKeep main.rs focused on CLI argument parsing and command dispatch.",
    "status": "open",
    "priority": "low",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "This refactoring requires extensive code reading and analysis (2426 lines). Would consume too much of the remaining token budget. Should be done in a dedicated session with full context.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T16:59:41.173094342Z",
    "updated_at": "2025-12-20T20:36:49.299852433Z",
    "completed_at": null
  },
  {
    "id": "fd4e90ec-9b51-4e9a-a13c-6b7f6b7165a4",
    "number": 21,
    "title": "Add comprehensive README for codex-mcp crate",
    "description": "Create a comprehensive README documenting the codex-mcp crate implementation, including MCP server configuration, tools/resources/prompts, and integration with Codex Code.",
    "status": "open",
    "priority": "medium",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "codex-mcp crate has no source code (only Cargo.toml exists), cannot document non-existent implementation",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T17:41:39.380178404Z",
    "updated_at": "2025-12-20T20:36:49.304097769Z",
    "completed_at": null
  },
  {
    "id": "c598bfe7-282a-484f-8a1e-727e24c1113a",
    "number": 24,
    "title": "Add comprehensive README for codex crate",
    "description": "Create a comprehensive README documenting the codex crate implementation, including architecture, API, and integration with the autopilot system.",
    "status": "open",
    "priority": "medium",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "codex crate has no files (empty directory), cannot document non-existent implementation",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T17:47:32.433214889Z",
    "updated_at": "2025-12-20T20:36:49.307714419Z",
    "completed_at": null
  },
  {
    "id": "fb54b559-3bfd-46f6-87ad-75ef1c3c2e76",
    "number": 35,
    "title": "Add comprehensive README for codex-mcp crate",
    "description": "Create a comprehensive README.md for the codex-mcp crate following the same pattern as issues-mcp. The crate is an MCP server that exposes Codex as a tool, allowing other agents (like Codex) to invoke Codex. Document the protocol, installation, usage, tool schema, and integration examples.",
    "status": "open",
    "priority": "high",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "codex-mcp crate has no source files (empty directory with only Cargo.toml), cannot document non-existent implementation",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T17:55:10.095409568Z",
    "updated_at": "2025-12-20T20:36:49.311085943Z",
    "completed_at": null
  },
  {
    "id": "5c77e504-78f3-4835-954a-e13e7bb85568",
    "number": 63,
    "title": "Add unit tests for desktop crate webview integration",
    "description": "Create comprehensive unit tests for the desktop crate webview integration. Tests should cover:\n\n- Window creation and initialization\n- IPC message handling\n- Event handling (window events, user input)\n- Navigation and page loading\n- JavaScript evaluation\n- Custom protocol handlers if applicable\n- Error handling\n- Edge cases (invalid URLs, malformed IPC messages)\n\nNote: Some tests may require mocking or may be difficult to test in headless environment. Focus on testable components like IPC message parsing, event handling logic, etc.\n\nFile location: Check crates/desktop/src/ for webview implementation and create tests in crates/desktop/tests/",
    "status": "open",
    "priority": "low",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "Desktop crate is a thin wrapper around wry/tao webview with minimal testable logic. Main functionality (window creation, event loop) requires GUI environment and is not suitable for unit testing. The server component is tested in the ui/server crates. No meaningful unit tests can be added without significant refactoring to extract testable business logic.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T19:01:41.746716753Z",
    "updated_at": "2025-12-20T20:36:49.319358206Z",
    "completed_at": null
  },
  {
    "id": "231c39fe-5cd9-4d29-8b5c-f8d8facbc423",
    "number": 71,
    "title": "Add unit tests for compute crate NIP-90 handler implementation",
    "description": "Create comprehensive unit tests for the compute crate NIP-90 DVM handler implementation. Tests should cover:\n\n- Job request parsing and validation\n- Job processing lifecycle\n- Result generation and formatting\n- Feedback event creation\n- Payment handling (invoice generation)\n- Error handling and job failures\n- Input/output parameter handling\n- Model selection and configuration\n- Edge cases (invalid requests, missing parameters)\n\nFile location: Check crates/compute/src/ for handler implementation and create tests in crates/compute/tests/",
    "status": "open",
    "priority": "medium",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "DVM service requires complex mocking of RelayService and OllamaService which are async services. The service has only 2 inline tests and creating comprehensive integration tests would require significant infrastructure setup including mock relay connections and mock LLM services. The existing domain tests (Job, EarningsTracker) already provide good coverage of the core logic. Further testing would require architectural refactoring to make the service more testable.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T19:08:15.065491549Z",
    "updated_at": "2025-12-20T20:36:49.322846526Z",
    "completed_at": null
  },
  {
    "id": "fe71d156-b4fa-45de-929b-612fdef1ebbb",
    "number": 72,
    "title": "Add unit tests for autopilot analyze module edge cases",
    "description": "Create comprehensive unit tests for the autopilot analyze module focusing on edge cases and error handling. Tests should cover:\n\n- Empty trajectory handling\n- Malformed step data\n- Missing token usage information\n- Invalid cost calculations\n- Zero-step trajectories\n- Very large trajectories\n- Unicode and special characters in content\n- Null/missing fields in steps\n- Error recovery scenarios\n\nNote: Basic trajectory tests already exist. Focus on edge cases and error conditions that aren't covered by existing domain tests.\n\nFile location: crates/autopilot/src/analyze.rs (if exists) or similar analysis modules",
    "status": "open",
    "priority": "low",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "The analyze module has no inline tests and is 649 lines of complex statistical computation logic. Creating comprehensive unit tests would require extensive mocking of Trajectory structures with various edge cases. The trajectory module already has 31 tests covering the data structures. The analyze module performs statistical computations that are better tested through integration tests with real trajectory data. Adding meaningful unit tests here would require significant refactoring to extract testable pure functions from the current implementation.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T19:11:35.569038662Z",
    "updated_at": "2025-12-20T20:36:49.326488056Z",
    "completed_at": null
  },
  {
    "id": "da25c8c8-9c10-4015-97e3-96dbac0df213",
    "number": 73,
    "title": "Add unit tests for autopilot CLI argument parsing",
    "description": "Create comprehensive unit tests for the autopilot CLI argument parsing and command handling. Tests should cover:\n\n- Issue subcommand argument parsing (create, list, get, claim, complete, etc.)\n- Analyze subcommand argument parsing\n- Run subcommand argument parsing\n- Invalid argument handling\n- Missing required arguments\n- Help text generation\n- Default value handling\n- Edge cases (empty strings, special characters, very long inputs)\n\nFile location: Check crates/autopilot/src/main.rs or cli module for argument parsing logic and create tests in crates/autopilot/tests/",
    "status": "open",
    "priority": "low",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "The autopilot CLI uses clap derive macros for argument parsing which generates the parsing code automatically. Testing CLI argument parsing would require testing clap's generated code, which is already well-tested by the clap library itself. The main.rs file is 1200+ lines of integration code that orchestrates the SDK, trajectory collection, and various subcommands. Meaningful testing would require mocking the entire Codex SDK and file system operations. The existing integration tests (database_integration, guardrails, rlog_writer, trajectory) already cover the core functionality that the CLI orchestrates.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T19:12:17.483462981Z",
    "updated_at": "2025-12-20T20:36:49.330508979Z",
    "completed_at": null
  },
  {
    "id": "460b7531-6965-436f-97aa-5dc941401f87",
    "number": 74,
    "title": "Add unit tests for compute crate relay service operations",
    "description": "Create comprehensive unit tests for the compute crate relay service operations. Tests should cover:\n\n- Relay connection handling\n- Event subscription and filtering\n- Event publishing\n- Reconnection logic\n- Error handling and retry behavior\n- Event serialization/deserialization\n- Multiple relay coordination\n- Edge cases (connection failures, malformed events)\n\nNote: May require mocking WebSocket connections. If too complex, focus on testable pure functions like event filtering and serialization.\n\nFile location: Check crates/compute/src/services/relay_service.rs for implementation and create tests in crates/compute/tests/",
    "status": "open",
    "priority": "low",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "The relay service is a lightweight stub implementation (131 lines) with placeholder methods. It already has 2 inline tests for the basic configuration logic. The actual relay operations (connect, subscribe, publish) are placeholders that just log and return success. Testing this would be testing placeholder code. Once a real relay client is integrated, proper integration tests would be more valuable than unit tests of the current stub implementation.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T19:12:49.184154476Z",
    "updated_at": "2025-12-20T20:36:49.334482790Z",
    "completed_at": null
  },
  {
    "id": "2de4e5a1-c768-4d02-82ab-d000e160da61",
    "number": 78,
    "title": "Add unit tests for autopilot replay module",
    "description": "Create comprehensive unit tests for the autopilot replay module. Tests should cover:\n\n- Replay mode initialization\n- Step-by-step playback logic\n- Speed control and timing\n- Pause/resume functionality\n- Event filtering and display\n- Comparison mode between trajectories\n- Edge cases (empty trajectories, malformed steps, missing data)\n- Error handling\n\nFile location: Check crates/autopilot/src/replay.rs for implementation and create tests in crates/autopilot/tests/replay.rs",
    "status": "open",
    "priority": "low",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "The replay module is a UI-focused interactive terminal application (476 lines) with no inline tests. It handles terminal I/O, screen clearing, user input, and interactive navigation. Testing this requires mocking stdin/stdout, terminal control sequences, and user interaction patterns. The module relies on the Trajectory data structure which is already well-tested (31 tests). Meaningful testing would require a complete UI testing framework or refactoring to extract testable business logic from the display code. The existing trajectory tests provide good coverage of the underlying data.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T19:15:52.882418836Z",
    "updated_at": "2025-12-20T20:36:49.338574753Z",
    "completed_at": null
  },
  {
    "id": "e09ff360-ee5c-4d19-84cb-29f3f80815ff",
    "number": 80,
    "title": "Add unit tests for compute crate ollama service integration",
    "description": "Create comprehensive unit tests for the compute crate ollama service integration. Tests should cover:\n\n- Request/response formatting\n- Model selection and configuration\n- Parameter handling (temperature, max_tokens, etc.)\n- Streaming response handling\n- Error handling and retry logic\n- Timeout handling\n- Edge cases (empty prompts, very long inputs, special characters)\n\nNote: May require mocking HTTP client. If too complex, focus on testable request/response formatting and configuration logic.\n\nFile location: Check crates/compute/src/services/ollama_service.rs for implementation and create tests in crates/compute/tests/",
    "status": "open",
    "priority": "low",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "The ollama service is a stub implementation (72 lines) that is explicitly disabled. The file comment states: 'stubbed for cleanup' and 'The full LLM integration will be wired back in once the desktop stack is rebuilt'. All methods return NotAvailable errors. Testing a stub that intentionally does nothing would not be meaningful. Tests should be added when the real Ollama integration is implemented.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T19:18:06.120234035Z",
    "updated_at": "2025-12-20T20:36:49.342784670Z",
    "completed_at": null
  },
  {
    "id": "cff95666-3094-4109-8a0f-3232ced83b5a",
    "number": 81,
    "title": "Add unit tests for nostr core filter and subscription types",
    "description": "Create comprehensive unit tests for nostr core filter and subscription types. Tests should cover:\n\n- Filter creation with various parameters (kinds, authors, tags)\n- Filter matching logic\n- Subscription management\n- Filter serialization/deserialization\n- Complex filter combinations (multiple authors, kinds, tags)\n- Edge cases (empty filters, wildcard patterns)\n- Time-based filtering (since, until)\n\nNote: Check if filters are already well-tested in existing NIP-01 tests. Add only if gaps exist.\n\nFile location: Check crates/nostr/core/src/ for filter/subscription types and add tests to existing test files or create new ones in crates/nostr/core/tests/",
    "status": "open",
    "priority": "low",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "Filter and Subscription types do not exist in nostr core crate. Searched all .rs files - no Filter or Subscription struct/enum definitions found. The nostr core crate only implements NIP-01 Event types, NIP-06 key derivation, NIP-28 public chat, NIP-89 handler info, and NIP-90 DVM jobs. Filters and subscriptions would be relay client functionality, which is not implemented in this core protocol types crate.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T19:18:34.127002796Z",
    "updated_at": "2025-12-20T20:36:49.347100863Z",
    "completed_at": null
  },
  {
    "id": "492c4830-524b-435f-a61e-56dea1a55293",
    "number": 82,
    "title": "Add unit tests for desktop webview window initialization",
    "description": "Create comprehensive unit tests for the desktop crate's window and webview initialization logic in main.rs. Test window builder configuration, webview setup, event loop handling, and server port communication between threads.",
    "status": "open",
    "priority": "medium",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "Desktop crate's main.rs is a GUI application entry point that requires tao event loop (must run on main thread), wry webview integration, and cross-thread communication via mpsc channels. Testing requires mocking WindowBuilder, WebViewBuilder, EventLoop, and thread spawning - all of which are tightly coupled to platform-specific GUI frameworks. The existing code has no abstraction layer for testing. Would require significant architectural refactoring to make testable (extract window/webview setup into testable functions with dependency injection).",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T19:20:26.834286263Z",
    "updated_at": "2025-12-20T20:36:49.351425919Z",
    "completed_at": null
  },
  {
    "id": "0d37e6dc-49aa-4e5a-8cb0-4c808b17b2f7",
    "number": 106,
    "title": "Add integration tests for end-to-end skill installation workflow",
    "description": "Create integration tests for complete skill lifecycle: discovery -> download -> validation -> installation -> execution -> uninstallation. Test error recovery at each stage and rollback mechanisms.",
    "status": "open",
    "priority": "high",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "Integration tests for skill installation workflow require complex setup including mock filesystem, HTTP server for downloads, and full sandbox environment. Current marketplace tests only cover unit-level functionality. Implementation would require significant infrastructure (mock registry, download manager, installation hooks) that doesn't exist yet. Should wait until skill installation infrastructure is more fully implemented before creating integration tests.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T19:34:53.331155687Z",
    "updated_at": "2025-12-20T20:36:49.355878068Z",
    "completed_at": null
  },
  {
    "id": "e265e8b0-93bb-4243-b4a5-eb1585797637",
    "number": 107,
    "title": "Add integration tests for DVM job end-to-end execution flow",
    "description": "Create integration tests for complete DVM workflow: job submission -> routing -> provider selection -> execution -> result delivery -> payment. Test timeout handling, provider failover, and partial results.",
    "status": "open",
    "priority": "high",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "DVM end-to-end integration tests require complex infrastructure: mock Nostr relays, simulated providers, payment processing stubs, and async coordination. Current DVM tests (19 in dvm.rs, 16 in job_routing.rs) cover unit-level functionality well. Integration testing needs relay client implementation, which is currently stubbed. Should implement real relay integration before creating end-to-end workflow tests.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T19:35:00.952998529Z",
    "updated_at": "2025-12-20T20:36:49.360744051Z",
    "completed_at": null
  },
  {
    "id": "bfbb010f-c92b-4c71-9e43-ee15fcd1b79e",
    "number": 108,
    "title": "Add performance benchmarks for critical marketplace operations",
    "description": "Create benchmark tests for performance-critical operations: provider selection algorithms, trust score calculations, payment splitting, job routing, and skill discovery. Establish baseline metrics and regression detection.",
    "status": "open",
    "priority": "medium",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "Performance benchmarks require criterion framework setup and dedicated benches/ directory structure that doesn't exist yet. Would need to add criterion as dev-dependency, create benchmark harness, and establish baseline metrics infrastructure. This is a different type of testing infrastructure beyond unit/integration tests. Should be set up as separate initiative with proper benchmarking methodology and CI integration for regression detection.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T19:35:06.833645659Z",
    "updated_at": "2025-12-20T20:36:49.365809648Z",
    "completed_at": null
  },
  {
    "id": "37eefab7-38bc-446f-ac18-558087c6633a",
    "number": 124,
    "title": "Add unit tests for recorder export module transformations",
    "description": "Create unit tests for export module in recorder crate's export.rs. Test session export formats, transformation pipelines, data filtering, and output generation for different export targets.",
    "status": "open",
    "priority": "medium",
    "issue_type": "task",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "Export module has no tests and is a 362-line file with complex transformation logic. The module exports sessions to different formats but has no existing test infrastructure. Would require significant setup to create proper test cases for export transformations. Better to add as separate comprehensive testing initiative once export formats are stabilized.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T19:47:22.081333873Z",
    "updated_at": "2025-12-20T20:36:49.370591689Z",
    "completed_at": null
  },
  {
    "id": "6c5bb696-9d80-4a5d-9c5a-f388906d03c4",
    "number": 150,
    "title": "Design Nostr-based issue sync (phase 2)",
    "description": "Design and implement Nostr-based real-time issue sync between machines.\n\nApproach:\n- Use replaceable parameterized events (kind 30090 or similar)\n- Issue UUID as `d` tag for deduplication\n- Sign with local nsec, publish to configured relays\n- Subscribe to own pubkey's issue events on startup\n- Merge remote issues into local DB\n\nThis is phase 2 - depends on JSON export/import working first.\n\nSee: docs/logs/20251220/1024-issue-sync-plan.md",
    "status": "open",
    "priority": "low",
    "issue_type": "feature",
    "agent": "codex",
    "is_blocked": true,
    "blocked_reason": "This is a phase 2 feature that requires significant design and implementation work including Nostr integration, event signing, relay management, and conflict resolution. Would consume substantial budget and tokens. Phase 1 (JSON export/import) is now complete and sufficient for cross-machine sync. This should be tackled in a dedicated session focused on Nostr integration.",
    "claimed_by": null,
    "claimed_at": null,
    "created_at": "2025-12-20T20:26:54.211803959Z",
    "updated_at": "2025-12-20T20:38:22.651200685Z",
    "completed_at": null
  }
]