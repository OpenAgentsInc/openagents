//! Tests validating symbolic recursion per Omar's RLM requirements.
//!
//! Key invariant: The model must NEVER verbalize O(N) sub-prompts.
//! Recursion must be driven by code, not tool calls.
//!
//! > "The way people tend to implement recursive sub-calls or 'sub-agents' don't work.
//! > In particular, you cannot express sub-agents as tool calls."
//! > â€” Omar (DSPy/RLM creator)
//!
//! These tests validate that our RLM implementation follows the correct pattern:
//! 1. Chunking and sub-query generation is done by CODE (programmatically)
//! 2. The LLM only processes individual chunks, never writes O(N) sub-calls
//! 3. Large contexts are handled via pointer references, not embedding

use rlm::chunking::{chunk_by_structure, detect_structure};

/// Validates that chunking is purely programmatic (symbolic recursion).
///
/// Per Omar: "recursion has to be symbolic through code, not tool calls"
///
/// This test generates a large document and verifies that:
/// 1. Chunks are created by code (not by asking an LLM)
/// 2. The number of chunks scales with document size
/// 3. Each chunk references a position range (pointer), not embedded content
#[test]
fn test_chunking_is_symbolic_not_llm_driven() {
    // Generate a large document (1MB)
    let large_content = "Section content here. ".repeat(50_000);

    let structure = detect_structure(&large_content);
    let chunks = chunk_by_structure(&large_content, &structure, 6000, 200);

    // Chunks generated by code, not LLM
    // With 1MB of content and 6000 char chunks, we expect ~170+ chunks
    assert!(
        chunks.len() > 100,
        "Should generate many chunks programmatically, got {}",
        chunks.len()
    );

    // Each chunk is a pointer (start_pos, end_pos), not embedded content
    for chunk in &chunks {
        assert!(
            chunk.start_pos < chunk.end_pos,
            "Chunk should have valid position range"
        );
        assert!(
            chunk.end_pos <= large_content.len(),
            "Chunk end should not exceed document length"
        );
        // Content is available but that's fine - the key is that
        // the chunking decision was made by CODE, not by asking an LLM
    }

    // Verify no gaps or excessive overlaps
    let mut positions: Vec<(usize, usize)> =
        chunks.iter().map(|c| (c.start_pos, c.end_pos)).collect();
    positions.sort_by_key(|p| p.0);

    for window in positions.windows(2) {
        let (_, prev_end) = window[0];
        let (next_start, _) = window[1];
        // With overlap, next_start should be slightly before prev_end
        // but not by too much (200 char overlap configured)
        assert!(
            next_start <= prev_end,
            "Chunks should overlap or be contiguous"
        );
    }
}

/// Validates that chunk generation scales linearly with document size.
///
/// This is a key property of symbolic recursion: the CODE generates O(N) sub-calls,
/// the LLM doesn't need to verbalize them.
#[test]
fn test_chunk_count_scales_with_document_size() {
    let chunk_size = 6000;
    let overlap = 200;

    // Test with different document sizes
    let sizes = [10_000, 100_000, 500_000];
    let mut chunk_counts = vec![];

    for size in sizes {
        let content = "x".repeat(size);
        let structure = detect_structure(&content);
        let chunks = chunk_by_structure(&content, &structure, chunk_size, overlap);
        chunk_counts.push((size, chunks.len()));
    }

    // Verify scaling: 10x content should produce ~10x chunks
    let (size_small, count_small) = chunk_counts[0];
    let (size_large, count_large) = chunk_counts[2];

    let size_ratio = size_large as f64 / size_small as f64;
    let count_ratio = count_large as f64 / count_small as f64;

    // Allow some variance due to overlap and boundary effects
    assert!(
        count_ratio > size_ratio * 0.5 && count_ratio < size_ratio * 2.0,
        "Chunk count should scale roughly linearly with document size. \
         Size ratio: {:.1}x, Count ratio: {:.1}x",
        size_ratio,
        count_ratio
    );
}

/// Validates that chunks reference positions (pointers), not embedded content in prompts.
///
/// Per Omar: "prompts/requests accessible through pointers as an object"
#[test]
fn test_chunks_are_position_references() {
    let content = "Line one.\nLine two.\nLine three.\nLine four.\nLine five.";

    let structure = detect_structure(content);
    // Use small chunk size to force multiple chunks
    let chunks = chunk_by_structure(content, &structure, 15, 5);

    for chunk in &chunks {
        // Verify the chunk content matches what's at those positions
        let actual_content = &content[chunk.start_pos..chunk.end_pos];
        assert_eq!(
            chunk.content, actual_content,
            "Chunk content should match position-based slice"
        );

        // The position metadata is the "pointer" - it references WHERE the content is,
        // not just what it is
        assert!(chunk.start_pos < content.len());
        assert!(chunk.end_pos <= content.len());
    }
}

/// Validates that the orchestrator pattern generates sub-queries programmatically.
///
/// This is the correct RLM pattern: code generates N sub-queries without
/// requiring the LLM to verbalize them.
#[test]
fn test_orchestrator_pattern_is_symbolic() {
    // Simulate what the orchestrator does: generate queries for each chunk
    let content = "Section A content here. ".repeat(1000);
    let structure = detect_structure(&content);
    let chunks = chunk_by_structure(&content, &structure, 500, 50);

    // In the orchestrator, we'd generate a query for each chunk
    // This happens in CODE, not by asking an LLM
    let queries: Vec<String> = chunks
        .iter()
        .map(|chunk| {
            format!(
                "Analyze chunk {} (chars {}-{}): {}",
                chunk.id,
                chunk.start_pos,
                chunk.end_pos,
                // In real usage, we'd pass chunk.content to the LLM
                // but the QUERY GENERATION is done by code
                &chunk.content[..50.min(chunk.content.len())]
            )
        })
        .collect();

    // We generated O(N) queries programmatically
    assert_eq!(queries.len(), chunks.len());
    assert!(
        queries.len() > 10,
        "Should generate many queries for large content"
    );

    // Each query was generated by CODE, not by asking an LLM to write them
    // This is the key insight from Omar: the recursion is SYMBOLIC
}

/// Test that demonstrates the anti-pattern we removed (llm_query parsing).
///
/// The old pattern expected the LLM to write code like:
/// ```python
/// result1 = llm_query("summarize", context[0:1000])
/// result2 = llm_query("summarize", context[1000:2000])
/// # ... N more calls
/// ```
///
/// This is WRONG because:
/// 1. The LLM can't verbalize O(N) sub-prompts for large N
/// 2. We'd need to fit the entire prompt in context to generate the calls
///
/// Instead, we use the orchestrator pattern where CODE generates the sub-calls.
#[test]
fn test_no_llm_verbalized_subqueries() {
    // The key property: we don't parse llm_query() calls from LLM output
    // Instead, code generates sub-queries based on document structure

    let content = "Test content ".repeat(10_000);
    let structure = detect_structure(&content);
    let chunks = chunk_by_structure(&content, &structure, 1000, 100);

    // With 100k chars and 1000 char chunks, we get ~100 chunks
    // The LLM could NEVER write 100 llm_query() calls in its output
    // But code can easily generate 100 sub-queries
    assert!(
        chunks.len() > 50,
        "Should have many chunks that code generates, LLM could never verbalize this many"
    );
}
