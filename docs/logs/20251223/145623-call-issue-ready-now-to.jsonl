{"type":"system","message":{"subtype":"init","agents":["general-purpose","statusline-setup","Explore","Plan"],"apiKeySource":"none","betas":null,"claude_code_version":"2.0.76","cwd":"/home/christopherdavid/code/openagents","tools":["Task","TaskOutput","Bash","Glob","Grep","ExitPlanMode","Read","Edit","Write","NotebookEdit","WebFetch","TodoWrite","WebSearch","KillShell","AskUserQuestion","Skill","EnterPlanMode","LSP","mcp__effect-docs__effect_docs_search","mcp__effect-docs__get_effect_doc","ListMcpResourcesTool","ReadMcpResourceTool"],"mcp_servers":[{"name":"effect-docs","status":"connected"},{"name":"issues","status":"failed"}],"model":"claude-sonnet-4-5-20250929","permissionMode":"bypassPermissions","slash_commands":["codex","compact","context","cost","init","pr-comments","release-notes","review","security-review"],"output_style":"default","skills":["codex"],"plugins":[{"name":"rust-analyzer-lsp","path":"/home/christopherdavid/.claude/plugins/cache/claude-plugins-official/rust-analyzer-lsp/1.0.0"}],"uuid":"5514196d-a39c-4f5b-849f-18b21b145d3b","session_id":"4deb941e-5b45-412c-b561-7c7d3e2acabd"},"timestamp":"2025-12-23T20:56:54.199634619+00:00"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_0185euZ2mYJqcGVgJ3CmZPWm","type":"message","role":"assistant","content":[{"type":"text","text":"I'll start the autonomous loop by calling issue_ready to get the first issue."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":12028,"cache_read_input_tokens":15479,"cache_creation":{"ephemeral_5m_input_tokens":12028,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"timestamp":"2025-12-23T20:56:57.888718790+00:00"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_0185euZ2mYJqcGVgJ3CmZPWm","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01REYpNkFpBbEJ6LK8YDYy3z","name":"Bash","input":{"command":"cargo run --bin issue_ready 2>&1","description":"Get first ready issue from autopilot"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":12028,"cache_read_input_tokens":15479,"cache_creation":{"ephemeral_5m_input_tokens":12028,"ephemeral_1h_input_tokens":0},"output_tokens":120,"service_tier":"standard"},"context_management":null},"timestamp":"2025-12-23T20:56:59.629148738+00:00"}
{"type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"Exit code 101\nerror: no bin target named `issue_ready` in default-run packages\nhelp: available bin targets:\n    openagents","is_error":true,"tool_use_id":"toolu_01REYpNkFpBbEJ6LK8YDYy3z"}]},"timestamp":"2025-12-23T20:57:01.529086147+00:00"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_014S8FQT9rfKkeBN72xbd6Na","type":"message","role":"assistant","content":[{"type":"text","text":"Let me check the correct way to call the issue_ready function:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":153,"cache_read_input_tokens":27507,"cache_creation":{"ephemeral_5m_input_tokens":153,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"timestamp":"2025-12-23T20:57:04.544612669+00:00"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_014S8FQT9rfKkeBN72xbd6Na","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01B8hW2EUU9M95UABEPbLPie","name":"Bash","input":{"command":"cargo run --bin openagents -- autopilot issue ready 2>&1","description":"Get first ready issue via openagents CLI"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":153,"cache_read_input_tokens":27507,"cache_creation":{"ephemeral_5m_input_tokens":153,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"timestamp":"2025-12-23T20:57:05.402334746+00:00"}
{"type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"Exit code 2\n   Compiling openssl-sys v0.9.111\n   Compiling secp256k1 v0.29.1\n   Compiling ring v0.17.14\n   Compiling rusqlite v0.32.1\n   Compiling actix-http v3.11.2\n   Compiling spark v0.1.0 (/home/christopherdavid/code/spark-sdk/crates/spark)\n   Compiling libz-sys v1.1.23\n   Compiling stacker v0.1.22\n   Compiling libssh2-sys v0.3.1\n   Compiling chumsky v0.9.3\n   Compiling bitcoin v0.32.8\n   Compiling nostr v0.43.1\n   Compiling openssl v0.10.75\n   Compiling rusqlite_migration v1.3.1\n   Compiling issues v0.1.0 (/home/christopherdavid/code/openagents/crates/issues)\n   Compiling libgit2-sys v0.18.3+1.9.2\n   Compiling git2 v0.20.3\n   Compiling rustls-webpki v0.103.8\n   Compiling hickory-proto v0.25.2\n   Compiling rustls v0.23.35\n   Compiling actix-web v4.12.1\n   Compiling native-tls v0.2.14\n   Compiling tokio-native-tls v0.3.1\n   Compiling tungstenite v0.24.0\n   Compiling lettre v0.11.19\n   Compiling hyper-tls v0.6.0\n   Compiling lnurl-models v0.1.0 (/home/christopherdavid/code/spark-sdk/crates/breez-sdk/lnurl-models)\n   Compiling tokio-tungstenite v0.24.0\n   Compiling lightning-types v0.2.0\n   Compiling nostr v0.1.0 (/home/christopherdavid/code/openagents/crates/nostr/core)\n   Compiling lightning-invoice v0.33.2\n   Compiling lightning v0.1.8\n   Compiling maud v0.26.0\n   Compiling actix-ws v0.3.0\n   Compiling actix-web-httpauth v0.8.2\n   Compiling ui v0.1.0 (/home/christopherdavid/code/openagents/crates/ui)\n   Compiling auth v0.1.0 (/home/christopherdavid/code/openagents/crates/auth)\n   Compiling hickory-resolver v0.25.2\n   Compiling tokio-rustls v0.26.4\n   Compiling hyper-rustls v0.27.7\n   Compiling tonic v0.12.3\n   Compiling reqwest v0.12.27\n   Compiling nostr-client v0.1.0 (/home/christopherdavid/code/openagents/crates/nostr/client)\n   Compiling frostr v0.1.0 (/home/christopherdavid/code/openagents/crates/frostr)\n   Compiling marketplace v0.1.0 (/home/christopherdavid/code/openagents/crates/marketplace)\n   Compiling spark-wallet v0.1.0 (/home/christopherdavid/code/spark-sdk/crates/spark-wallet)\n   Compiling breez-sdk-common v0.1.0 (/home/christopherdavid/code/spark-sdk/crates/breez-sdk/common)\n   Compiling breez-sdk-spark v0.1.0 (/home/christopherdavid/code/spark-sdk/crates/breez-sdk/core)\n   Compiling openagents-spark v0.1.0 (/home/christopherdavid/code/openagents/crates/spark)\n   Compiling wallet v0.1.0 (/home/christopherdavid/code/openagents/crates/wallet)\n   Compiling autopilot v0.1.0 (/home/christopherdavid/code/openagents/crates/autopilot)\n   Compiling gitafter v0.1.0 (/home/christopherdavid/code/openagents/crates/gitafter)\n   Compiling openagents v0.1.0 (/home/christopherdavid/code/openagents)\nwarning: function `render_agents_table` is never used\n   --> src/gui/routes/parallel.rs:296:4\n    |\n296 | fn render_agents_table(agents: &[autopilot::parallel::AgentInfo]) -> Markup {\n    |    ^^^^^^^^^^^^^^^^^^^\n    |\n    = note: `#[warn(dead_code)]` (part of `#[warn(unused)]`) on by default\n\nwarning: `openagents` (bin \"openagents\") generated 1 warning\n    Finished `dev` profile [optimized + debuginfo] target(s) in 24.21s\n     Running `target/debug/openagents autopilot issue ready`\nerror: unrecognized subcommand 'ready'\n\n  tip: a similar subcommand exists: 'create'\n\nUsage: openagents autopilot issue [OPTIONS] <COMMAND>\n\nFor more information, try '--help'.","is_error":true,"tool_use_id":"toolu_01B8hW2EUU9M95UABEPbLPie"}]},"timestamp":"2025-12-23T20:57:30.758122933+00:00"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01UsJyaporHkrd8yLuLDXQXX","type":"message","role":"assistant","content":[{"type":"text","text":"Let me check the correct autopilot command structure:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":1522,"cache_read_input_tokens":27660,"cache_creation":{"ephemeral_5m_input_tokens":1522,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"timestamp":"2025-12-23T20:57:33.481870898+00:00"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01UsJyaporHkrd8yLuLDXQXX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01AbpYAaX3eMf5xMQrq1vSnN","name":"Bash","input":{"command":"cargo run --bin openagents -- autopilot issue --help","description":"Check available autopilot issue commands"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":1522,"cache_read_input_tokens":27660,"cache_creation":{"ephemeral_5m_input_tokens":1522,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"timestamp":"2025-12-23T20:57:34.151058779+00:00"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01AbpYAaX3eMf5xMQrq1vSnN","type":"tool_result","content":"Issue management\n\nUsage: openagents autopilot issue [OPTIONS] <COMMAND>\n\nCommands:\n  list      List issues\n  create    Create a new issue\n  show      Show issue details\n  claim     Claim an issue for work\n  complete  Mark issue as complete\n  help      Print this message or the help of the given subcommand(s)\n\nOptions:\n  -v, --verbose  Enable verbose logging\n  -h, --help     Print help\n    Blocking waiting for file lock on build directory\n   Compiling openssl-sys v0.9.111\n   Compiling secp256k1-sys v0.10.1\n   Compiling ring v0.17.14\n   Compiling libz-sys v1.1.23\n   Compiling zstd-sys v2.0.16+zstd.1.5.7\n   Compiling libsqlite3-sys v0.30.1\n   Compiling psm v0.1.28\n   Compiling openssl v0.10.75\n   Compiling native-tls v0.2.14\n   Compiling libgit2-sys v0.18.3+1.9.2\n   Compiling libssh2-sys v0.3.1\n   Compiling stacker v0.1.22\n   Compiling chumsky v0.9.3\n   Compiling git2 v0.20.3\n   Compiling rustls v0.23.35\n   Compiling built v0.8.0\n   Compiling spark v0.1.0 (/home/christopherdavid/code/spark-sdk/crates/spark)\n   Compiling breez-sdk-spark v0.1.0 (/home/christopherdavid/code/spark-sdk/crates/breez-sdk/core)\n   Compiling rustls-webpki v0.103.8\n   Compiling hickory-proto v0.25.2\n   Compiling tokio-native-tls v0.3.1\n   Compiling tungstenite v0.24.0\n   Compiling lettre v0.11.19\n   Compiling hyper-tls v0.6.0\n   Compiling secp256k1 v0.29.1\n   Compiling tokio-tungstenite v0.24.0\n   Compiling bitcoin v0.32.8\n   Compiling nostr v0.43.1\n   Compiling hickory-resolver v0.25.2\n   Compiling lnurl-models v0.1.0 (/home/christopherdavid/code/spark-sdk/crates/breez-sdk/lnurl-models)\n   Compiling tokio-rustls v0.26.4\n   Compiling hyper-rustls v0.27.7\n   Compiling tonic v0.12.3\n   Compiling reqwest v0.12.27\n   Compiling lightning-types v0.2.0\n   Compiling nostr v0.1.0 (/home/christopherdavid/code/openagents/crates/nostr/core)\n   Compiling lightning-invoice v0.33.2\n   Compiling lightning v0.1.8\n   Compiling zstd-safe v7.2.4\n   Compiling zstd v0.13.3\n   Compiling actix-http v3.11.2\n   Compiling actix-web v4.12.1\n   Compiling spark-wallet v0.1.0 (/home/christopherdavid/code/spark-sdk/crates/spark-wallet)\n   Compiling maud v0.26.0\n   Compiling actix-ws v0.3.0\n   Compiling actix-web-httpauth v0.8.2\n   Compiling ui v0.1.0 (/home/christopherdavid/code/openagents/crates/ui)\n   Compiling auth v0.1.0 (/home/christopherdavid/code/openagents/crates/auth)\n   Compiling rusqlite v0.32.1\n   Compiling breez-sdk-common v0.1.0 (/home/christopherdavid/code/spark-sdk/crates/breez-sdk/common)\n   Compiling nostr-client v0.1.0 (/home/christopherdavid/code/openagents/crates/nostr/client)\n   Compiling rusqlite_migration v1.3.1\n   Compiling issues v0.1.0 (/home/christopherdavid/code/openagents/crates/issues)\n   Compiling frostr v0.1.0 (/home/christopherdavid/code/openagents/crates/frostr)\n   Compiling marketplace v0.1.0 (/home/christopherdavid/code/openagents/crates/marketplace)\n   Compiling openagents-spark v0.1.0 (/home/christopherdavid/code/openagents/crates/spark)\n   Compiling wallet v0.1.0 (/home/christopherdavid/code/openagents/crates/wallet)\n   Compiling autopilot v0.1.0 (/home/christopherdavid/code/openagents/crates/autopilot)\n   Compiling gitafter v0.1.0 (/home/christopherdavid/code/openagents/crates/gitafter)\n   Compiling openagents v0.1.0 (/home/christopherdavid/code/openagents)\nwarning: function `render_agents_table` is never used\n   --> src/gui/routes/parallel.rs:296:4\n    |\n296 | fn render_agents_table(agents: &[autopilot::parallel::AgentInfo]) -> Markup {\n    |    ^^^^^^^^^^^^^^^^^^^\n    |\n    = note: `#[warn(dead_code)]` (part of `#[warn(unused)]`) on by default\n\nwarning: `openagents` (bin \"openagents\") generated 1 warning\n    Finished `dev` profile [optimized + debuginfo] target(s) in 34.06s\n     Running `target/debug/openagents autopilot issue --help`","is_error":false}]},"timestamp":"2025-12-23T20:58:09.133305008+00:00"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_015SYc8MrQ69kaA97aePEP2P","type":"message","role":"assistant","content":[{"type":"text","text":"I need to check the autopilot analyze module to understand how issue_ready works:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":1681,"cache_read_input_tokens":29182,"cache_creation":{"ephemeral_5m_input_tokens":1681,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"timestamp":"2025-12-23T20:58:11.942161803+00:00"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_015SYc8MrQ69kaA97aePEP2P","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_015aMd8i3cQrbdoythsTowZk","name":"Read","input":{"file_path":"/home/christopherdavid/code/openagents/crates/autopilot/src/analyze.rs"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":1681,"cache_read_input_tokens":29182,"cache_creation":{"ephemeral_5m_input_tokens":1681,"ephemeral_1h_input_tokens":0},"output_tokens":98,"service_tier":"standard"},"context_management":null},"timestamp":"2025-12-23T20:58:12.402266052+00:00"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_015aMd8i3cQrbdoythsTowZk","type":"tool_result","content":"     1→//! Trajectory analysis and metrics computation for autopilot performance monitoring.\n     2→//!\n     3→//! This module provides comprehensive analysis of autopilot execution trajectories,\n     4→//! extracting performance, cost, quality, and error metrics from trajectory logs.\n     5→//! Used by the `cargo autopilot analyze` command to generate reports and detect\n     6→//! regressions.\n     7→//!\n     8→//! # Key Types\n     9→//!\n    10→//! - [`TrajectoryAnalysis`]: Complete analysis results for a single trajectory\n    11→//! - [`PerformanceMetrics`]: Timing and duration measurements\n    12→//! - [`CostMetrics`]: Token usage and API cost calculations\n    13→//! - [`ErrorMetrics`]: Tool errors and failure patterns\n    14→//! - [`QualityMetrics`]: Task completion and success indicators\n    15→//! - [`ToolUsageMetrics`]: Per-tool usage statistics\n    16→//!\n    17→//! # Usage\n    18→//!\n    19→//! ```no_run\n    20→//! use autopilot::analyze::analyze_trajectory_file;\n    21→//! use std::path::Path;\n    22→//!\n    23→//! let trajectory_path = Path::new(\"docs/logs/20251222/094938-task.json\");\n    24→//! let analysis = analyze_trajectory_file(trajectory_path)?;\n    25→//!\n    26→//! println!(\"Duration: {}ms\", analysis.performance.total_duration_ms);\n    27→//! println!(\"Tokens: {}\", analysis.cost.total_tokens);\n    28→//! println!(\"Error rate: {:.1}%\", analysis.errors.error_rate * 100.0);\n    29→//! # Ok::<(), Box<dyn std::error::Error>>(())\n    30→//! ```\n    31→//!\n    32→//! # Related Modules\n    33→//!\n    34→//! - [`crate::dashboard`]: Web UI for viewing analysis results\n    35→//! - [`crate::trajectory`]: Trajectory log parsing and types\n    36→//! - [`crate::learning`]: Apply learnings from analysis\n    37→\n    38→use std::collections::HashMap;\n    39→use std::path::Path;\n    40→\n    41→use chrono::{DateTime, Utc};\n    42→use colored::*;\n    43→use serde::{Deserialize, Serialize};\n    44→\n    45→use crate::trajectory::{StepType, Trajectory};\n    46→\n    47→/// Complete analysis of a single trajectory\n    48→#[derive(Debug, Clone, Serialize, Deserialize)]\n    49→pub struct TrajectoryAnalysis {\n    50→    pub session_id: String,\n    51→    pub model: String,\n    52→    pub prompt_preview: String,\n    53→    pub started_at: DateTime<Utc>,\n    54→    pub ended_at: Option<DateTime<Utc>>,\n    55→    pub performance: PerformanceMetrics,\n    56→    pub cost: CostMetrics,\n    57→    pub errors: ErrorMetrics,\n    58→    pub quality: QualityMetrics,\n    59→    pub tool_usage: ToolUsageMetrics,\n    60→}\n    61→\n    62→/// Performance timing metrics\n    63→#[derive(Debug, Clone, Default, Serialize, Deserialize)]\n    64→pub struct PerformanceMetrics {\n    65→    pub total_duration_ms: u64,\n    66→    pub tool_latency_stats: LatencyStats,\n    67→    pub parallel_tool_batches: u32,\n    68→    pub total_tool_calls: u32,\n    69→}\n    70→\n    71→/// Latency statistics for distributions\n    72→#[derive(Debug, Clone, Default, Serialize, Deserialize)]\n    73→pub struct LatencyStats {\n    74→    pub min_ms: u64,\n    75→    pub max_ms: u64,\n    76→    pub mean_ms: f64,\n    77→    pub p50_ms: u64,\n    78→    pub p95_ms: u64,\n    79→    pub count: usize,\n    80→}\n    81→\n    82→/// Cost and token metrics\n    83→#[derive(Debug, Clone, Default, Serialize, Deserialize)]\n    84→pub struct CostMetrics {\n    85→    pub total_cost_usd: f64,\n    86→    pub input_tokens: u64,\n    87→    pub output_tokens: u64,\n    88→    pub cache_read_tokens: u64,\n    89→    pub cache_hit_rate: f64,\n    90→    pub tokens_by_step_type: HashMap<String, u64>,\n    91→}\n    92→\n    93→/// Error and failure metrics\n    94→#[derive(Debug, Clone, Default, Serialize, Deserialize)]\n    95→pub struct ErrorMetrics {\n    96→    pub success: bool,\n    97→    pub total_tool_calls: u32,\n    98→    pub failed_tool_calls: u32,\n    99→    pub tool_error_rate: f64,\n   100→    pub errors_by_tool: HashMap<String, u32>,\n   101→}\n   102→\n   103→/// Quality and behavioral metrics\n   104→#[derive(Debug, Clone, Default, Serialize, Deserialize)]\n   105→pub struct QualityMetrics {\n   106→    pub num_turns: u32,\n   107→    pub thinking_blocks: u32,\n   108→    pub avg_thinking_length: f64,\n   109→    pub tool_diversity: f64,\n   110→    pub unique_tools: u32,\n   111→}\n   112→\n   113→/// Tool usage breakdown\n   114→#[derive(Debug, Clone, Default, Serialize, Deserialize)]\n   115→pub struct ToolUsageMetrics {\n   116→    pub total_calls: u32,\n   117→    pub calls_by_tool: HashMap<String, u32>,\n   118→    pub success_rate_by_tool: HashMap<String, f64>,\n   119→    pub most_used_tool: Option<String>,\n   120→}\n   121→\n   122→/// Aggregated metrics across multiple trajectories\n   123→#[derive(Debug, Clone, Default, Serialize, Deserialize)]\n   124→pub struct AggregateAnalysis {\n   125→    pub trajectory_count: usize,\n   126→    pub date_range: Option<(DateTime<Utc>, DateTime<Utc>)>,\n   127→    pub by_model: HashMap<String, ModelStats>,\n   128→    pub total_cost_usd: f64,\n   129→    pub avg_cost_usd: f64,\n   130→    pub avg_duration_ms: f64,\n   131→    pub overall_success_rate: f64,\n   132→    pub avg_tool_error_rate: f64,\n   133→    pub avg_cache_hit_rate: f64,\n   134→    pub total_tool_calls: u32,\n   135→    pub top_error_tools: Vec<(String, u32)>,\n   136→}\n   137→\n   138→#[derive(Debug, Clone, Default, Serialize, Deserialize)]\n   139→pub struct ModelStats {\n   140→    pub count: usize,\n   141→    pub avg_cost: f64,\n   142→    pub avg_duration_ms: f64,\n   143→    pub success_rate: f64,\n   144→}\n   145→\n   146→/// Load a trajectory from a JSON file\n   147→pub fn load_trajectory(path: &Path) -> anyhow::Result<Trajectory> {\n   148→    let content = std::fs::read_to_string(path)?;\n   149→    let trajectory: Trajectory = serde_json::from_str(&content)?;\n   150→    Ok(trajectory)\n   151→}\n   152→\n   153→/// Analyze a single trajectory\n   154→pub fn analyze_trajectory(trajectory: &Trajectory) -> TrajectoryAnalysis {\n   155→    let performance = compute_performance(trajectory);\n   156→    let cost = compute_cost(trajectory);\n   157→    let errors = compute_errors(trajectory);\n   158→    let quality = compute_quality(trajectory);\n   159→    let tool_usage = compute_tool_usage(trajectory);\n   160→\n   161→    TrajectoryAnalysis {\n   162→        session_id: trajectory.session_id.clone(),\n   163→        model: trajectory.model.clone(),\n   164→        prompt_preview: truncate(&trajectory.prompt, 80),\n   165→        started_at: trajectory.started_at,\n   166→        ended_at: trajectory.ended_at,\n   167→        performance,\n   168→        cost,\n   169→        errors,\n   170→        quality,\n   171→        tool_usage,\n   172→    }\n   173→}\n   174→\n   175→/// Compute performance metrics\n   176→fn compute_performance(trajectory: &Trajectory) -> PerformanceMetrics {\n   177→    let total_duration_ms = trajectory\n   178→        .result\n   179→        .as_ref()\n   180→        .map(|r| r.duration_ms)\n   181→        .unwrap_or(0);\n   182→\n   183→    // Track tool call timestamps to compute latencies\n   184→    let mut pending_calls: HashMap<String, DateTime<Utc>> = HashMap::new();\n   185→    let mut latencies: Vec<u64> = Vec::new();\n   186→    let mut tool_intervals: Vec<(DateTime<Utc>, DateTime<Utc>)> = Vec::new();\n   187→    let mut total_tool_calls = 0u32;\n   188→\n   189→    for step in &trajectory.steps {\n   190→        match &step.step_type {\n   191→            StepType::ToolCall { tool_id, .. } => {\n   192→                total_tool_calls += 1;\n   193→                pending_calls.insert(tool_id.clone(), step.timestamp);\n   194→            }\n   195→            StepType::ToolResult { tool_id, .. } => {\n   196→                if let Some(start_time) = pending_calls.remove(tool_id) {\n   197→                    let latency = (step.timestamp - start_time).num_milliseconds().max(0) as u64;\n   198→                    latencies.push(latency);\n   199→                    tool_intervals.push((start_time, step.timestamp));\n   200→                }\n   201→            }\n   202→            _ => {}\n   203→        }\n   204→    }\n   205→\n   206→    let tool_latency_stats = compute_latency_stats(&latencies);\n   207→    let parallel_tool_batches = detect_parallel_batches(&tool_intervals);\n   208→\n   209→    PerformanceMetrics {\n   210→        total_duration_ms,\n   211→        tool_latency_stats,\n   212→        parallel_tool_batches,\n   213→        total_tool_calls,\n   214→    }\n   215→}\n   216→\n   217→/// Compute latency statistics from a list of latencies\n   218→fn compute_latency_stats(latencies: &[u64]) -> LatencyStats {\n   219→    if latencies.is_empty() {\n   220→        return LatencyStats::default();\n   221→    }\n   222→\n   223→    let mut sorted = latencies.to_vec();\n   224→    sorted.sort();\n   225→\n   226→    let count = sorted.len();\n   227→    let sum: u64 = sorted.iter().sum();\n   228→\n   229→    LatencyStats {\n   230→        min_ms: sorted[0],\n   231→        max_ms: sorted[count - 1],\n   232→        mean_ms: sum as f64 / count as f64,\n   233→        p50_ms: sorted[count / 2],\n   234→        p95_ms: sorted[(count * 95 / 100).min(count - 1)],\n   235→        count,\n   236→    }\n   237→}\n   238→\n   239→/// Detect number of parallel tool batches (overlapping tool calls)\n   240→fn detect_parallel_batches(intervals: &[(DateTime<Utc>, DateTime<Utc>)]) -> u32 {\n   241→    if intervals.is_empty() {\n   242→        return 0;\n   243→    }\n   244→\n   245→    let mut sorted: Vec<_> = intervals.to_vec();\n   246→    sorted.sort_by_key(|(start, _)| *start);\n   247→\n   248→    let mut batches = 1u32;\n   249→    let mut current_batch_end = sorted[0].1;\n   250→\n   251→    for (start, end) in sorted.iter().skip(1) {\n   252→        if *start > current_batch_end {\n   253→            // New batch - no overlap\n   254→            batches += 1;\n   255→            current_batch_end = *end;\n   256→        } else {\n   257→            // Overlapping - extend batch\n   258→            current_batch_end = std::cmp::max(current_batch_end, *end);\n   259→        }\n   260→    }\n   261→\n   262→    batches\n   263→}\n   264→\n   265→/// Compute cost metrics\n   266→fn compute_cost(trajectory: &Trajectory) -> CostMetrics {\n   267→    let usage = &trajectory.usage;\n   268→\n   269→    // Compute cache hit rate\n   270→    let total_input = usage.input_tokens + usage.cache_read_tokens;\n   271→    let cache_hit_rate = if total_input > 0 {\n   272→        usage.cache_read_tokens as f64 / total_input as f64\n   273→    } else {\n   274→        0.0\n   275→    };\n   276→\n   277→    // Tokens by step type\n   278→    let mut tokens_by_step_type: HashMap<String, u64> = HashMap::new();\n   279→\n   280→    for step in &trajectory.steps {\n   281→        let step_name = match &step.step_type {\n   282→            StepType::Thinking { .. } => \"thinking\",\n   283→            StepType::Assistant { .. } => \"assistant\",\n   284→            StepType::ToolCall { .. } => \"tool_call\",\n   285→            StepType::ToolResult { .. } => \"tool_result\",\n   286→            StepType::User { .. } => \"user\",\n   287→            StepType::SystemInit { .. } => \"system_init\",\n   288→            StepType::SystemStatus { .. } => \"system_status\",\n   289→            StepType::Subagent { .. } => \"subagent\",\n   290→        };\n   291→\n   292→        let entry = tokens_by_step_type.entry(step_name.to_string()).or_insert(0);\n   293→        if let Some(t) = step.tokens_out {\n   294→            *entry += t;\n   295→        }\n   296→    }\n   297→\n   298→    CostMetrics {\n   299→        total_cost_usd: usage.cost_usd,\n   300→        input_tokens: usage.input_tokens,\n   301→        output_tokens: usage.output_tokens,\n   302→        cache_read_tokens: usage.cache_read_tokens,\n   303→        cache_hit_rate,\n   304→        tokens_by_step_type,\n   305→    }\n   306→}\n   307→\n   308→/// Compute error metrics\n   309→fn compute_errors(trajectory: &Trajectory) -> ErrorMetrics {\n   310→    let mut total_tool_calls = 0u32;\n   311→    let mut failed_tool_calls = 0u32;\n   312→    let mut errors_by_tool: HashMap<String, u32> = HashMap::new();\n   313→    let mut pending_tools: HashMap<String, String> = HashMap::new(); // tool_id -> tool_name\n   314→\n   315→    for step in &trajectory.steps {\n   316→        match &step.step_type {\n   317→            StepType::ToolCall { tool, tool_id, .. } => {\n   318→                total_tool_calls += 1;\n   319→                pending_tools.insert(tool_id.clone(), tool.clone());\n   320→            }\n   321→            StepType::ToolResult { tool_id, success, .. } => {\n   322→                if !*success {\n   323→                    failed_tool_calls += 1;\n   324→                    if let Some(tool_name) = pending_tools.get(tool_id) {\n   325→                        *errors_by_tool.entry(tool_name.clone()).or_insert(0) += 1;\n   326→                    }\n   327→                }\n   328→            }\n   329→            _ => {}\n   330→        }\n   331→    }\n   332→\n   333→    let tool_error_rate = if total_tool_calls > 0 {\n   334→        failed_tool_calls as f64 / total_tool_calls as f64\n   335→    } else {\n   336→        0.0\n   337→    };\n   338→\n   339→    ErrorMetrics {\n   340→        success: trajectory.result.as_ref().map(|r| r.success).unwrap_or(false),\n   341→        total_tool_calls,\n   342→        failed_tool_calls,\n   343→        tool_error_rate,\n   344→        errors_by_tool,\n   345→    }\n   346→}\n   347→\n   348→/// Compute quality metrics\n   349→fn compute_quality(trajectory: &Trajectory) -> QualityMetrics {\n   350→    let mut thinking_blocks = 0u32;\n   351→    let mut thinking_lengths: Vec<usize> = Vec::new();\n   352→    let mut unique_tools: std::collections::HashSet<String> = std::collections::HashSet::new();\n   353→    let mut tool_call_count = 0u32;\n   354→\n   355→    for step in &trajectory.steps {\n   356→        match &step.step_type {\n   357→            StepType::Thinking { content, .. } => {\n   358→                thinking_blocks += 1;\n   359→                thinking_lengths.push(content.len());\n   360→            }\n   361→            StepType::ToolCall { tool, .. } => {\n   362→                tool_call_count += 1;\n   363→                unique_tools.insert(tool.clone());\n   364→            }\n   365→            _ => {}\n   366→        }\n   367→    }\n   368→\n   369→    let avg_thinking_length = if thinking_lengths.is_empty() {\n   370→        0.0\n   371→    } else {\n   372→        thinking_lengths.iter().sum::<usize>() as f64 / thinking_lengths.len() as f64\n   373→    };\n   374→\n   375→    let tool_diversity = if tool_call_count > 0 {\n   376→        unique_tools.len() as f64 / tool_call_count as f64\n   377→    } else {\n   378→        0.0\n   379→    };\n   380→\n   381→    QualityMetrics {\n   382→        num_turns: trajectory.result.as_ref().map(|r| r.num_turns).unwrap_or(0),\n   383→        thinking_blocks,\n   384→        avg_thinking_length,\n   385→        tool_diversity,\n   386→        unique_tools: unique_tools.len() as u32,\n   387→    }\n   388→}\n   389→\n   390→/// Compute tool usage metrics\n   391→fn compute_tool_usage(trajectory: &Trajectory) -> ToolUsageMetrics {\n   392→    let mut calls_by_tool: HashMap<String, u32> = HashMap::new();\n   393→    let mut successes_by_tool: HashMap<String, u32> = HashMap::new();\n   394→    let mut pending_tools: HashMap<String, String> = HashMap::new();\n   395→\n   396→    for step in &trajectory.steps {\n   397→        match &step.step_type {\n   398→            StepType::ToolCall { tool, tool_id, .. } => {\n   399→                *calls_by_tool.entry(tool.clone()).or_insert(0) += 1;\n   400→                pending_tools.insert(tool_id.clone(), tool.clone());\n   401→            }\n   402→            StepType::ToolResult { tool_id, success, .. } => {\n   403→                if *success {\n   404→                    if let Some(tool_name) = pending_tools.get(tool_id) {\n   405→                        *successes_by_tool.entry(tool_name.clone()).or_insert(0) += 1;\n   406→                    }\n   407→                }\n   408→            }\n   409→            _ => {}\n   410→        }\n   411→    }\n   412→\n   413→    let total_calls: u32 = calls_by_tool.values().sum();\n   414→\n   415→    let success_rate_by_tool: HashMap<String, f64> = calls_by_tool\n   416→        .iter()\n   417→        .map(|(tool, count)| {\n   418→            let successes = successes_by_tool.get(tool).copied().unwrap_or(0);\n   419→            let rate = if *count > 0 {\n   420→                successes as f64 / *count as f64\n   421→            } else {\n   422→                0.0\n   423→            };\n   424→            (tool.clone(), rate)\n   425→        })\n   426→        .collect();\n   427→\n   428→    let most_used_tool = calls_by_tool\n   429→        .iter()\n   430→        .max_by_key(|(_, count)| *count)\n   431→        .map(|(tool, _)| tool.clone());\n   432→\n   433→    ToolUsageMetrics {\n   434→        total_calls,\n   435→        calls_by_tool,\n   436→        success_rate_by_tool,\n   437→        most_used_tool,\n   438→    }\n   439→}\n   440→\n   441→/// Aggregate analysis across multiple trajectories\n   442→pub fn aggregate_analyses(analyses: &[TrajectoryAnalysis]) -> AggregateAnalysis {\n   443→    if analyses.is_empty() {\n   444→        return AggregateAnalysis::default();\n   445→    }\n   446→\n   447→    let trajectory_count = analyses.len();\n   448→\n   449→    // Date range\n   450→    let min_date = analyses.iter().map(|a| a.started_at).min();\n   451→    let max_date = analyses.iter().filter_map(|a| a.ended_at).max();\n   452→    let date_range = min_date.and_then(|min| max_date.map(|max| (min, max)));\n   453→\n   454→    // By model stats\n   455→    let mut by_model: HashMap<String, Vec<&TrajectoryAnalysis>> = HashMap::new();\n   456→    for analysis in analyses {\n   457→        by_model\n   458→            .entry(analysis.model.clone())\n   459→            .or_default()\n   460→            .push(analysis);\n   461→    }\n   462→\n   463→    let by_model_stats: HashMap<String, ModelStats> = by_model\n   464→        .iter()\n   465→        .map(|(model, runs)| {\n   466→            let count = runs.len();\n   467→            let avg_cost = runs.iter().map(|r| r.cost.total_cost_usd).sum::<f64>() / count as f64;\n   468→            let avg_duration =\n   469→                runs.iter().map(|r| r.performance.total_duration_ms).sum::<u64>() as f64\n   470→                    / count as f64;\n   471→            let success_rate = runs.iter().filter(|r| r.errors.success).count() as f64 / count as f64;\n   472→            (\n   473→                model.clone(),\n   474→                ModelStats {\n   475→                    count,\n   476→                    avg_cost,\n   477→                    avg_duration_ms: avg_duration,\n   478→                    success_rate,\n   479→                },\n   480→            )\n   481→        })\n   482→        .collect();\n   483→\n   484→    // Aggregates\n   485→    let total_cost_usd: f64 = analyses.iter().map(|a| a.cost.total_cost_usd).sum();\n   486→    let avg_cost_usd = total_cost_usd / trajectory_count as f64;\n   487→    let avg_duration_ms = analyses\n   488→        .iter()\n   489→        .map(|a| a.performance.total_duration_ms)\n   490→        .sum::<u64>() as f64\n   491→        / trajectory_count as f64;\n   492→    let overall_success_rate =\n   493→        analyses.iter().filter(|a| a.errors.success).count() as f64 / trajectory_count as f64;\n   494→    let avg_tool_error_rate =\n   495→        analyses.iter().map(|a| a.errors.tool_error_rate).sum::<f64>() / trajectory_count as f64;\n   496→    let avg_cache_hit_rate =\n   497→        analyses.iter().map(|a| a.cost.cache_hit_rate).sum::<f64>() / trajectory_count as f64;\n   498→    let total_tool_calls: u32 = analyses.iter().map(|a| a.tool_usage.total_calls).sum();\n   499→\n   500→    // Top error tools\n   501→    let mut error_tools: HashMap<String, u32> = HashMap::new();\n   502→    for analysis in analyses {\n   503→        for (tool, count) in &analysis.errors.errors_by_tool {\n   504→            *error_tools.entry(tool.clone()).or_insert(0) += count;\n   505→        }\n   506→    }\n   507→    let mut top_error_tools: Vec<(String, u32)> = error_tools.into_iter().collect();\n   508→    top_error_tools.sort_by(|a, b| b.1.cmp(&a.1));\n   509→    top_error_tools.truncate(5);\n   510→\n   511→    AggregateAnalysis {\n   512→        trajectory_count,\n   513→        date_range,\n   514→        by_model: by_model_stats,\n   515→        total_cost_usd,\n   516→        avg_cost_usd,\n   517→        avg_duration_ms,\n   518→        overall_success_rate,\n   519→        avg_tool_error_rate,\n   520→        avg_cache_hit_rate,\n   521→        total_tool_calls,\n   522→        top_error_tools,\n   523→    }\n   524→}\n   525→\n   526→/// Print analysis in human-readable format\n   527→pub fn print_analysis(analysis: &TrajectoryAnalysis) {\n   528→    let sep = \"=\".repeat(80);\n   529→    println!(\"{}\", sep.bright_blue());\n   530→    println!(\n   531→        \"{}\",\n   532→        format!(\"Trajectory Analysis: {}\", &analysis.session_id[..8.min(analysis.session_id.len())])\n   533→            .bright_white()\n   534→            .bold()\n   535→    );\n   536→    println!(\"{}\", sep.bright_blue());\n   537→\n   538→    // Performance\n   539→    println!(\"\\n{}\", \"PERFORMANCE\".yellow().bold());\n   540→    println!(\n   541→        \"  Duration:          {:.1}s\",\n   542→        analysis.performance.total_duration_ms as f64 / 1000.0\n   543→    );\n   544→    if analysis.performance.tool_latency_stats.count > 0 {\n   545→        let stats = &analysis.performance.tool_latency_stats;\n   546→        println!(\n   547→            \"  Tool Latency:      p50={}ms, p95={}ms (n={})\",\n   548→            stats.p50_ms, stats.p95_ms, stats.count\n   549→        );\n   550→    }\n   551→    println!(\n   552→        \"  Parallel Batches:  {}\",\n   553→        analysis.performance.parallel_tool_batches\n   554→    );\n   555→    println!(\"  Total Tool Calls:  {}\", analysis.performance.total_tool_calls);\n   556→\n   557→    // Cost\n   558→    println!(\"\\n{}\", \"COST\".yellow().bold());\n   559→    println!(\"  Total:             ${:.4}\", analysis.cost.total_cost_usd);\n   560→    println!(\n   561→        \"  Input Tokens:      {}\",\n   562→        format_tokens(analysis.cost.input_tokens)\n   563→    );\n   564→    println!(\n   565→        \"  Output Tokens:     {}\",\n   566→        format_tokens(analysis.cost.output_tokens)\n   567→    );\n   568→    println!(\"  Cache Hit Rate:    {:.1}%\", analysis.cost.cache_hit_rate * 100.0);\n   569→\n   570→    // Errors\n   571→    println!(\"\\n{}\", \"ERRORS\".yellow().bold());\n   572→    let success_str = if analysis.errors.success {\n   573→        \"YES\".green()\n   574→    } else {\n   575→        \"NO\".red()\n   576→    };\n   577→    println!(\"  Success:           {}\", success_str);\n   578→    println!(\n   579→        \"  Tool Error Rate:   {:.1}% ({}/{})\",\n   580→        analysis.errors.tool_error_rate * 100.0,\n   581→        analysis.errors.failed_tool_calls,\n   582→        analysis.errors.total_tool_calls\n   583→    );\n   584→    if !analysis.errors.errors_by_tool.is_empty() {\n   585→        println!(\"  Errors by Tool:\");\n   586→        for (tool, count) in &analysis.errors.errors_by_tool {\n   587→            println!(\"    {}: {}\", tool, count);\n   588→        }\n   589→    }\n   590→\n   591→    // Quality\n   592→    println!(\"\\n{}\", \"QUALITY\".yellow().bold());\n   593→    println!(\"  Turns:             {}\", analysis.quality.num_turns);\n   594→    println!(\"  Thinking Blocks:   {}\", analysis.quality.thinking_blocks);\n   595→    println!(\n   596→        \"  Tool Diversity:    {:.0}% ({} unique)\",\n   597→        analysis.quality.tool_diversity * 100.0,\n   598→        analysis.quality.unique_tools\n   599→    );\n   600→\n   601→    // Tool Usage\n   602→    if !analysis.tool_usage.calls_by_tool.is_empty() {\n   603→        println!(\"\\n{}\", \"TOOL USAGE\".yellow().bold());\n   604→        let mut tools: Vec<_> = analysis.tool_usage.calls_by_tool.iter().collect();\n   605→        tools.sort_by(|a, b| b.1.cmp(a.1));\n   606→        for (tool, count) in tools.iter().take(10) {\n   607→            let success_rate = analysis\n   608→                .tool_usage\n   609→                .success_rate_by_tool\n   610→                .get(*tool)\n   611→                .copied()\n   612→                .unwrap_or(0.0);\n   613→            println!(\"  {:20} {} calls, {:.0}% success\", tool, count, success_rate * 100.0);\n   614→        }\n   615→    }\n   616→\n   617→    println!(\"\\n{}\", sep.bright_blue());\n   618→}\n   619→\n   620→/// Print aggregate analysis\n   621→pub fn print_aggregate(analysis: &AggregateAnalysis) {\n   622→    let sep = \"=\".repeat(80);\n   623→    println!(\"{}\", sep.bright_blue());\n   624→    println!(\n   625→        \"{}\",\n   626→        format!(\"Aggregate Analysis: {} trajectories\", analysis.trajectory_count)\n   627→            .bright_white()\n   628→            .bold()\n   629→    );\n   630→    println!(\"{}\", sep.bright_blue());\n   631→\n   632→    // Overview\n   633→    println!(\"\\n{}\", \"OVERVIEW\".yellow().bold());\n   634→    println!(\"  Trajectories:      {}\", analysis.trajectory_count);\n   635→    println!(\"  Total Cost:        ${:.4}\", analysis.total_cost_usd);\n   636→    println!(\"  Avg Cost:          ${:.4}\", analysis.avg_cost_usd);\n   637→    println!(\"  Avg Duration:      {:.1}s\", analysis.avg_duration_ms / 1000.0);\n   638→    println!(\"  Success Rate:      {:.1}%\", analysis.overall_success_rate * 100.0);\n   639→    println!(\"  Avg Tool Errors:   {:.1}%\", analysis.avg_tool_error_rate * 100.0);\n   640→    println!(\"  Avg Cache Hit:     {:.1}%\", analysis.avg_cache_hit_rate * 100.0);\n   641→    println!(\"  Total Tool Calls:  {}\", analysis.total_tool_calls);\n   642→\n   643→    // By Model\n   644→    if !analysis.by_model.is_empty() {\n   645→        println!(\"\\n{}\", \"BY MODEL\".yellow().bold());\n   646→        for (model, stats) in &analysis.by_model {\n   647→            let model_short = model.split('-').take(3).collect::<Vec<_>>().join(\"-\");\n   648→            println!(\n   649→                \"  {:30} {} runs, ${:.4} avg, {:.1}% success\",\n   650→                model_short,\n   651→                stats.count,\n   652→                stats.avg_cost,\n   653→                stats.success_rate * 100.0\n   654→            );\n   655→        }\n   656→    }\n   657→\n   658→    // Top Error Tools\n   659→    if !analysis.top_error_tools.is_empty() {\n   660→        println!(\"\\n{}\", \"TOP ERROR TOOLS\".yellow().bold());\n   661→        for (tool, count) in &analysis.top_error_tools {\n   662→            println!(\"  {:20} {} errors\", tool, count);\n   663→        }\n   664→    }\n   665→\n   666→    println!(\"\\n{}\", sep.bright_blue());\n   667→}\n   668→\n   669→fn truncate(s: &str, max_len: usize) -> String {\n   670→    if s.len() <= max_len {\n   671→        s.to_string()\n   672→    } else {\n   673→        format!(\"{}...\", &s[..max_len - 3])\n   674→    }\n   675→}\n   676→\n   677→fn format_tokens(tokens: u64) -> String {\n   678→    if tokens >= 1_000_000 {\n   679→        format!(\"{:.1}M\", tokens as f64 / 1_000_000.0)\n   680→    } else if tokens >= 1_000 {\n   681→        format!(\"{:.1}K\", tokens as f64 / 1_000.0)\n   682→    } else {\n   683→        tokens.to_string()\n   684→    }\n   685→}\n   686→\n   687→//\n   688→// ===== Aggregate Metrics Analysis =====\n   689→//\n   690→\n   691→use crate::metrics::{MetricsDb, SessionMetrics};\n   692→\n   693→/// Aggregate statistics for a metric dimension from metrics database\n   694→#[derive(Debug, Clone, Serialize, Deserialize)]\n   695→pub struct MetricAggregateStats {\n   696→    pub dimension: String,\n   697→    pub count: usize,\n   698→    pub mean: f64,\n   699→    pub median: f64,\n   700→    pub p90: f64,\n   701→    pub p99: f64,\n   702→    pub min: f64,\n   703→    pub max: f64,\n   704→    pub stddev: f64,\n   705→}\n   706→\n   707→/// Trend direction for a metric\n   708→#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\n   709→pub enum TrendDirection {\n   710→    Improving,\n   711→    Stable,\n   712→    Degrading,\n   713→}\n   714→\n   715→/// Trend analysis for a metric over time\n   716→#[derive(Debug, Clone, Serialize, Deserialize)]\n   717→pub struct MetricTrend {\n   718→    pub dimension: String,\n   719→    pub direction: TrendDirection,\n   720→    pub percent_change: f64,\n   721→    pub recent: MetricAggregateStats,\n   722→    pub baseline: Option<MetricAggregateStats>,\n   723→}\n   724→\n   725→/// Detected regression in metrics\n   726→#[derive(Debug, Clone, Serialize, Deserialize)]\n   727→pub struct Regression {\n   728→    pub dimension: String,\n   729→    pub baseline_value: f64,\n   730→    pub current_value: f64,\n   731→    pub percent_worse: f64,\n   732→    pub is_critical: bool,\n   733→    pub severity: RegressionSeverity,\n   734→    pub deviation_sigma: f64,\n   735→}\n   736→\n   737→#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\n   738→pub enum RegressionSeverity {\n   739→    Warning,\n   740→    Error,\n   741→    Critical,\n   742→}\n   743→\n   744→/// Time period for analysis\n   745→#[derive(Debug, Clone, Copy)]\n   746→pub enum TimePeriod {\n   747→    Last7Days,\n   748→    Last30Days,\n   749→    LastWeek,\n   750→    ThisWeek,\n   751→    Custom { start: DateTime<Utc>, end: DateTime<Utc> },\n   752→}\n   753→\n   754→impl TimePeriod {\n   755→    /// Get the start and end timestamps for this period\n   756→    pub fn bounds(&self) -> (DateTime<Utc>, DateTime<Utc>) {\n   757→        use chrono::Datelike;\n   758→\n   759→        let now = Utc::now();\n   760→        match self {\n   761→            TimePeriod::Last7Days => (now - chrono::Duration::days(7), now),\n   762→            TimePeriod::Last30Days => (now - chrono::Duration::days(30), now),\n   763→            TimePeriod::LastWeek => {\n   764→                let days_since_monday = now.weekday().num_days_from_monday();\n   765→                let last_monday = now - chrono::Duration::days((days_since_monday + 7) as i64);\n   766→                let last_sunday = last_monday + chrono::Duration::days(6);\n   767→                (last_monday, last_sunday)\n   768→            }\n   769→            TimePeriod::ThisWeek => {\n   770→                let days_since_monday = now.weekday().num_days_from_monday();\n   771→                let this_monday = now - chrono::Duration::days(days_since_monday as i64);\n   772→                (this_monday, now)\n   773→            }\n   774→            TimePeriod::Custom { start, end } => (*start, *end),\n   775→        }\n   776→    }\n   777→\n   778→    pub fn name(&self) -> &'static str {\n   779→        match self {\n   780→            TimePeriod::Last7Days => \"Last 7 Days\",\n   781→            TimePeriod::Last30Days => \"Last 30 Days\",\n   782→            TimePeriod::LastWeek => \"Last Week\",\n   783→            TimePeriod::ThisWeek => \"This Week\",\n   784→            TimePeriod::Custom { .. } => \"Custom Period\",\n   785→        }\n   786→    }\n   787→}\n   788→\n   789→/// Calculate aggregate statistics from a list of values\n   790→fn calculate_metric_stats(dimension: &str, values: &[f64]) -> Option<MetricAggregateStats> {\n   791→    if values.is_empty() {\n   792→        return None;\n   793→    }\n   794→\n   795→    use statrs::statistics::{Data, Distribution, OrderStatistics, Min, Max};\n   796→\n   797→    let mut data = Data::new(values.to_vec());\n   798→    let mean = data.mean().unwrap_or(0.0);\n   799→    let stddev = data.std_dev().unwrap_or(0.0);\n   800→    let median = data.median();\n   801→    let p90 = data.percentile(90);\n   802→    let p99 = data.percentile(99);\n   803→    let min = data.min();\n   804→    let max = data.max();\n   805→\n   806→    Some(MetricAggregateStats {\n   807→        dimension: dimension.to_string(),\n   808→        count: values.len(),\n   809→        mean,\n   810→        median,\n   811→        p90,\n   812→        p99,\n   813→        min,\n   814→        max,\n   815→        stddev,\n   816→    })\n   817→}\n   818→\n   819→/// Get sessions within a time period\n   820→pub fn get_sessions_in_period(\n   821→    db: &MetricsDb,\n   822→    period: TimePeriod,\n   823→) -> anyhow::Result<Vec<SessionMetrics>> {\n   824→    let (start, end) = period.bounds();\n   825→    let all_sessions = db.get_all_sessions()?;\n   826→\n   827→    Ok(all_sessions\n   828→        .into_iter()\n   829→        .filter(|s| s.timestamp >= start && s.timestamp <= end)\n   830→        .collect())\n   831→}\n   832→\n   833→/// Get sessions between two specific dates\n   834→pub fn get_sessions_between_dates(\n   835→    db: &MetricsDb,\n   836→    start: DateTime<Utc>,\n   837→    end: DateTime<Utc>,\n   838→) -> anyhow::Result<Vec<SessionMetrics>> {\n   839→    let all_sessions = db.get_all_sessions()?;\n   840→\n   841→    Ok(all_sessions\n   842→        .into_iter()\n   843→        .filter(|s| s.timestamp >= start && s.timestamp <= end)\n   844→        .collect())\n   845→}\n   846→\n   847→/// Calculate aggregate statistics for all metrics from sessions\n   848→pub fn calculate_aggregate_stats_from_sessions(sessions: &[SessionMetrics]) -> HashMap<String, MetricAggregateStats> {\n   849→    let mut stats_map = HashMap::new();\n   850→\n   851→    // Tool error rate\n   852→    let error_rates: Vec<f64> = sessions\n   853→        .iter()\n   854→        .filter(|s| s.tool_calls > 0)\n   855→        .map(|s| (s.tool_errors as f64) / (s.tool_calls as f64))\n   856→        .collect();\n   857→    if let Some(stats) = calculate_metric_stats(\"tool_error_rate\", &error_rates) {\n   858→        stats_map.insert(\"tool_error_rate\".to_string(), stats);\n   859→    }\n   860→\n   861→    // Tokens per issue\n   862→    let tokens_per_issue: Vec<f64> = sessions\n   863→        .iter()\n   864→        .filter(|s| s.issues_completed > 0)\n   865→        .map(|s| ((s.tokens_in + s.tokens_out) as f64) / (s.issues_completed as f64))\n   866→        .collect();\n   867→    if let Some(stats) = calculate_metric_stats(\"tokens_per_issue\", &tokens_per_issue) {\n   868→        stats_map.insert(\"tokens_per_issue\".to_string(), stats);\n   869→    }\n   870→\n   871→    // Duration per issue\n   872→    let duration_per_issue: Vec<f64> = sessions\n   873→        .iter()\n   874→        .filter(|s| s.issues_completed > 0)\n   875→        .map(|s| s.duration_seconds / (s.issues_completed as f64))\n   876→        .collect();\n   877→    if let Some(stats) = calculate_metric_stats(\"duration_per_issue\", &duration_per_issue) {\n   878→        stats_map.insert(\"duration_per_issue\".to_string(), stats);\n   879→    }\n   880→\n   881→    // Cost per issue\n   882→    let cost_per_issue: Vec<f64> = sessions\n   883→        .iter()\n   884→        .filter(|s| s.issues_completed > 0)\n   885→        .map(|s| s.cost_usd / (s.issues_completed as f64))\n   886→        .collect();\n   887→    if let Some(stats) = calculate_metric_stats(\"cost_per_issue\", &cost_per_issue) {\n   888→        stats_map.insert(\"cost_per_issue\".to_string(), stats);\n   889→    }\n   890→\n   891→    // Session duration\n   892→    let durations: Vec<f64> = sessions.iter().map(|s| s.duration_seconds).collect();\n   893→    if let Some(stats) = calculate_metric_stats(\"session_duration\", &durations) {\n   894→        stats_map.insert(\"session_duration\".to_string(), stats);\n   895→    }\n   896→\n   897→    // Completion rate\n   898→    let completion_rates: Vec<f64> = sessions\n   899→        .iter()\n   900→        .filter(|s| s.issues_claimed > 0)\n   901→        .map(|s| (s.issues_completed as f64) / (s.issues_claimed as f64))\n   902→        .collect();\n   903→    if let Some(stats) = calculate_metric_stats(\"completion_rate\", &completion_rates) {\n   904→        stats_map.insert(\"completion_rate\".to_string(), stats);\n   905→    }\n   906→\n   907→    stats_map\n   908→}\n   909→\n   910→/// Detect trends by comparing recent vs baseline periods\n   911→pub fn detect_trends(\n   912→    db: &MetricsDb,\n   913→    recent_period: TimePeriod,\n   914→    baseline_period: Option<TimePeriod>,\n   915→) -> anyhow::Result<Vec<MetricTrend>> {\n   916→    let recent_sessions = get_sessions_in_period(db, recent_period)?;\n   917→    let recent_stats = calculate_aggregate_stats_from_sessions(&recent_sessions);\n   918→\n   919→    let baseline_sessions = if let Some(bp) = baseline_period {\n   920→        Some(get_sessions_in_period(db, bp)?)\n   921→    } else {\n   922→        None\n   923→    };\n   924→\n   925→    let baseline_stats = baseline_sessions\n   926→        .as_ref()\n   927→        .map(|sessions| calculate_aggregate_stats_from_sessions(sessions));\n   928→\n   929→    let mut trends = Vec::new();\n   930→\n   931→    for (dimension, recent) in recent_stats {\n   932→        let baseline = baseline_stats\n   933→            .as_ref()\n   934→            .and_then(|map| map.get(&dimension).cloned());\n   935→\n   936→        let (direction, percent_change) = if let Some(ref base) = baseline {\n   937→            let change = if base.mean != 0.0 {\n   938→                ((recent.mean - base.mean) / base.mean) * 100.0\n   939→            } else {\n   940→                0.0\n   941→            };\n   942→\n   943→            // Determine if change is improvement or degradation based on metric type\n   944→            let direction = match dimension.as_str() {\n   945→                \"tool_error_rate\" | \"cost_per_issue\" | \"duration_per_issue\" => {\n   946→                    // Lower is better\n   947→                    if change < -5.0 {\n   948→                        TrendDirection::Improving\n   949→                    } else if change > 5.0 {\n   950→                        TrendDirection::Degrading\n   951→                    } else {\n   952→                        TrendDirection::Stable\n   953→                    }\n   954→                }\n   955→                \"completion_rate\" => {\n   956→                    // Higher is better\n   957→                    if change > 5.0 {\n   958→                        TrendDirection::Improving\n   959→                    } else if change < -5.0 {\n   960→                        TrendDirection::Degrading\n   961→                    } else {\n   962→                        TrendDirection::Stable\n   963→                    }\n   964→                }\n   965→                _ => TrendDirection::Stable,\n   966→            };\n   967→\n   968→            (direction, change)\n   969→        } else {\n   970→            (TrendDirection::Stable, 0.0)\n   971→        };\n   972→\n   973→        trends.push(MetricTrend {\n   974→            dimension,\n   975→            direction,\n   976→            percent_change,\n   977→            recent,\n   978→            baseline,\n   979→        });\n   980→    }\n   981→\n   982→    Ok(trends)\n   983→}\n   984→\n   985→/// Detect regressions by comparing against baselines\n   986→pub fn detect_regressions(db: &MetricsDb, period: TimePeriod) -> anyhow::Result<Vec<Regression>> {\n   987→    let sessions = get_sessions_in_period(db, period)?;\n   988→    let current_stats = calculate_aggregate_stats_from_sessions(&sessions);\n   989→    let mut regressions = Vec::new();\n   990→\n   991→    // Check each dimension against stored baseline\n   992→    for (dimension, current) in current_stats {\n   993→        if let Some(baseline) = db.get_baseline(&dimension)? {\n   994→            // Calculate standard deviation-based regression threshold\n   995→            // A metric is regressed if it's >2 standard deviations worse than baseline\n   996→            let deviation_sigma = if baseline.stddev > 0.0 {\n   997→                (current.mean - baseline.mean).abs() / baseline.stddev\n   998→            } else {\n   999→                0.0\n  1000→            };\n  1001→\n  1002→            let percent_worse = match dimension.as_str() {\n  1003→                \"tool_error_rate\" | \"cost_per_issue\" | \"duration_per_issue\" => {\n  1004→                    // Lower is better - check if current is higher than baseline\n  1005→                    if current.mean > baseline.mean && baseline.mean > 0.0 {\n  1006→                        ((current.mean - baseline.mean) / baseline.mean) * 100.0\n  1007→                    } else {\n  1008→                        continue; // Not a regression\n  1009→                    }\n  1010→                }\n  1011→                \"completion_rate\" => {\n  1012→                    // Higher is better - check if current is lower than baseline\n  1013→                    if current.mean < baseline.mean && baseline.mean > 0.0 {\n  1014→                        ((baseline.mean - current.mean) / baseline.mean) * 100.0\n  1015→                    } else {\n  1016→                        continue; // Not a regression\n  1017→                    }\n  1018→                }\n  1019→                _ => continue,\n  1020→            };\n  1021→\n  1022→            // Determine severity based on both percent change and standard deviations\n  1023→            let severity = if percent_worse > 50.0 || deviation_sigma > 3.0 {\n  1024→                RegressionSeverity::Critical\n  1025→            } else if percent_worse > 25.0 || deviation_sigma > 2.5 {\n  1026→                RegressionSeverity::Error\n  1027→            } else if percent_worse > 10.0 || deviation_sigma > 2.0 {\n  1028→                RegressionSeverity::Warning\n  1029→            } else {\n  1030→                continue; // Not significant enough to report\n  1031→            };\n  1032→\n  1033→            regressions.push(Regression {\n  1034→                dimension,\n  1035→                baseline_value: baseline.mean,\n  1036→                current_value: current.mean,\n  1037→                percent_worse,\n  1038→                is_critical: percent_worse > 50.0,\n  1039→                severity,\n  1040→                deviation_sigma,\n  1041→            });\n  1042→        }\n  1043→    }\n  1044→\n  1045→    Ok(regressions)\n  1046→}\n  1047→\n  1048→/// Store detected regressions as anomalies in the database\n  1049→///\n  1050→/// Converts each regression into an Anomaly and stores it for tracking.\n  1051→/// This enables issue creation from regressions and historical analysis.\n  1052→///\n  1053→/// # Arguments\n  1054→/// * `db` - Metrics database\n  1055→/// * `regressions` - List of detected regressions to store\n  1056→/// * `session_id` - Session ID to associate anomalies with (use \"aggregate\" for period-based detection)\n  1057→///\n  1058→/// # Returns\n  1059→/// Number of anomalies successfully stored\n  1060→pub fn store_regressions_as_anomalies(\n  1061→    db: &MetricsDb,\n  1062→    regressions: &[Regression],\n  1063→    session_id: &str,\n  1064→) -> anyhow::Result<usize> {\n  1065→    use crate::metrics::{Anomaly, AnomalySeverity};\n  1066→\n  1067→    let mut stored_count = 0;\n  1068→\n  1069→    for regression in regressions {\n  1070→        // Convert RegressionSeverity to AnomalySeverity\n  1071→        let severity = match regression.severity {\n  1072→            RegressionSeverity::Critical => AnomalySeverity::Critical,\n  1073→            RegressionSeverity::Error => AnomalySeverity::Error,\n  1074→            RegressionSeverity::Warning => AnomalySeverity::Warning,\n  1075→        };\n  1076→\n  1077→        let anomaly = Anomaly {\n  1078→            session_id: session_id.to_string(),\n  1079→            dimension: regression.dimension.clone(),\n  1080→            expected_value: regression.baseline_value,\n  1081→            actual_value: regression.current_value,\n  1082→            severity,\n  1083→            investigated: false,\n  1084→            issue_number: None,\n  1085→        };\n  1086→\n  1087→        db.store_anomaly(&anomaly)?;\n  1088→        stored_count += 1;\n  1089→    }\n  1090→\n  1091→    Ok(stored_count)\n  1092→}\n  1093→\n  1094→/// Get top error tools from sessions\n  1095→pub fn get_top_error_tools(db: &MetricsDb, period: TimePeriod, limit: usize) -> anyhow::Result<Vec<(String, u32)>> {\n  1096→    let sessions = get_sessions_in_period(db, period)?;\n  1097→    let mut error_counts: HashMap<String, u32> = HashMap::new();\n  1098→\n  1099→    for session in &sessions {\n  1100→        let tool_calls = db.get_tool_calls(&session.id)?;\n  1101→        for tc in tool_calls {\n  1102→            if !tc.success {\n  1103→                *error_counts.entry(tc.tool_name).or_insert(0) += 1;\n  1104→            }\n  1105→        }\n  1106→    }\n  1107→\n  1108→    let mut errors: Vec<(String, u32)> = error_counts.into_iter().collect();\n  1109→    errors.sort_by(|a, b| b.1.cmp(&a.1));\n  1110→    errors.truncate(limit);\n  1111→\n  1112→    Ok(errors)\n  1113→}\n  1114→\n  1115→/// Get slowest tools by average duration\n  1116→pub fn get_slowest_tools(db: &MetricsDb, period: TimePeriod, limit: usize) -> anyhow::Result<Vec<(String, f64, usize)>> {\n  1117→    let sessions = get_sessions_in_period(db, period)?;\n  1118→    let mut tool_durations: HashMap<String, Vec<i64>> = HashMap::new();\n  1119→\n  1120→    for session in &sessions {\n  1121→        let tool_calls = db.get_tool_calls(&session.id)?;\n  1122→        for tc in tool_calls {\n  1123→            tool_durations\n  1124→                .entry(tc.tool_name)\n  1125→                .or_default()\n  1126→                .push(tc.duration_ms);\n  1127→        }\n  1128→    }\n  1129→\n  1130→    let mut slowest: Vec<(String, f64, usize)> = tool_durations\n  1131→        .into_iter()\n  1132→        .map(|(tool, durations)| {\n  1133→            let count = durations.len();\n  1134→            let avg = durations.iter().sum::<i64>() as f64 / count as f64;\n  1135→            (tool, avg, count)\n  1136→        })\n  1137→        .collect();\n  1138→\n  1139→    slowest.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));\n  1140→    slowest.truncate(limit);\n  1141→\n  1142→    Ok(slowest)\n  1143→}\n  1144→\n  1145→#[cfg(test)]\n  1146→mod tests_edge_cases {\n  1147→    use super::*;\n  1148→    use crate::trajectory::{Step, StepType, TokenUsage, TrajectoryResult};\n  1149→    use chrono::Utc;\n  1150→\n  1151→    /// Helper to create a minimal trajectory for testing\n  1152→    fn create_test_trajectory() -> Trajectory {\n  1153→        Trajectory {\n  1154→            session_id: \"test-session-id\".to_string(),\n  1155→            prompt: \"Test prompt\".to_string(),\n  1156→            model: \"claude-sonnet-4-5\".to_string(),\n  1157→            cwd: \"/test\".to_string(),\n  1158→            repo_sha: \"abc123\".to_string(),\n  1159→            branch: Some(\"main\".to_string()),\n  1160→            started_at: Utc::now(),\n  1161→            ended_at: Some(Utc::now()),\n  1162→            steps: vec![],\n  1163→            result: Some(TrajectoryResult {\n  1164→                success: true,\n  1165→                num_turns: 1,\n  1166→                duration_ms: 1000,\n  1167→                result_text: None,\n  1168→                errors: vec![],\n  1169→                issues_completed: 0,\n  1170→                apm: None,\n  1171→            }),\n  1172→            usage: TokenUsage {\n  1173→                input_tokens: 100,\n  1174→                output_tokens: 50,\n  1175→                cache_read_tokens: 20,\n  1176→                cache_creation_tokens: 0,\n  1177→                cost_usd: 0.001,\n  1178→            },\n  1179→        }\n  1180→    }\n  1181→\n  1182→    #[test]\n  1183→    fn test_empty_trajectory() {\n  1184→        let mut trajectory = create_test_trajectory();\n  1185→        trajectory.steps = vec![];\n  1186→        trajectory.result = None;\n  1187→\n  1188→        let analysis = analyze_trajectory(&trajectory);\n  1189→\n  1190→        // Should not panic, should return zero/default values\n  1191→        assert_eq!(analysis.performance.total_tool_calls, 0);\n  1192→        assert_eq!(analysis.performance.parallel_tool_batches, 0);\n  1193→        assert_eq!(analysis.errors.total_tool_calls, 0);\n  1194→        assert_eq!(analysis.errors.failed_tool_calls, 0);\n  1195→        assert_eq!(analysis.quality.thinking_blocks, 0);\n  1196→        assert_eq!(analysis.tool_usage.total_calls, 0);\n  1197→    }\n  1198→\n  1199→    #[test]\n  1200→    fn test_malformed_step_data_missing_tokens() {\n  1201→        let mut trajectory = create_test_trajectory();\n  1202→        trajectory.steps = vec![Step {\n  1203→            step_id: 1,\n  1204→            timestamp: Utc::now(),\n  1205→            step_type: StepType::Thinking {\n  1206→                content: \"test\".to_string(),\n  1207→                signature: None,\n  1208→            },\n  1209→            tokens_in: None,  // Missing token data\n  1210→            tokens_out: None,\n  1211→            tokens_cached: None,\n  1212→        }];\n  1213→\n  1214→        let analysis = analyze_trajectory(&trajectory);\n  1215→\n  1216→        // Should handle missing token data gracefully\n  1217→        assert_eq!(analysis.quality.thinking_blocks, 1);\n  1218→    }\n  1219→\n  1220→    #[test]\n  1221→    fn test_zero_tokens_no_division_by_zero() {\n  1222→        let mut trajectory = create_test_trajectory();\n  1223→        trajectory.usage = TokenUsage {\n  1224→            input_tokens: 0,\n  1225→            output_tokens: 0,\n  1226→            cache_read_tokens: 0,\n  1227→            cache_creation_tokens: 0,\n  1228→            cost_usd: 0.0,\n  1229→        };\n  1230→\n  1231→        let analysis = analyze_trajectory(&trajectory);\n  1232→\n  1233→        // Cache hit rate should be 0, not NaN or panic\n  1234→        assert_eq!(analysis.cost.cache_hit_rate, 0.0);\n  1235→    }\n  1236→\n  1237→    #[test]\n  1238→    fn test_zero_tool_calls_no_division_by_zero() {\n  1239→        let trajectory = create_test_trajectory();\n  1240→\n  1241→        let analysis = analyze_trajectory(&trajectory);\n  1242→\n  1243→        // Error rate should be 0 when no tool calls\n  1244→        assert_eq!(analysis.errors.tool_error_rate, 0.0);\n  1245→        assert_eq!(analysis.quality.tool_diversity, 0.0);\n  1246→    }\n  1247→\n  1248→    #[test]\n  1249→    fn test_orphaned_tool_result_no_matching_call() {\n  1250→        let mut trajectory = create_test_trajectory();\n  1251→        let now = Utc::now();\n  1252→\n  1253→        // Tool result without matching tool call\n  1254→        trajectory.steps = vec![Step {\n  1255→            step_id: 1,\n  1256→            timestamp: now,\n  1257→            step_type: StepType::ToolResult {\n  1258→                tool_id: \"nonexistent\".to_string(),\n  1259→                success: false,\n  1260→                output: None,\n  1261→            },\n  1262→            tokens_in: None,\n  1263→            tokens_out: None,\n  1264→            tokens_cached: None,\n  1265→        }];\n  1266→\n  1267→        let analysis = analyze_trajectory(&trajectory);\n  1268→\n  1269→        // Should not panic, orphaned result should not affect counts\n  1270→        assert_eq!(analysis.errors.total_tool_calls, 0);\n  1271→    }\n  1272→\n  1273→    #[test]\n  1274→    fn test_unicode_in_content() {\n  1275→        let mut trajectory = create_test_trajectory();\n  1276→        trajectory.prompt = \"测试 🚀 مرحبا Тест\".to_string();\n  1277→\n  1278→        trajectory.steps = vec![Step {\n  1279→            step_id: 1,\n  1280→            timestamp: Utc::now(),\n  1281→            step_type: StepType::Thinking {\n  1282→                content: \"Emoji test 🔥💯✨ and unicode 你好世界\".to_string(),\n  1283→                signature: None,\n  1284→            },\n  1285→            tokens_in: None,\n  1286→            tokens_out: None,\n  1287→            tokens_cached: None,\n  1288→        }];\n  1289→\n  1290→        let analysis = analyze_trajectory(&trajectory);\n  1291→\n  1292→        // Should handle unicode characters properly\n  1293→        assert_eq!(analysis.quality.thinking_blocks, 1);\n  1294→        assert!(analysis.prompt_preview.contains(\"测试\"));\n  1295→    }\n  1296→\n  1297→    #[test]\n  1298→    fn test_very_long_trajectory() {\n  1299→        let mut trajectory = create_test_trajectory();\n  1300→        let now = Utc::now();\n  1301→\n  1302→        // Create 10,000 steps\n  1303→        for i in 0..10_000 {\n  1304→            trajectory.steps.push(Step {\n  1305→                step_id: i,\n  1306→                timestamp: now,\n  1307→                step_type: StepType::Thinking {\n  1308→                    content: format!(\"Step {}\", i),\n  1309→                    signature: None,\n  1310→                },\n  1311→                tokens_in: Some(10),\n  1312→                tokens_out: Some(5),\n  1313→                tokens_cached: None,\n  1314→            });\n  1315→        }\n  1316→\n  1317→        let analysis = analyze_trajectory(&trajectory);\n  1318→\n  1319→        // Should complete without timeout or memory issues\n  1320→        assert_eq!(analysis.quality.thinking_blocks, 10_000);\n  1321→    }\n  1322→\n  1323→    #[test]\n  1324→    fn test_latency_stats_single_value() {\n  1325→        let latencies = vec![100];\n  1326→        let stats = compute_latency_stats(&latencies);\n  1327→\n  1328→        // Single value should work correctly\n  1329→        assert_eq!(stats.min_ms, 100);\n  1330→        assert_eq!(stats.max_ms, 100);\n  1331→        assert_eq!(stats.mean_ms, 100.0);\n  1332→        assert_eq!(stats.p50_ms, 100);\n  1333→        assert_eq!(stats.p95_ms, 100);\n  1334→        assert_eq!(stats.count, 1);\n  1335→    }\n  1336→\n  1337→    #[test]\n  1338→    fn test_latency_stats_empty() {\n  1339→        let latencies = vec![];\n  1340→        let stats = compute_latency_stats(&latencies);\n  1341→\n  1342→        // Empty latencies should return default\n  1343→        assert_eq!(stats.count, 0);\n  1344→        assert_eq!(stats.min_ms, 0);\n  1345→        assert_eq!(stats.max_ms, 0);\n  1346→    }\n  1347→\n  1348→    #[test]\n  1349→    fn test_detect_parallel_batches_empty() {\n  1350→        let intervals = vec![];\n  1351→        let batches = detect_parallel_batches(&intervals);\n  1352→\n  1353→        assert_eq!(batches, 0);\n  1354→    }\n  1355→\n  1356→    #[test]\n  1357→    fn test_detect_parallel_batches_no_overlap() {\n  1358→        let now = Utc::now();\n  1359→        let intervals = vec![\n  1360→            (now, now + chrono::Duration::milliseconds(100)),\n  1361→            (now + chrono::Duration::milliseconds(200), now + chrono::Duration::milliseconds(300)),\n  1362→        ];\n  1363→\n  1364→        let batches = detect_parallel_batches(&intervals);\n  1365→\n  1366→        // No overlap = 2 separate batches\n  1367→        assert_eq!(batches, 2);\n  1368→    }\n  1369→\n  1370→    #[test]\n  1371→    fn test_detect_parallel_batches_complete_overlap() {\n  1372→        let now = Utc::now();\n  1373→        let intervals = vec![\n  1374→            (now, now + chrono::Duration::milliseconds(500)),\n  1375→            (now + chrono::Duration::milliseconds(100), now + chrono::Duration::milliseconds(200)),\n  1376→            (now + chrono::Duration::milliseconds(300), now + chrono::Duration::milliseconds(400)),\n  1377→        ];\n  1378→\n  1379→        let batches = detect_parallel_batches(&intervals);\n  1380→\n  1381→        // All overlap within first interval = 1 batch\n  1382→        assert_eq!(batches, 1);\n  1383→    }\n  1384→\n  1385→    #[test]\n  1386→    fn test_aggregate_analyses_empty() {\n  1387→        let analyses = vec![];\n  1388→        let aggregate = aggregate_analyses(&analyses);\n  1389→\n  1390→        assert_eq!(aggregate.trajectory_count, 0);\n  1391→        assert_eq!(aggregate.total_cost_usd, 0.0);\n  1392→        assert!(aggregate.by_model.is_empty());\n  1393→    }\n  1394→\n  1395→    #[test]\n  1396→    fn test_aggregate_analyses_single() {\n  1397→        let trajectory = create_test_trajectory();\n  1398→        let analysis = analyze_trajectory(&trajectory);\n  1399→\n  1400→        let aggregate = aggregate_analyses(&[analysis]);\n  1401→\n  1402→        assert_eq!(aggregate.trajectory_count, 1);\n  1403→        assert!(aggregate.total_cost_usd > 0.0);\n  1404→    }\n  1405→\n  1406→    #[test]\n  1407→    fn test_tool_error_tracking_all_successful() {\n  1408→        let mut trajectory = create_test_trajectory();\n  1409→        let now = Utc::now();\n  1410→\n  1411→        trajectory.steps = vec![\n  1412→            Step {\n  1413→                step_id: 1,\n  1414→                timestamp: now,\n  1415→                step_type: StepType::ToolCall {\n  1416→                    tool: \"Read\".to_string(),\n  1417→                    tool_id: \"t1\".to_string(),\n  1418→                    input: serde_json::json!({}),\n  1419→                },\n  1420→                tokens_in: None,\n  1421→                tokens_out: None,\n  1422→                tokens_cached: None,\n  1423→            },\n  1424→            Step {\n  1425→                step_id: 2,\n  1426→                timestamp: now + chrono::Duration::milliseconds(100),\n  1427→                step_type: StepType::ToolResult {\n  1428→                    tool_id: \"t1\".to_string(),\n  1429→                    success: true,\n  1430→                    output: Some(\"ok\".to_string()),\n  1431→                },\n  1432→                tokens_in: None,\n  1433→                tokens_out: None,\n  1434→                tokens_cached: None,\n  1435→            },\n  1436→        ];\n  1437→\n  1438→        let analysis = analyze_trajectory(&trajectory);\n  1439→\n  1440→        assert_eq!(analysis.errors.total_tool_calls, 1);\n  1441→        assert_eq!(analysis.errors.failed_tool_calls, 0);\n  1442→        assert_eq!(analysis.errors.tool_error_rate, 0.0);\n  1443→        assert!(analysis.errors.errors_by_tool.is_empty());\n  1444→    }\n  1445→\n  1446→    #[test]\n  1447→    fn test_tool_error_tracking_all_failed() {\n  1448→        let mut trajectory = create_test_trajectory();\n  1449→        let now = Utc::now();\n  1450→\n  1451→        trajectory.steps = vec![\n  1452→            Step {\n  1453→                step_id: 1,\n  1454→                timestamp: now,\n  1455→                step_type: StepType::ToolCall {\n  1456→                    tool: \"Read\".to_string(),\n  1457→                    tool_id: \"t1\".to_string(),\n  1458→                    input: serde_json::json!({}),\n  1459→                },\n  1460→                tokens_in: None,\n  1461→                tokens_out: None,\n  1462→                tokens_cached: None,\n  1463→            },\n  1464→            Step {\n  1465→                step_id: 2,\n  1466→                timestamp: now + chrono::Duration::milliseconds(100),\n  1467→                step_type: StepType::ToolResult {\n  1468→                    tool_id: \"t1\".to_string(),\n  1469→                    success: false,\n  1470→                    output: Some(\"error\".to_string()),\n  1471→                },\n  1472→                tokens_in: None,\n  1473→                tokens_out: None,\n  1474→                tokens_cached: None,\n  1475→            },\n  1476→        ];\n  1477→\n  1478→        let analysis = analyze_trajectory(&trajectory);\n  1479→\n  1480→        assert_eq!(analysis.errors.total_tool_calls, 1);\n  1481→        assert_eq!(analysis.errors.failed_tool_calls, 1);\n  1482→        assert_eq!(analysis.errors.tool_error_rate, 1.0);\n  1483→        assert_eq!(analysis.errors.errors_by_tool.get(\"Read\"), Some(&1));\n  1484→    }\n  1485→\n  1486→    #[test]\n  1487→    fn test_tool_diversity_single_tool_many_calls() {\n  1488→        let mut trajectory = create_test_trajectory();\n  1489→        let now = Utc::now();\n  1490→\n  1491→        // 5 calls to same tool\n  1492→        for i in 0..5 {\n  1493→            trajectory.steps.push(Step {\n  1494→                step_id: i,\n  1495→                timestamp: now,\n  1496→                step_type: StepType::ToolCall {\n  1497→                    tool: \"Read\".to_string(),\n  1498→                    tool_id: format!(\"t{}\", i),\n  1499→                    input: serde_json::json!({}),\n  1500→                },\n  1501→                tokens_in: None,\n  1502→                tokens_out: None,\n  1503→                tokens_cached: None,\n  1504→            });\n  1505→        }\n  1506→\n  1507→        let analysis = analyze_trajectory(&trajectory);\n  1508→\n  1509→        // 1 unique tool / 5 calls = 0.2 diversity\n  1510→        assert_eq!(analysis.quality.unique_tools, 1);\n  1511→        assert!((analysis.quality.tool_diversity - 0.2).abs() < 0.01);\n  1512→    }\n  1513→\n  1514→    #[test]\n  1515→    fn test_tool_diversity_many_tools_one_call_each() {\n  1516→        let mut trajectory = create_test_trajectory();\n  1517→        let now = Utc::now();\n  1518→\n  1519→        // 5 different tools, 1 call each\n  1520→        for i in 0..5 {\n  1521→            trajectory.steps.push(Step {\n  1522→                step_id: i,\n  1523→                timestamp: now,\n  1524→                step_type: StepType::ToolCall {\n  1525→                    tool: format!(\"Tool{}\", i),\n  1526→                    tool_id: format!(\"t{}\", i),\n  1527→                    input: serde_json::json!({}),\n  1528→                },\n  1529→                tokens_in: None,\n  1530→                tokens_out: None,\n  1531→                tokens_cached: None,\n  1532→            });\n  1533→        }\n  1534→\n  1535→        let analysis = analyze_trajectory(&trajectory);\n  1536→\n  1537→        // 5 unique tools / 5 calls = 1.0 diversity\n  1538→        assert_eq!(analysis.quality.unique_tools, 5);\n  1539→        assert_eq!(analysis.quality.tool_diversity, 1.0);\n  1540→    }\n  1541→\n  1542→    #[test]\n  1543→    fn test_truncate_very_long_prompt() {\n  1544→        let long_prompt = \"a\".repeat(500);\n  1545→        let truncated = truncate(&long_prompt, 80);\n  1546→\n  1547→        assert_eq!(truncated.len(), 80);\n  1548→        assert!(truncated.ends_with(\"...\"));\n  1549→    }\n  1550→\n  1551→    #[test]\n  1552→    fn test_truncate_short_prompt() {\n  1553→        let short_prompt = \"Short prompt\";\n  1554→        let truncated = truncate(short_prompt, 80);\n  1555→\n  1556→        assert_eq!(truncated, \"Short prompt\");\n  1557→    }\n  1558→\n  1559→    #[test]\n  1560→    fn test_tokens_by_step_type_all_types() {\n  1561→        let mut trajectory = create_test_trajectory();\n  1562→        let now = Utc::now();\n  1563→\n  1564→        trajectory.steps = vec![\n  1565→            Step {\n  1566→                step_id: 1,\n  1567→                timestamp: now,\n  1568→                step_type: StepType::Thinking {\n  1569→                    content: \"think\".to_string(),\n  1570→                    signature: None,\n  1571→                },\n  1572→                tokens_in: None,\n  1573→                tokens_out: Some(100),\n  1574→                tokens_cached: None,\n  1575→            },\n  1576→            Step {\n  1577→                step_id: 2,\n  1578→                timestamp: now,\n  1579→                step_type: StepType::Assistant {\n  1580→                    content: \"response\".to_string(),\n  1581→                },\n  1582→                tokens_in: None,\n  1583→                tokens_out: Some(50),\n  1584→                tokens_cached: None,\n  1585→            },\n  1586→            Step {\n  1587→                step_id: 3,\n  1588→                timestamp: now,\n  1589→                step_type: StepType::User {\n  1590→                    content: \"question\".to_string(),\n  1591→                },\n  1592→                tokens_in: None,\n  1593→                tokens_out: Some(25),\n  1594→                tokens_cached: None,\n  1595→            },\n  1596→        ];\n  1597→\n  1598→        let analysis = analyze_trajectory(&trajectory);\n  1599→\n  1600→        assert_eq!(analysis.cost.tokens_by_step_type.get(\"thinking\"), Some(&100));\n  1601→        assert_eq!(analysis.cost.tokens_by_step_type.get(\"assistant\"), Some(&50));\n  1602→        assert_eq!(analysis.cost.tokens_by_step_type.get(\"user\"), Some(&25));\n  1603→    }\n  1604→\n  1605→    #[test]\n  1606→    fn test_negative_timestamp_delta_latency() {\n  1607→        let mut trajectory = create_test_trajectory();\n  1608→        let now = Utc::now();\n  1609→\n  1610→        // Tool result BEFORE tool call (time anomaly)\n  1611→        trajectory.steps = vec![\n  1612→            Step {\n  1613→                step_id: 1,\n  1614→                timestamp: now + chrono::Duration::milliseconds(200),\n  1615→                step_type: StepType::ToolCall {\n  1616→                    tool: \"Read\".to_string(),\n  1617→                    tool_id: \"t1\".to_string(),\n  1618→                    input: serde_json::json!({}),\n  1619→                },\n  1620→                tokens_in: None,\n  1621→                tokens_out: None,\n  1622→                tokens_cached: None,\n  1623→            },\n  1624→            Step {\n  1625→                step_id: 2,\n  1626→                timestamp: now, // Earlier timestamp\n  1627→                step_type: StepType::ToolResult {\n  1628→                    tool_id: \"t1\".to_string(),\n  1629→                    success: true,\n  1630→                    output: None,\n  1631→                },\n  1632→                tokens_in: None,\n  1633→                tokens_out: None,\n  1634→                tokens_cached: None,\n  1635→            },\n  1636→        ];\n  1637→\n  1638→        let analysis = analyze_trajectory(&trajectory);\n  1639→\n  1640→        // Should handle gracefully with .max(0)\n  1641→        assert_eq!(analysis.performance.tool_latency_stats.min_ms, 0);\n  1642→    }\n  1643→}\n  1644→\n  1645→/// Calculate improvement velocity from trends\n  1646→pub fn calculate_velocity(\n  1647→    db: &MetricsDb,\n  1648→    period: TimePeriod,\n  1649→) -> anyhow::Result<crate::metrics::VelocitySnapshot> {\n  1650→    use crate::metrics::{MetricVelocity, VelocitySnapshot};\n  1651→    use chrono::Utc;\n  1652→\n  1653→    // Get trends comparing this period vs previous period\n  1654→    let baseline_period = match period {\n  1655→        TimePeriod::Last7Days | TimePeriod::ThisWeek => Some(TimePeriod::LastWeek),\n  1656→        TimePeriod::Last30Days => Some(TimePeriod::Last30Days), // Compare vs same period\n  1657→        _ => None,\n  1658→    };\n  1659→\n  1660→    let trends = detect_trends(db, period, baseline_period)?;\n  1661→\n  1662→    let mut improving_count = 0;\n  1663→    let mut degrading_count = 0;\n  1664→    let mut stable_count = 0;\n  1665→    let mut key_metrics = Vec::new();\n  1666→\n  1667→    // Key metrics to track for velocity\n  1668→    let key_metric_names = [\"tool_error_rate\",\n  1669→        \"completion_rate\",\n  1670→        \"cost_per_issue\",\n  1671→        \"duration_per_issue\"];\n  1672→\n  1673→    for trend in trends {\n  1674→        // Only include key metrics in the snapshot\n  1675→        if key_metric_names.contains(&trend.dimension.as_str()) {\n  1676→            let direction_str = match trend.direction {\n  1677→                TrendDirection::Improving => {\n  1678→                    improving_count += 1;\n  1679→                    \"improving\"\n  1680→                }\n  1681→                TrendDirection::Degrading => {\n  1682→                    degrading_count += 1;\n  1683→                    \"degrading\"\n  1684→                }\n  1685→                TrendDirection::Stable => {\n  1686→                    stable_count += 1;\n  1687→                    \"stable\"\n  1688→                }\n  1689→            };\n  1690→\n  1691→            key_metrics.push(MetricVelocity {\n  1692→                dimension: trend.dimension.clone(),\n  1693→                percent_change: trend.percent_change,\n  1694→                direction: direction_str.to_string(),\n  1695→            });\n  1696→        } else {\n  1697→            // Still count for the totals\n  1698→            match trend.direction {\n  1699→                TrendDirection::Improving => improving_count += 1,\n  1700→                TrendDirection::Degrading => degrading_count += 1,\n  1701→                TrendDirection::Stable => stable_count += 1,\n  1702→            }\n  1703→        }\n  1704→    }\n  1705→\n  1706→    // Calculate velocity score (-1.0 to 1.0)\n  1707→    // Weighted: improving metrics add positive, degrading subtract\n  1708→    let total_metrics = (improving_count + degrading_count + stable_count) as f64;\n  1709→    let velocity_score = if total_metrics > 0.0 {\n  1710→        (improving_count as f64 - degrading_count as f64) / total_metrics\n  1711→    } else {\n  1712→        0.0\n  1713→    };\n  1714→\n  1715→    // Count total issues completed in period\n  1716→    let sessions = get_sessions_in_period(db, period)?;\n  1717→    let issues_completed: i32 = sessions.iter().map(|s| s.issues_completed).sum();\n  1718→\n  1719→    Ok(VelocitySnapshot {\n  1720→        timestamp: Utc::now(),\n  1721→        period: period.name().to_string(),\n  1722→        velocity_score,\n  1723→        improving_metrics: improving_count,\n  1724→        degrading_metrics: degrading_count,\n  1725→        stable_metrics: stable_count,\n  1726→        issues_completed,\n  1727→        key_metrics,\n  1728→    })\n  1729→}\n  1730→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"timestamp":"2025-12-23T20:58:12.774724995+00:00"}