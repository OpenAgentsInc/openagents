# 1614 Phase 3.5 Validation Campaign

## Objective
Run FM validation campaign to measure learning loop effectiveness after Phase 3 implementation.

## Tasks
- [x] 3.5-A: Run baseline (no learning) - 3 iterations
- [x] 3.5-A: Run with learning - 3 iterations
- [x] 3.5-A: Write validation report
- [ ] 3.5-B: Smoke-test FM on real repo task
- [ ] 3.5-C: Capture issues/TODOs

## Progress

### Checking FM Bridge Status
- FM bridge healthy at localhost:11435
- Apple FM model available

### Bug Fix: FM_MAX_CONTEXT_CHARS
Fixed typo in `src/bench/model-adapter.ts:1290`:
- Was: `FM_MAX_CONTEXT_CHARS` (undefined)
- Fixed: `FM_MAX_CONTEXT_CHARS_DEFAULT`

### Baseline Campaign (--no-skills)
Run ID: `tbrun-20251206-223150-kchc`
- Iterations: 3
- Pass Rate: **28.6%** (consistent across all iterations)
- Tasks passed: hello-world, append-to-file (2/7)
- Tasks failed: read-and-echo, list-directory, create-and-run, simple-edit, word-count

### Learning Campaign (--skills --memory --reflect --learn)
Run ID: `tbrun-20251206-224908-0baa`
- Iterations: 3
- Pass Rate: **0.0%** (all iterations)
- Error: "Foundation Models request failed: Exceeded model context window size"

## Critical Bug Found

### Issue: Embedding calls interfere with task execution

When `--skills` is enabled, the skill retrieval system calls FM for embedding generation
BEFORE checking if any skills exist. This happens in `src/skills/retrieval.ts:126`:

```typescript
const queryEmbedding = yield* embedding.embed(q.query).pipe(
  Effect.mapError(SkillRetrievalError.fromEmbeddingError),
);
// ... only THEN checks if skills.length === 0
```

The embedding call appears to:
1. Send a request to FM for semantic extraction
2. This request somehow interferes with subsequent FM calls
3. Results in "Exceeded model context window size" errors for all task calls

Evidence:
- First task takes 1.8s (embedding + failed task call)
- Subsequent tasks take 0.3s (immediate failure)
- This pattern suggests FM gets into a bad state after the embedding call

### Root Cause Analysis

The FM bridge (foundation-bridge) appears to have state management issues:
- Concurrent or sequential requests may share context state
- The embedding call leaves FM in a corrupted state
- All subsequent calls fail with context exceeded errors

### Workaround
Use `--no-skills` to bypass the learning loop until this is fixed.

## Results Summary

| Mode | Pass Rate | Notes |
|------|-----------|-------|
| Baseline (--no-skills) | 28.6% | 2/7 tasks pass consistently |
| Learning (--skills) | 0.0% | All tasks fail with context error |

## Issues Discovered

1. **P0 - Critical**: Embedding calls break FM task execution
   - Location: `src/skills/retrieval.ts:126`
   - Impact: Learning mode completely broken
   - Fix: Check skills.length BEFORE generating query embedding

2. **P1 - High**: FM bridge state management
   - The bridge doesn't properly isolate requests
   - May need to restart bridge between embedding and task calls

3. **P2 - Medium**: Duplicate skills in library
   - Archivist creating skills with same ID but different timestamps
   - Need deduplication logic

4. **P2 - Medium**: Large memory file (85KB, 114 entries)
   - Memory injection would blow FM context
   - Need truncation/selection logic

## Recommended Next Steps

1. Fix the embedding call order bug in `src/skills/retrieval.ts`
2. Add early-exit check: if skill store is empty, skip embedding entirely
3. Consider disabling FM-based embeddings for now (use hash-based only)
4. Add request isolation to FM bridge
5. Re-run validation after fixes

## Commits Made

- Bug fix: `FM_MAX_CONTEXT_CHARS` -> `FM_MAX_CONTEXT_CHARS_DEFAULT` (to be committed)

## Files Modified/Created

- `src/bench/model-adapter.ts` - Fixed undefined constant
- `tasks/fm-mini-suite.json` - Fixed setup format in earlier session
- `.openagents/skills/library.jsonl.backup` - Backed up corrupted skills
- `.openagents/memories.jsonl.backup` - Backed up large memory file
- `.openagents/memory/reflections.jsonl.backup` - Backed up reflections
