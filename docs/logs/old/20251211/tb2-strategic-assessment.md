# Terminal-Bench 2: Strategic Assessment & Conquest Plan

**Date:** 2024-12-11 19:00
**Status:** Strategic Planning Complete
**Goal:** Systematic approach to solving all 90 TB2 tasks

---

## Executive Summary

This assessment evaluates all 90 Terminal-Bench 2 tasks to determine:
1. Which tasks are achievable with current TestGen v2 architecture
2. Which tasks require enhanced approaches or new tooling
3. A phased roadmap to maximize TB2 coverage

### Key Findings

| Category | Task Count | Approach | Expected Success Rate |
|----------|------------|----------|----------------------|
| **A: TestGen v2 Ready** | 25 | Use as-is | 70%+ |
| **B: Enhanced TestGen** | 20 | Add PARSE INPUTS phase | 60%+ |
| **C: Specialized Tooling** | 30 | Build skill libraries | 50%+ |
| **D: Manual/Creative** | 15 | Claude Opus + extended time | 40%+ |

**Current Validation:** 3/4 tasks passed (75% success rate)
- ‚úÖ regex-log (medium, 2 runs)
- ‚úÖ log-summary-date-ranges (medium)
- ‚ùå constraints-scheduling (medium, input file parsing issue)

---

## Part 1: Complete Task Catalog

### TB2 Distribution Overview

**Total Tasks:** 90
**By Difficulty:**
- Easy: 4 (4.4%)
- Medium: 53 (58.9%)
- Hard: 33 (36.7%)

**By Category:**
- software-engineering: 26 tasks
- system-administration: 9 tasks
- security: 8 tasks
- data-science: 8 tasks
- scientific-computing: 8 tasks
- debugging: 5 tasks
- file-operations: 5 tasks
- model-training: 4 tasks
- mathematics: 3 tasks
- 14 other categories

---

## Part 2: TestGen v2 Architecture Analysis

### What Makes TestGen v2 Succeed

**Success Pattern (3/3 passed tasks):**

1. **Self-Contained Task Structure**
   - All information needed is in the task description
   - No external file semantics to interpret
   - Example: regex-log specifies exact date format, IPv4 requirement, boundary rules

2. **Deterministic Validation**
   - Binary pass/fail outcomes
   - No interpretation required
   - Example: CSV format either matches spec or doesn't

3. **Entity-Constraint Matrix Coverage**
   - Systematic test generation prevents gaps
   - Each constraint √ó entity combination tested
   - Example: regex-log generated 66-83 comprehensive tests

4. **Subagent Review Quality**
   - Iteratively finds missing edge cases
   - Example: Review added tests for month range (01-12), IPv4 octet boundaries

5. **Cost Efficiency**
   - ~$0.03 per run with Haiku 4.5
   - 3-5 minutes total time
   - 30-80 comprehensive tests generated

### What Makes TestGen v2 Fail

**Failure Pattern (constraints-scheduling):**

**Root Cause:** External data dependency mismatch

The task required:
- Read calendar files (alice_calendar.ics, bob_calendar.ics, carol_calendar.ics)
- Parse existing meetings with times/dates
- Schedule new meeting avoiding conflicts

**What Claude did:**
- Generated simplified test fixtures (synthetic calendar data)
- Created 16 tests against simplified data
- Tests PASSED against synthetic data
- TB2 verification FAILED against real benchmark calendars

**The Problem:**
- Alice's actual calendar: `alice-003: 10:00-12:00 Tuesday Jan 16`
- Claude's synthetic calendar: Didn't include this conflict
- Solution scheduled 10:00-11:00 meeting ‚Üí **CONFLICT**

### Architectural Limitation

TestGen v2's deterministic expansion works from:
```
Task Description ‚Üí Entities/Constraints ‚Üí Matrix ‚Üí Test Scaffold
```

This assumes everything testable is **derivable from the description.**

For input-dependent tasks:
- Real input files have complexity NOT in description
- FM generates simplified test fixtures
- Tests validate against wrong data
- Solution fails on real data

---

## Part 3: Task Categorization by Approach

### Category A: TestGen v2 Ready (25 tasks)

**Characteristics:**
- ‚úÖ Self-contained (all info in description)
- ‚úÖ Deterministic validation
- ‚úÖ Generatable fixtures from rules
- ‚úÖ Pattern-based (regex, parsing, format validation)

**Confirmed Working (2):**
1. **regex-log** (medium, 45 min)
   - Pattern matching with constraints
   - 66-83 tests, TB2 PASS (2 runs)

2. **log-summary-date-ranges** (medium, 75 min)
   - Log parsing + date aggregation
   - 32 tests, TB2 PASS

**High Confidence - Ready to Test (15):**

3. **overfull-hbox** (easy, 60 min)
   - LaTeX overfull hbox fixing
   - Self-contained: LaTeX rules in instruction

4. **prove-plus-comm** (easy, 5 min)
   - Formal proof: addition commutativity
   - Self-contained: Mathematical proof

5. **filter-js-from-html** (medium, 45 min)
   - JavaScript removal from HTML
   - Self-contained: Filter logic specified

6. **largest-eigenval** (medium, 60 min)
   - Eigenvalue computation optimization
   - Self-contained: Matrix operations

7. **adaptive-rejection-sampler** (medium, 180 min)
   - Statistical sampling algorithm
   - Self-contained: Algorithm from research paper (Gilks et al. 1992)

8. **cancel-async-tasks** (hard, 120 min)
   - Python asyncio graceful shutdown
   - Self-contained: Async pattern specified

9. **headless-terminal** (medium, 120 min)
   - Terminal emulation implementation
   - Self-contained: Terminal protocol

10. **kv-store-grpc** (medium, 15 min)
    - Key-value store with gRPC interface
    - Self-contained: KV store logic

11. **schemelike-metacircular-eval** (medium, 300 min)
    - Scheme interpreter
    - Self-contained: Language spec in instruction

12. **polyglot-c-py** (medium, 20 min)
    - File valid as both C and Python
    - Self-contained: Syntax knowledge

13. **winning-avg-corewars** (medium, 60 min)
    - CoreWars Redcode program
    - Self-contained: CoreWars rules

14. **git-multibranch** (medium, 180 min)
    - Git multi-branch workflow
    - Self-contained: Git operations

15. **nginx-request-logging** (medium, 20 min)
    - Nginx log configuration
    - Self-contained: Nginx config format

16. **openssl-selfsigned-cert** (medium, 20 min)
    - SSL certificate generation
    - Self-contained: OpenSSL commands

17. **llm-inference-batching-scheduler** (hard, 45 min)
    - LLM batching scheduler logic
    - Self-contained: Scheduler algorithm

**Moderate Confidence - Needs Validation (8):**

18. **fix-git** (easy, 5 min)
19. **pypi-server** (medium, 60 min)
20. **qemu-startup** (medium, 30 min)
21. **qemu-alpine-ssh** (medium, 30 min)
22. **sqlite-with-gcov** (medium, 30 min)
23. **configure-git-webserver** (medium/hard, 15 min)
24. **mailman** (medium, 60 min)
25. **regex-chess** (hard, 1440 min) - Extremely complex but theoretically self-contained

---

### Category B: Enhanced TestGen (20 tasks)

**Characteristics:**
- ‚ö†Ô∏è External input files required
- ‚ö†Ô∏è File contents affect test validity
- ‚ö†Ô∏è Complex semantics (calendars, configs, databases)
- **Solution:** Add PARSE INPUTS phase before ANALYZE

**Known Failure (1):**
1. **constraints-scheduling** (medium, 15 min)
   - Calendar conflict resolution
   - FAILED: Created synthetic calendars instead of parsing real ones

**Needs Enhanced Protocol (19):**

2. **cobol-modernization** (easy, 20 min)
   - Re-implement COBOL in Python
   - Requires analyzing existing COBOL code

3. **multi-source-data-merger** (medium, 30 min)
   - ETL with schema mapping, conflict resolution
   - Requires parsing Parquet files

4. **financial-document-processor** (medium, 30 min)
   - OCR financial documents from images
   - Requires reading actual images

5. **reshard-c4-data** (medium, 30 min)
   - Dataset resharding
   - Requires understanding C4 structure

6. **break-filter-js-from-html** (medium, 20 min)
   - XSS exploit against filter.py
   - Requires analyzing filter implementation

7. **dna-insert** (medium, 30 min)
   - DNA sequence insertion
   - Requires parsing sequence files

8. **raman-fitting** (medium, 5 min)
   - Raman spectroscopy data fitting
   - Requires actual spectroscopy data

9. **tune-mjcf** (medium, 30 min)
   - MuJoCo physics parameter tuning
   - Requires parsing MJCF files

10. **modernize-scientific-stack** (medium, 120 min)
    - Migrate legacy scientific Python code
    - Requires analyzing legacy codebase

11. **build-cython-ext** (medium, 60 min)
    - Fix Numpy 2.x compatibility in pyknotid
    - Requires reading source code

12. **build-pmars** (medium, 90 min)
    - Build pMARS from Debian sources
    - Requires source download and modification

13. **build-pov-ray** (medium, 60 min)
    - Build POV-Ray 2.2 from source
    - Requires legacy source compilation

14. **compile-compcert** (medium, 60 min)
    - Build CompCert verified compiler
    - Requires OCaml build system

15. **git-leak-recovery** (medium, 30 min)
    - Recover leaked secrets from git history
    - Requires parsing git objects

16. **sanitize-git-repo** (medium, 30 min)
    - Remove sensitive data from git
    - Requires analyzing git history

17. **vulnerable-secret** (medium, 20 min)
    - Find/fix vulnerable secrets
    - Requires scanning codebase

18. **crack-7z-hash** (medium, 5 min)
    - Crack password-protected 7z archive
    - Requires 7z file

19. **db-wal-recovery** (medium, 45 min)
    - Database Write-Ahead Log recovery
    - Requires parsing WAL format

20. **extract-elf** (medium, 30 min)
    - Extract memory values from ELF binary
    - Requires parsing binary file

21. **gcode-to-text** (medium, 60 min)
    - Convert G-code to text description
    - Requires parsing G-code files

22. **sqlite-db-truncate** (medium, 60 min)
    - Fix truncated SQLite database
    - Requires SQLite file format knowledge

23. **merge-diff-arc-agi-task** (medium, 20 min)
    - Resolve git merge conflicts
    - Requires analyzing conflict markers

24. **code-from-image** (medium, 30 min)
    - Implement pseudocode from image
    - Requires OCR of /app/code.png

**Required Enhancement:**
```markdown
### Step 0: PARSE INPUTS (before ANALYZE step)

1. **Discover files:**
   - List all files in /app/, /tests/, environment
   - Identify input data vs infrastructure files

2. **Detect formats:**
   - JSON, CSV, Parquet, iCal, binary, images
   - Use file extension + magic bytes

3. **Extract structure:**
   - Schema (columns, fields, event types)
   - Constraints (date ranges, foreign keys, formats)
   - Data samples (representative examples)

4. **Generate tests using REAL characteristics:**
   - Test against actual schema
   - Use actual date ranges from data
   - Validate against real constraints

5. **Add validation tests:**
   - Solution must respect parsed constraints
   - Output must match input structure
```

---

### Category C: Specialized Tooling Required (30 tasks)

**Characteristics:**
- üîß Requires specific libraries/frameworks
- üß† Domain-specific knowledge (ML, crypto, bio)
- üõ†Ô∏è Multi-step setup (build from source, dependencies)

**ML/AI Tasks (8):**

1. **count-dataset-tokens** (medium, 30 min)
   - HuggingFace datasets, tokenizers
   - Requires: datasets library, model tokenizer

2. **hf-model-inference** (medium, 20 min)
   - HuggingFace API inference
   - Requires: transformers, API access

3. **mteb-leaderboard** (medium, 5 min)
   - MTEB benchmark leaderboard
   - Requires: MTEB framework

4. **mteb-retrieve** (medium, 15 min)
   - MTEB retrieval task
   - Requires: MTEB, retrieval systems

5. **caffe-cifar-10** (medium, unspecified)
   - Train CNN on CIFAR-10 with Caffe
   - Requires: BVLC Caffe 1.0.0, CIFAR-10 dataset

6. **pytorch-model-cli** (medium, 30 min)
   - PyTorch model CLI in C
   - Requires: PyTorch C++ API

7. **pytorch-model-recovery** (medium, 15 min)
   - Recover corrupted PyTorch model
   - Requires: PyTorch internals

8. **train-fasttext** (hard, 30 min)
   - fastText model training
   - Requires: fastText library, training data

**Scientific Computing (7):**

9. **rstan-to-pystan** (medium, 180 min)
   - Convert R Stan models to Python Stan
   - Requires: R, Python, Stan, Gaussian processes

10. **portfolio-optimization** (medium, 120 min)
    - Portfolio optimization in C with Python bindings
    - Requires: C compiler, Python extensions, optimization libs

11. **query-optimize** (medium, 60 min)
    - SQL query optimization
    - Requires: Database, query analyzer

12. **bn-fit-modify** (hard, 480 min)
    - Bayesian network DAG recovery + causal intervention
    - Requires: BN learning, causal inference libraries

13. **dna-assembly** (hard, 60 min)
    - Golden Gate assembly primer design
    - Requires: Primer3, BsaI-HF v2 recognition sites

14. **protein-assembly** (hard, 60 min)
    - Protein sequence assembly
    - Requires: Bioinformatics libraries

15. **mcmc-sampling-stan** (hard, 180 min)
    - MCMC sampling with Stan
    - Requires: R, rstan, Bayesian statistics

**System/Build Tasks (8):**

16. **custom-memory-heap-crash** (medium, 30 min)
    - Debug C++ custom memory heap
    - Requires: C++, memory debugging tools

17. **distribution-search** (medium, 120 min)
    - Find probability distribution with KL divergence properties
    - Requires: numpy, scipy, statistics

18. **large-scale-text-editing** (medium, 40 min)
    - Large-scale text transformations with vim macros
    - Requires: Vim, macro automation

19. **gpt2-codegolf** (hard, 2400 min)
    - GPT-2 in <5000 bytes C
    - Requires: GPT-2 architecture, extreme optimization

20. **write-compressor** (hard, 1440 min)
    - Custom compression algorithm
    - Requires: Compression theory, data structures

21. **make-doom-for-mips** (hard, 480 min)
    - Port Doom to MIPS
    - Requires: MIPS assembly, Doom source, cross-compilation

22. **make-mips-interpreter** (hard, 480 min)
    - MIPS instruction set interpreter
    - Requires: MIPS ISA specification, emulator design

23. **torch-pipeline-parallelism** (hard, 240 min)
    - PyTorch pipeline parallelism
    - Requires: PyTorch distributed training

24. **torch-tensor-parallelism** (hard, 240 min)
    - PyTorch tensor parallelism
    - Requires: PyTorch tensor sharding

**Security/Crypto (5):**

25. **feal-differential-cryptanalysis** (hard, 480 min)
    - Differential cryptanalysis on FEAL cipher
    - Requires: Cryptography, differential analysis algorithms

26. **feal-linear-cryptanalysis** (hard, 960 min)
    - Linear cryptanalysis on FEAL cipher
    - Requires: Cryptography, linear approximation tables

27. **model-extraction-relu-logits** (hard, 480 min)
    - Extract neural network via queries
    - Requires: ML security, model extraction techniques

28. **fix-code-vulnerability** (hard, 120 min)
    - Fix CWE vulnerabilities
    - Requires: Security analysis, vulnerability databases

29. **password-recovery** (hard, 100 min)
    - Recover lost passwords
    - Requires: Password cracking tools, forensics

**Graphics/Media (5):**

30. **path-tracing** (hard, 360 min)
    - Path tracing renderer implementation
    - Requires: Ray tracing algorithms, graphics math

31. **path-tracing-reverse** (hard, 120 min)
    - Reverse-engineer scene from path-traced image
    - Requires: Inverse rendering, optimization

32. **sam-cell-seg** (hard, 600 min)
    - Cell segmentation with Segment Anything Model
    - Requires: SAM model, histopathology data

33. **video-processing** (hard, 400 min)
    - Complex video processing pipeline
    - Requires: FFmpeg, video codecs

34. **extract-moves-from-video** (hard, 120 min)
    - Extract chess moves from video
    - Requires: Computer vision, chess engine

**Other Complex (6):**

35. **sparql-university** (hard, 800 min)
    - SPARQL query on university knowledge graph
    - Requires: SPARQL, RDF/Turtle, knowledge graphs

36. **circuit-fibsqrt** (hard, 960 min)
    - Logic gate circuit for fib(isqrt(N))%(2^32)
    - Requires: Digital logic design, algorithms

37. **install-windows-3.11** (hard, 300 min)
    - Install Windows 3.11 in QEMU with VNC
    - Requires: QEMU, legacy OS installation

38. **polyglot-rust-c** (hard, 180 min)
    - Rust/C polyglot (WARNING: no verified solution)
    - Requires: Rust, C, polyglot tricks

39. **fix-ocaml-gc** (hard, 1440 min)
    - Debug OCaml garbage collector
    - Requires: OCaml internals, GC debugging

40. **chess-best-move** (medium, 45 min)
    - Analyze chess board image, find best move
    - Requires: OCR/CV for board, chess engine

**Strategy:** Build reusable skill libraries for each domain.

**Proposed Skill Libraries:**
1. `ml-inference-skill.md` - HuggingFace, PyTorch patterns
2. `bioinformatics-skill.md` - DNA/protein manipulation, primer design
3. `build-from-source-skill.md` - Configure, make, install workflows
4. `crypto-analysis-skill.md` - Cipher implementation, attack templates
5. `scientific-python-skill.md` - Numpy, Scipy, Pandas, curve fitting

---

### Category D: Manual/Creative Tasks (15 tasks)

**Characteristics:**
- üé® Requires human judgment or creativity
- üé≤ Non-deterministic validation
- ‚è±Ô∏è Extremely high expert time (>480 min)
- üß© Research-level challenges

**Extreme Outliers:**

1. **feal-linear-cryptanalysis** (hard, 960 min expert, 19200 min junior)
   - Research-level cryptographic attack
   - 320 hours for junior developer

2. **feal-differential-cryptanalysis** (hard, 480 min expert, 19200 min junior)
   - Research-level cryptographic attack
   - 320 hours for junior developer

3. **sparql-university** (hard, 800 min expert, 10000 min junior)
   - Complex knowledge graph reasoning
   - 166.7 hours for junior developer

4. **gpt2-codegolf** (hard, 2400 min expert, 9600 min junior)
   - Extreme optimization challenge
   - 160 hours for junior developer

5. **regex-chess** (hard, 1440 min expert, 4800 min junior)
   - Generate ALL chess moves via regex only
   - 80 hours for junior developer

6. **write-compressor** (hard, 1440 min expert, 4800 min junior)
   - Custom compression algorithm
   - 80 hours for junior developer

**Other High-Complexity:**

7. **circuit-fibsqrt** (hard, 960 min)
8. **fix-ocaml-gc** (hard, 1440 min)
9. **schemelike-metacircular-eval** (medium, 300 min)
10. **make-doom-for-mips** (hard, 480 min)
11. **make-mips-interpreter** (hard, 480 min)
12. **path-tracing** (hard, 360 min)
13. **sam-cell-seg** (hard, 600 min)
14. **video-processing** (hard, 400 min)
15. **bn-fit-modify** (hard, 480 min)

**Strategy:** Claude Opus with extended timeouts, potentially human-in-the-loop validation.

---

## Part 4: Phased Execution Roadmap

### Phase 1: Validate TestGen v2 (Week 1)

**Goal:** Confirm TestGen v2 success rate on Category A tasks.

**Tasks (10 selected for diversity):**
1. overfull-hbox (easy, LaTeX)
2. prove-plus-comm (easy, formal proof)
3. fix-git (easy, git operations)
4. filter-js-from-html (medium, HTML parsing)
5. largest-eigenval (medium, numerical computation)
6. headless-terminal (medium, terminal emulation)
7. kv-store-grpc (medium, distributed systems)
8. nginx-request-logging (medium, system admin)
9. openssl-selfsigned-cert (medium, security)
10. git-multibranch (medium, git workflow)

**Success Criteria:**
- 70%+ pass rate ‚Üí TestGen v2 validated for Category A
- <70% pass rate ‚Üí identify failure patterns, refine protocol

**Deliverables:**
- `docs/logs/20251211/tb2-phase1-results.md`
- Track: task_id, tests_generated, tb2_result, cost, duration, failure_reason
- ATIF trajectories in `results/trajectories/{task-id}/`

**Estimated:**
- Duration: 1 week
- Cost: ~$5 (10 tasks √ó $0.50 avg)
- Expected pass: 7/10 tasks

---

### Phase 2: Enhanced TestGen (Week 2)

**Goal:** Implement and validate PARSE INPUTS enhancement.

**Implementation Steps:**
1. Create `.claude/skills/testgen-protocol/parse_inputs_phase.md`
2. Update `crates/gym/src/mechacoder/testgen_wrapper.rs` to inject Phase 0
3. Test on constraints-scheduling (known failure)
4. Expand to 5 more Category B tasks

**Tasks (6 selected):**
1. constraints-scheduling (medium) - **Retest** with enhancement
2. cobol-modernization (easy) - Simple file analysis
3. build-cython-ext (medium) - Source code reading
4. git-leak-recovery (medium) - Git object parsing
5. extract-elf (medium) - Binary parsing
6. code-from-image (medium) - OCR + implementation

**Success Criteria:**
- constraints-scheduling NOW passes
- 60%+ pass rate on Category B tasks (4/6)

**Deliverables:**
- `docs/logs/20251211/tb2-phase2-results.md`
- Enhanced TestGen protocol implementation
- Validation metrics

**Estimated:**
- Duration: 1 week
- Cost: ~$10
- Expected pass: 4/6 tasks

---

### Phase 3: Specialized Skill Libraries (Weeks 3-4)

**Goal:** Build reusable skill libraries for common domains.

**Libraries to Create:**

1. **ml-inference-skill.md**
   - HuggingFace model loading
   - PyTorch inference patterns
   - Common ML preprocessing

2. **bioinformatics-skill.md**
   - DNA/protein sequence manipulation
   - Primer design workflows
   - FASTA/FASTQ parsing

3. **build-from-source-skill.md**
   - Configure, make, install patterns
   - Dependency resolution
   - Compilation error debugging

4. **crypto-analysis-skill.md**
   - Cipher implementation templates
   - Differential/linear cryptanalysis patterns
   - Common attack frameworks

5. **scientific-python-skill.md**
   - Numpy/Scipy patterns
   - Curve fitting workflows
   - Statistical distributions

**Validation Tasks (25 total, 5 per library):**

**ML Library:**
- count-dataset-tokens
- hf-model-inference
- caffe-cifar-10
- pytorch-model-recovery
- train-fasttext

**Bio Library:**
- dna-insert
- dna-assembly
- protein-assembly
- (+ 2 from other categories)

**Build Library:**
- build-pmars
- build-pov-ray
- compile-compcert
- build-cython-ext
- (+ 1 from other categories)

**Crypto Library:**
- feal-differential-cryptanalysis
- feal-linear-cryptanalysis
- model-extraction-relu-logits
- (+ 2 from security tasks)

**Science Library:**
- raman-fitting
- adaptive-rejection-sampler
- bn-fit-modify
- distribution-search
- rstan-to-pystan

**Success Criteria:**
- 50%+ pass rate per library
- Each skill demonstrably reusable

**Deliverables:**
- 5 skill library files
- 25 task attempts logged
- Skill usage patterns documented

**Estimated:**
- Duration: 2 weeks
- Cost: ~$50 (mixed Haiku/Opus)
- Expected pass: 12-15/25 tasks

---

### Phase 4: Hard Task Assault (Weeks 5-8)

**Goal:** Tackle hard tasks with specialized approaches.

**Tier 1: Hard but TestGen-compatible (use Haiku):**
- cancel-async-tasks
- llm-inference-batching-scheduler
- polyglot-c-py
- headless-terminal (if not done in Phase 1)

**Tier 2: Hard with skills (use Haiku + skills):**
- sam-cell-seg (ML inference skill)
- mcmc-sampling-stan (scientific-python skill)
- dna-assembly (bioinformatics skill)
- path-tracing (graphics algorithms)
- video-processing (FFmpeg patterns)

**Tier 3: Extreme hard (use Opus, extended timeouts):**
- gpt2-codegolf (2400 min expert)
- feal-linear-cryptanalysis (960 min expert)
- sparql-university (800 min expert)
- circuit-fibsqrt (960 min expert)
- regex-chess (1440 min expert)

**Success Criteria:**
- 40%+ pass rate on hard tasks
- Identify which extreme tasks are tractable

**Deliverables:**
- Logs for all hard task attempts
- Analysis: which hard tasks are solvable with current architecture
- Recommendations for future improvements

**Estimated:**
- Duration: 4 weeks
- Cost: ~$100 (mostly Opus)
- Expected pass: 13-16/33 hard tasks

---

### Phase 5: Completionist Cleanup (Weeks 9-12)

**Goal:** Solve remaining tasks, optimize pass rate.

**Activities:**
1. Retry all failed tasks with refined protocols
2. Use HillClimber for overnight MAP-Elites optimization
3. A/B test Haiku vs Opus on borderline tasks
4. Document all solutions in `results/trajectories/`
5. Generate TB2 leaderboard submission

**Focus Areas:**
- Low-hanging fruit (tasks that nearly passed)
- Tasks with partial solutions
- Tasks where we learned new patterns

**Success Criteria:**
- 80%+ retry success rate on previously-failed tasks
- 50+ total tasks passing (55%+ of TB2)

**Deliverables:**
- Final TB2 leaderboard submission
- Complete documentation of all attempts
- Lessons learned report
- Architecture improvement recommendations

**Estimated:**
- Duration: 4 weeks
- Cost: ~$50
- Expected final total: 50-60/90 tasks passing

---

## Part 5: Success Metrics & Targets

### Per-Phase Targets

| Phase | Tasks | Target Pass | Actual Cost | Time |
|-------|-------|-------------|-------------|------|
| 1 (Validation) | 10 | 7/10 (70%) | $5 | Week 1 |
| 2 (Enhanced) | 6 | 4/6 (67%) | $10 | Week 2 |
| 3 (Skills) | 25 | 13/25 (52%) | $50 | Weeks 3-4 |
| 4 (Hard) | 33 | 13/33 (39%) | $100 | Weeks 5-8 |
| 5 (Cleanup) | 16 | 13/16 (81%) | $50 | Weeks 9-12 |
| **TOTAL** | **90** | **50/90 (56%)** | **$215** | **12 weeks** |

### Cost Breakdown by Model

| Model | Use Case | Est. Cost | Tasks |
|-------|----------|-----------|-------|
| Haiku 4.5 | Category A (TestGen v2) | $50 | 40-50 |
| Haiku 4.5 | Category B (Enhanced) | $30 | 15-20 |
| Haiku 4.5 | Category C (With skills) | $50 | 15-20 |
| Opus 4.5 | Category D (Extreme hard) | $85 | 10-15 |
| **TOTAL** | | **~$215** | **80-105 attempts** |

### Quality Metrics

**Per Task:**
- TestGen tests generated (30-80 expected)
- TB2 verification result (pass/fail)
- Cost per task (<$0.50 for Haiku, <$5 for Opus)
- Duration (3-10 minutes for Haiku, 30-120 for Opus)
- Failure reason (if failed)

**Aggregate:**
- Pass rate by category (A: 70%, B: 60%, C: 50%, D: 40%)
- Pass rate by difficulty (Easy: 90%, Medium: 60%, Hard: 40%)
- Average cost per task
- Average cost per pass
- TestGen v2 effectiveness (tasks where it helped vs hindered)

---

## Part 6: Infrastructure & Tooling

### Current Infrastructure (Ready)

**Harbor Framework:**
- Location: `/home/christopherdavid/code/harbor`
- Official TB2 evaluation harness
- ATIF trajectory format support
- Streaming output, parallel execution
- Multi-agent support (Claude Code, Aider, OpenHands, etc.)

**OpenAgents GYM:**
- Location: `/home/christopherdavid/code/openagents/crates/gym/src/mechacoder/`
- Components:
  - `tb2_loader.rs` - Task discovery and loading
  - `docker_runner.rs` - Claude Code in Docker with streaming
  - `testgen_wrapper.rs` - TestGen Protocol v2 injection
  - `verifier.rs` - TB2 test execution and verification

**TestGen Protocol v2:**
- Location: `/home/christopherdavid/code/openagents/.claude/skills/testgen-protocol/`
- Files:
  - `SKILL.md` - Protocol specification
  - `expand_tests.py` - Deterministic test scaffold generator
- Proven success: 3/4 tasks (75%)

**HillClimber:**
- Location: `/home/christopherdavid/code/openagents/crates/hillclimber/`
- MAP-Elites based overnight optimization
- Not yet used for TB2, but ready

**Scripts:**
- `scripts/tb2-run.sh` - CLI convenience wrapper (already created)
- Need to create: `tb2-batch.sh`, `tb2-stats.sh`

### Infrastructure Gaps (To Create)

**Phase 2 Enhancement:**
- `.claude/skills/testgen-protocol/parse_inputs_phase.md`
- Update to `testgen_wrapper.rs` for Phase 0 injection

**Phase 3 Skills:**
- `.claude/skills/ml-inference/SKILL.md`
- `.claude/skills/bioinformatics/SKILL.md`
- `.claude/skills/build-from-source/SKILL.md`
- `.claude/skills/crypto-analysis/SKILL.md`
- `.claude/skills/scientific-python/SKILL.md`

**Automation:**
- `scripts/tb2-batch.sh` - Run multiple tasks in sequence
- `scripts/tb2-stats.sh` - Aggregate success rates, costs
- `scripts/tb2-retry.sh` - Retry failed tasks with different approaches

### Critical Files to Monitor

**Task Definitions:**
- `/home/christopherdavid/code/terminal-bench-2/{task-id}/task.toml`
- `/home/christopherdavid/code/terminal-bench-2/{task-id}/instruction.md`
- `/home/christopherdavid/code/terminal-bench-2/{task-id}/tests/test.sh`

**Results Storage:**
- `results/trajectories/{task-id}/{session-id}/trajectory.json` (ATIF)
- `results/trajectories/{task-id}/{session-id}/events.jsonl`
- `results/trajectories/{task-id}/{session-id}/metrics.json`

**Logs:**
- `docs/logs/20251211/tb2-phase{1-5}-results.md`
- `docs/logs/20251211/tb2-final-report.md`

---

## Part 7: Risk Mitigation

### Risk 1: TestGen v2 Success Rate Lower Than Expected

**Probability:** Medium
**Impact:** High (invalidates Category A strategy)

**Mitigation:**
- Phase 1 explicitly tests this (10 diverse tasks)
- If <70% pass rate, pivot to direct Claude Code (no TestGen wrapper)
- Keep TestGen for tasks that benefit, skip for others
- Document which task characteristics predict TestGen success

**Contingency:**
- Budget $20 extra for Phase 1 retries without TestGen
- Adjust Phase 2-5 expectations if Category A smaller than estimated

---

### Risk 2: Specialized Libraries Take Too Long to Build

**Probability:** Medium
**Impact:** Medium (delays Phase 3-4)

**Mitigation:**
- Start with simplest library first (scientific-python)
- Validate with 2 tasks before building full library
- If >40 hours per library, switch to Opus-per-task instead
- Reuse existing documentation (e.g., HuggingFace docs)

**Contingency:**
- Skip library building, use Opus directly for Category C tasks
- Budget adjustment: +$50 for Opus, -$50 for library dev time

---

### Risk 3: Docker/Container Issues

**Probability:** Low
**Impact:** High (blocks all testing)

**Mitigation:**
- Test Phase 1 tasks on variety of Docker images
- Keep DockerRunner fallback mode
- Document Docker troubleshooting patterns
- Test locally before containers when possible

**Contingency:**
- Use Harbor instead of GYM if GYM has issues
- Fallback to local Python/node execution for compatible tasks

---

### Risk 4: Cost Overrun

**Probability:** Low-Medium
**Impact:** Medium

**Mitigation:**
- Use Haiku by default (10x cheaper than Opus)
- Reserve Opus for confirmed-hard tasks only
- Set per-task cost alerts
- Stop if trending >$300 total

**Contingency:**
- Prioritize highest-value tasks (most learning)
- Skip extreme outliers (gpt2-codegolf, feal-linear-cryptanalysis)
- Accept 40-45% pass rate instead of 55%

---

### Risk 5: Extreme Tasks Unsolvable

**Probability:** High
**Impact:** Low (expected for research-level tasks)

**Mitigation:**
- Clearly separate "tractable hard" from "extreme hard"
- Don't expect to solve tasks with 2400 min expert time
- Focus on learning from attempts, not just passing
- Document what would be needed (e.g., specialized models, domain experts)

**Acceptance Criteria:**
- If 0/15 Category D tasks pass, that's acceptable
- Goal is proving architecture on tractable tasks, not solving research problems

---

## Part 8: Immediate Next Steps

### Week 1 Action Plan

**Day 1-2: Setup**
1. ‚úÖ Create this strategic assessment document
2. Create `scripts/tb2-batch.sh` for batch execution
3. Create `scripts/tb2-stats.sh` for aggregating results
4. Set up `results/trajectories/` directory structure
5. Create `docs/logs/20251211/tb2-phase1-results.md` template

**Day 3-5: Phase 1 Execution**
6. Run TestGen v2 on 10 Category A tasks (in order):
   - overfull-hbox
   - prove-plus-comm
   - fix-git
   - filter-js-from-html
   - largest-eigenval
   - headless-terminal
   - kv-store-grpc
   - nginx-request-logging
   - openssl-selfsigned-cert
   - git-multibranch

**Day 6-7: Phase 1 Analysis**
7. Analyze results, calculate pass rate
8. Identify failure patterns
9. Document lessons learned
10. Decide: Continue to Phase 2 or adjust strategy?

---

## Conclusion

This strategic assessment provides:

1. **Complete taxonomy** of all 90 TB2 tasks by approach needed
2. **Validated baseline** (75% success rate on 4 tested tasks)
3. **Phased roadmap** to systematically increase coverage
4. **Realistic targets** (50+ tasks passing in 12 weeks, ~$215 cost)
5. **Risk mitigation** for known failure modes

**Key Insight:** Architecture optimization (TestGen v2, skills libraries, enhanced protocols) can achieve 55%+ TB2 coverage with Haiku 4.5, proving "architecture beats model size" for tractable tasks.

**Next Milestone:** Phase 1 validation (Week 1) will confirm or refute the 70% Category A success rate assumption. All subsequent phases depend on this validation.

---

**Created:** 2024-12-11 19:00
**Status:** Plan complete, ready for execution
**Next Action:** Run Phase 1 validation (10 tasks)
