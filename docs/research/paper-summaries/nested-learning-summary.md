# Nested Learning: The Illusion of Deep Learning Architecture

**Authors**: Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, Vahab Mirrokni (Google Research)
**Published**: NeurIPS 2025

## Executive Summary

This paper introduces **Nested Learning (NL)**, a paradigm that reframes machine learning models as interconnected systems of nested, multi-level optimization problems. The key insight is that both neural architectures and optimizers are **associative memory modules** that compress their own "context flow" into parameters. This perspective unifies concepts like pre-training, in-context learning, and continual learning as manifestations of the same underlying mechanism operating at different levels and timescales.

---

## 1. Core Thesis: The Illusion of Architecture

The paper argues that what we perceive as heterogeneous deep learning architectures (Transformers, RNNs, MLPs) are actually **uniform structures** when viewed through nested learning:

> "Neural learning modules consist of a set of feedforward networks, each of which are optimized in different levels and time scales. The heterogeneity we observe in deep learning architectures is due to the lack of view to this new NL's axis, resulting in observing only the solution of optimization problems and so causing the illusion of deep learning architectures."

### Key Observations:
- All components are fundamentally feedforward layers (linear or deep MLPs)
- They differ only in their **update frequency**, **internal objective**, and **optimization process**
- Modern "hybrid" architectures are simply Transformers with additional in-context learning levels added to some MLP blocks

---

## 2. Neurophysiological Motivation

The design draws from two key brain properties:

### 2.1 Multi-Timescale Processing
Brain oscillations (brainwaves) coordinate neural computation at different frequencies:
- **Gamma waves (30-150 Hz)**: Sensory information processing
- **Beta waves (13-30 Hz)**: Active thinking
- **Delta/Theta waves (0.5-8 Hz)**: Memory consolidation and learning

Current deep learning uses uniform update rates, but NL advocates for **multi-timescale updates**.

### 2.2 Uniform and Reusable Structure
The brain's neuroplasticity enables neural elements to be flexibly redeployed (e.g., hemispherectomy patients can live normally). Memory is **distributed** across the brain, not isolated to specific regions.

### The Anterograde Amnesia Analogy
LLMs suffer from a condition analogous to anterograde amnesia—they cannot form new long-term memories after pre-training. Their knowledge is limited to:
- The immediate context (fits in context window)
- Long-past knowledge in MLPs (before "end of pre-training")

---

## 3. Formal Framework

### 3.1 Associative Memory Definition

**Definition 1**: Given keys K ⊆ ℝ^{d_k} and values V ⊆ ℝ^{d_v}, associative memory is an operator M(·) that maps K to V, learned by minimizing:

```
M* = arg min_M L̃(M(K); V)
```

**Learning vs. Memorization**: Memory is a neural update caused by an input; learning is the process of acquiring *effective and useful* memory.

### 3.2 Update Frequency

**Definition 2**: For any component A, its frequency f_A is its number of updates per unit time (one update step over one data point = unit time).

Components are ordered by (· ≻ ·): A ≻ B if f_A > f_B, or if f_A = f_B but B's computation requires A's state.

### 3.3 Nested System of Associative Memories (NSAM)

**Definition 4**: A NSAM is a system with K ordered levels, where each level k consists of optimization problems {(L^(k)_i, C^(k)_i, Θ^(k)_i)}, each optimized using gradient descent:

```
θ^(k)_{i,t+1} = arg min_{Φ} ⟨Φ k^(i)_{t+1}, -∇L^(k)_i(θ^(k)_{i,t}; k^(i)_{t+1}, v^(i)_{t+1})⟩ + (1/2η) ||Φ - θ^(k)_{i,t}||²
```

---

## 4. Optimizers as Learning Modules

### 4.1 Backpropagation as Associative Memory

Training a linear layer with backpropagation can be reformulated as:

```
W_{t+1} = arg min_W ⟨W x_t, ∇_{y_t} L(W_t; x_t)⟩ + (1/2η_{t+1}) ||W - W_t||²
```

**Key Insight**: Each layer learns to map data samples to their **Local Surprise Signal (LSS)**—the error in the corresponding prediction. This is a **self-referential process** where values are generated by the memory itself.

### 4.2 Momentum as Associative Memory

Gradient descent with momentum is a **2-level optimization procedure**:
- Inner level: Learns the momentum (compresses past gradients)
- Outer level: Uses momentum to update weights

The momentum term aims to minimize:
```
min_m ⟨m x̂_{ℓ-1}, δ_ℓ⟩
```

### 4.3 Adam as Optimal Associative Memory

Adam can be derived as the **optimal solution** to an L₂ regression objective that maps gradients to their variance:

```
L̃_t = Σ ||m_ℓ ⊙ g_{ℓ,i+1} - P_{ℓt}||² + λ_ℓ ||m_ℓ||²_F
```

The solution yields:
```
W_{ℓ,i+1} ≈ W_{ℓi} - (η_t/√β_2) * M̃^(t)_{ℓ,i} / (H^(t)_{ℓ,i}^{1/2} + ε)
```

Which is exactly Adam's update rule!

### 4.4 Long Context Problem in Optimizers

Current momentum design is a **low-pass filter** with limited memory:
- With β = 0.9, the last 6 gradients account for 50% of cumulative contribution
- Last 43 gradients account for 99%
- Gradients beyond 43 steps contribute <1%

This limits understanding of the global loss landscape and ability to find effective solutions in continual learning.

### 4.5 More Expressive Optimizers

The paper proposes several extensions:

| Extension | Approach |
|-----------|----------|
| **Delta Momentum** | Use L₂ regression instead of dot-product similarity |
| **Deep Momentum (DMGD)** | Replace linear momentum with MLP |
| **Higher-order Features** | Apply feature maps to gradients |
| **Delta Gradient Descent (DGD)** | State-dependent update incorporating previous weights |

**Delta Gradient Descent** update rule:
```
W_{t+1} = W_t(I - η'_t x_t x_t^⊤) - η'_t ∇_{y_t} L(W_t; x_t) ⊗ x_t
```

---

## 5. Architectures as Neural Learning Modules

### 5.1 Softmax Attention
Non-parametric solution to L₂ regression with Nadaraya-Watson estimators:
```
M* = arg min_M Σ s(k_i, q) ||v_i - M||²
```

### 5.2 Linear Attention (Hebbian Rule)
Optimizes dot-product similarity with gradient descent:
```
M_t = α_t M_{t-1} + η_t v_t φ(k_t^⊤)
```

### 5.3 Delta Rule (DeltaNet, RWKV-7)
Optimizes MSE objective:
```
M_t = (I - η_t k_t k_t^⊤) M_{t-1} + η_t v_t k_t^⊤
```

### 5.4 MLP vs. Linear Attention++

**Key Insight**: A linear attention with meta-learned initial memory state is essentially an MLP block with **in-context learning ability**:

| Component | Level | Frequency |
|-----------|-------|-----------|
| W_MLP | 1 (pre-training) | Once per dataset |
| W_LinAttn (initial) | 1 | Once per dataset |
| W_LinAttn (in-context) | 2 | Every token |

---

## 6. Knowledge Transfer Between Levels

### Types of Knowledge Transfer:

1. **Direct Connection (Parametric)**: M^(0)(·) := M^(0)(· ; M^(1)(·))
2. **Direct Connection (Non-Parametric)**: M^(0)(·) := M^(0)(· ; C^(1))
3. **Via Backpropagation**: Both states optimized in same gradient flow but different frequencies
4. **Via Initialization (MAML)**: Higher level learns best initial value for lower level
5. **Via Generation**: One level generates weights/context for another (e.g., hypernetworks, optimizers)

---

## 7. Revisiting Common Terms

### 7.1 Memory and Learning
> "Memory is not an isolated system and is distributed throughout the parameters. Any update caused by the input is stored memory."

### 7.2 Pre-training
> "Pre-training is in-context learning with ultra-large context length."

### 7.3 In-Context Learning
- Not emergent—it's a direct consequence of having multiple levels
- **Parametric ICL**: RNNs compressing context into parameters
- **Non-parametric ICL**: Attention conditioning on entire context

### 7.4 Continual Learning
> "For a neural learning module, there is no border between training and test time. The model only experiences two states: when it receives input, or when it is isolated."

### 7.5 Test-Time Training
> "Test-time training and test-time memorization are instances of parametric in-context learning."

---

## 8. Continuum Memory System (CMS)

### 8.1 Motivation
Traditional "long-term/short-term memory" view is limiting. CMS treats memory as a **spectrum** of blocks with different update frequencies.

### 8.2 Formulation

CMS is a chain of MLP blocks: MLP^(f_1)(·), ..., MLP^(f_k)(·), each with chunk size C^(ℓ) = max_i C^(i) / f_ℓ.

Update rule (parameters updated every C^(ℓ) steps):
```
θ^(f_ℓ)_{i+1} = θ^(f_ℓ)_i - {
    Σ η^(ℓ)_t f(θ^(f_ℓ)_t; x_t)  if i ≡ 0 (mod C^(ℓ))
    0                             otherwise
}
```

### 8.3 CMS Variants

| Variant | Knowledge Transfer |
|---------|-------------------|
| **Nested CMS** | Initial state of level s+1 meta-learned in level s |
| **Sequential CMS** | All initial states connected via backprop in lowest level |
| **Independent (Head-wise) CMS** | y_t = Agg(MLP^(f_k)(x_t), ..., MLP^(f_1)(x_t)) |

### 8.4 Why CMS Helps Continual Learning

When updating MLP^(f_s)(·):
1. Forgotten knowledge is still in lower-frequency MLP^(f_{s'})(·) where s' < s
2. Backpropagation through initial states can **circle back** forgotten knowledge
3. Creates a **loop through time** preventing catastrophic forgetting

### 8.5 Multi-scale Momentum Muon (M3)

CMS applied to optimizers with two momentum levels:
```
M^(1)_t = M^(1)_{t-1} + β_1 g_t
M^(2)_t = M^(2)_{t-1} + β_3 Σ g_i  (every Ĉ steps)

θ_t ← θ_{t-1} - η * (O^(1)_t + α O^(2)_t) / √(V_t + ε)
```

Where O^(i) = NewtonSchulz_T(M^(i)).

---

## 9. Hope Architecture

### 9.1 Self-Referential Titans

All components adapt in-context and generate their own values:

```python
y_t = M_memory,t-1(q_t)
k_t = M_k,t-1(x_t), v_t = M_v,t-1(x_t), η_t = M_η,t-1(x_t), α_t = M_α,t-1(x_t)

# Self-generated values
v̂_□,t = M_□,t-1(v_t)

# Delta Gradient Descent update
M_□,t = M_□,t-1(α_t I - η_t k_t k_t^⊤) - η_t ∇L_{M_□,t-1}(M_□,t-1; k_t, v̂_□,t)
```

### 9.2 Hope = Self-Referential Titans + CMS

```
o_t = M_memory,t-1(q_t)                    # Self-modifying Titans output
y_t = MLP^(f_k)(MLP^(f_{k-1})(...MLP^(f_1)(o_t)))  # CMS chain
```

### 9.3 Hope-Attention Variant
Replace self-modifying Titans with softmax attention, keep CMS for FFN blocks.

### 9.4 Parallelizable Training
- Chunk-wise training: Split sequence into ⌈L/C⌉ chunks
- Generate all elements at end of each chunk for next chunk
- All gradients computed in parallel before processing current chunk

---

## 10. Experimental Results

### 10.1 Continual Learning

**Class-Incremental Learning** (CLINC, Banking, DBpedia):
- Hope-enhanced Llama3-8B outperforms ICL, EWC, and InCA baselines
- Multiple memory levels critical for performance

**New Language Learning (CTNL task)**:
- ICL faces catastrophic forgetting when learning 2 languages sequentially
- Hope-3 (3 additional memory levels) recovers ICL performance

### 10.2 Long Context Understanding

**Needle-in-a-Haystack (RULER)**:

| Model | S-NIAH-1 16K | MK-NIAH 16K | MQ-NIAH 16K |
|-------|--------------|-------------|-------------|
| Transformer | 79.8 | 61.4 | 29.8 |
| Hope-Attention | 100 | 60.8 | 30.6 |
| RWKV-7 | 99.6 | 9.6 | 8.6 |
| Titans | 100 | 8.2 | 9.4 |
| **Hope** | **100** | **14.8** | **14.2** |

**BABILong**: Hope maintains performance up to 10M context length while other models degrade.

### 10.3 Language Modeling

| Model | Wiki ppl↓ | LMB acc↑ | Avg↑ |
|-------|-----------|----------|------|
| Transformer++ | 17.92 | 42.6 | 53.38 |
| Titans | 15.60 | 49.1 | 56.82 |
| **Hope** | **14.39** | **51.0** | **58.04** |

### 10.4 Language Recognition (State Tracking)
Hope achieves 100% on all formal language tasks (Parity, (aa)*, (abab)*, a^n b^n, etc.), unlike Transformers which fail at length generalization.

---

## 11. Key Takeaways

### 11.1 Unified View
> "Architectures generates the context for optimizers. The proper memory management of gradients relies on the choice of architectures."

### 11.2 Parameters
> "The parameters of a neural learning module are not limited to those optimized in pre-training; all parameters in the NL representation contribute to performance."

### 11.3 Design Philosophy
Two high-level choices in neural learning modules:
1. Design of optimization problems and their frequency
2. Design of knowledge transfer between levels

### 11.4 Catastrophic Forgetting
> "Catastrophic forgetting is a natural consequence of compression—limited capacity forces the model to forget to retain capacity for new information."

NL is a **roadmap** suggesting progress will come from exploiting the **levels axis** rather than ever-deeper static networks.

---

## 12. Connections to Prior Work

| Concept | NL Interpretation |
|---------|-------------------|
| Meta-learning | Two levels with one meta-learning the other |
| MAML | Knowledge transfer via initialization |
| Hypernetworks | One block generates weights for another |
| Learned optimizers | Different frequency/context flow from vanilla optimizers |
| Fast Weight Programmers | Projections = slow weights, memory update = fast weights |
| Test-time training | Parametric in-context learning |

---

## 13. Practical Implications

### For Architecture Design:
- Add levels to MLP blocks for in-context learning
- Use CMS instead of single MLP for persistent memory
- Meta-learn initial states for fast adaptation

### For Optimizer Design:
- Consider architecture-specific optimizers
- Use multi-scale momentum for long training runs
- Apply preconditioning as learned mappings

### For Continual Learning:
- Multi-timescale updates prevent forgetting
- Knowledge can "loop back" through initialization
- No fundamental train/test boundary needed

---

## References

Key papers building on or related to this work:
- Titans (Behrouz et al. 2025c) - Deep memory modules
- Miras (Behrouz et al. 2025b) - Associative memory framework
- TTT (Sun et al. 2024) - Test-time training
- MAML (Finn et al. 2017) - Meta-learning
- Linear Attention (Katharopoulos et al. 2020)
- DeltaNet (Schlag et al. 2021)
