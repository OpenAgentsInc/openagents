# 2512.24601v1 (Recursive Language Models) Notes

Source PDF: `docs/rlm/2512.24601v1.pdf`

Title: **Recursive Language Models**
Authors: **Alex L. Zhang; Tim Kraska; Omar Khattab**
arXiv: **2512.24601v1** (2025-12-31)

## Summary

The paper proposes **Recursive Language Models (RLMs)**: an inference-time strategy for handling prompts far beyond the model context window by treating the long prompt as an **external environment**. The model:
- programmatically examines and decomposes the prompt,
- recursively calls itself over selected snippets,
- and composes intermediate results into a final answer.

Key claims (from abstract/intro):
- Handles inputs **up to two orders of magnitude** beyond base context windows.
- Improves quality even on shorter prompts vs baseline long-context scaffolds.
- Cost per query is comparable or cheaper than common approaches.

## Why This Matters To OpenAgents

OpenAgents work often involves:
- out-of-core reasoning over repos and long sessions (RLM/FRLM modes),
- deterministic artifact production (receipts/replay),
- and explicit execution plans that can run for hours.

RLM-style decomposition and recursive self-calls maps directly onto:
- repository traversal strategies (chunk selection, progressive disclosure),
- long-horizon agent execution (plan -> subplan -> verification loop),
- and making "what the model can see" explicit and replayable (log snippet selection and hashes).

## Open Questions / Follow-Ups

- What should be the canonical "environment interface" for repo-scale contexts (files, diffs, traces)?
- How do we make recursive selection decisions auditable (hashes, counterfactuals, budgets)?
- Where should these policies live: DSE signatures/modules vs runtime heuristics?

