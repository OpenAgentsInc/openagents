# OpenAgents Video Series - Episode 164

## Video Information

- **Title**: OpenAgents ⚡ - Episode 164: The New  We demo our new agentic chat product at availab...
- **Creator**: OpenAgents ⚡
- **Duration**: 22:50
- **Published**: March 13, 2025
- **URL**: https://x.com/OpenAgentsInc/status/1900279256606794235
- **Transcribed**: June 08, 2025
- **Language**: en

## Full Transcript

And we're back. Let's take a tour of the new OpenAgents.com. So we've got an Agentechat UI here. You can try it by clicking a prompt. In this case we're analyzing an issue from the Bitcoin Core repo and it fetches all the open issues. One of them had a bunch of good information in the body so I was able to kind of give a proposed solution. I don't know how I hear this is but you can get some good stuff here. You can chain multiple tools together. The tools that you have access to even in free mode are the basic read-only ones interacting with GitHub. You don't have to interact with GitHub if you don't want to but this is kind of intended to have really good Coder Agents. The Coder agent itself for like right access to make code changes is in our pro plan which also gets you actually to son it 3.7.35. Gemini Flash, GBT Mini, we'll be adding a bunch more models there and all of the right access tools. So let me flip over to my pro account where I've got sonnet, I've got Coder, I've got all these tools. And let's do a little demo of what I do, how I code. Let's see this could fail miserably but so here's one thing that I do not like. We want to improve this chat history thing. So right now every time I open this there's a load spinner because every time I open this it's doing this kind of query from our database convex and it's not being persistent. And what I want to do is use a indexed DB library to connect my repo there to save that locally. This could be very complex but you know my hope is that in the next 10 minutes or so we can get it working. So let's just do how I would actually do. There's a library I know and he used called Dexie. Okay so here's what I'm gonna do. On the V4 repo we want to change the history recent threads component to not always fetch from convex. I want to store locally in indexed DB, V4 Dexie. Look around to find relevant files and scrape this and then open a GitHub issue. So this is one thing that I like a lot that I haven't seen any other cursor or anything do is be able to open issues for you. Okay this is cool. It's also using the Grap tool which is new and while most of these tools use the GitHub API the Grap tool calls a service that we're running that will clone the repo down so we can run bash commands on it. And then we'll be extending that to do other things like you know being able to run scripts and other kind of bash things so that you don't always have to like have your own environment. Let's see what it's doing here. So checking this component. Now it's scraping to get information about Dexie. Now it's creating a GitHub issue outlining the implementation plan and I'll pop over to V4 and check out issue 80. Ampliment local storage for chat history using doxie.js. Currently the reason threads component fetches chat history directly from convex blah blah blah. Install Dexie and Dexie React hooks. Create the database schema. Do you think this will work? So what I'll do is I'll run this command. See if I can avoid even going into cursor at all. Let's do this. All right so let's it. I'll have it create a branch. Create a branch. I'll do the installation of packages myself. I want you to make all other needed code changes. And whatever. Then open a pull request when ready. Do you think that'll work? Feature Dexie local storage. So I think the GitHub integration will even though I like using it for my own personal workflows, I think this will be especially helpful if you're on a team of people. Not just solo coding or figuring out the GitHub integration yourself but having things documented on GitHub. So one of the things we're going to be pulling over from our previous like internal business facing product is teams and projects. So if you have a team of engineers that you'd like to get using shared conversations through this, I'd love to onboard you to do that. So I created a branch. It's creating the database schema file. That's a modify, recent threads and create a pull request. Let's see implement local storage for chat history using Dexie. Okay, now I'm going to do some stuff locally which in the future we want to remove the need to do this. Oops, get pull. Check it out. First thing I'll do is I'll install this stuff. We're using PNPM. What the hell? Also new to PNPM. I don't know what that error is about. Okay, PNPM. This stuff. Okay, so I will come in that. And then I'll run type check. Another thing that will be done in a loop. So CN that just needs to be imported. And I can fix that easily myself but fix this. You do it. You do it. Okay, there's a couple of things here where the cursor workflow is better than ours because it already has that local demon running. And we'll pull over patterns like that. But the point is that even what we have now is already competitive to the point where there's workflows, particularly if I'm trying to interact with the GitHub APIs where the open agent's flow is better. Sometimes I'll use it kind of as a supplement to cursor or cloud coder. But I've been able to use more and more of this. Let me fix that in the recent thread. Is this getting tired? Why is it not working yet? Oh, so one other thing that we need to pull over is surgical edits. Correct. So right now it's doing full file edits which is not so good. Oh, looks like that might have worked. Okay, get pulled. Okay, we have no type errors. Now shall I test what's going to happen? What's going to happen? There's no way it would just work one shot. There's no way, right? PMP and run. There's no way this would work one shot. It's like an advanced library with a thing and a stuff and I don't know. Let me look up console log. Let's do what happens here. All right, I'm going to open chat history. I saw the loading. I'm still seeing the loading. Is it faster? I run it and it works with no errors. But I still see the load spinner thing which I don't want to see. It shouldn't re-render each time. And I can't tell if it's loading from Dexie or if there's any speed up. Add logging so I can tell what's going on. Kind of seems faster, but I don't know. Test offline functionality. That's something I could probably do. How would I test that? I can't take my computer offline because I'm recording. Why do you go to throttle stuff here? Oh, here we go. Offline. I guess that's the real test. If I refresh, well, first of all, that may not work. So now I'm emulating a slow network. Well, I guess if it loads, wow, 3G out. That's it. Now out of curiosity, what happens if I now go and turn it offline? I'm now offline. What happens if I click this? Well, then, okay, that confirms it worked. This ordinarily would always pull from the nice, nice, nice. Okay. So let's just pull the latest and we'll get the logging. I don't think I'll be able to refresh. So I'm going to have to refresh. I guess there's other parts of the app that don't work offline. So let's get full offline support. There's more I'll need to do. Offline. Let's take a look at the history. Fetching from index DB got 275 threads from index DB. I think it just takes a fraction of a second to get us. So I still see that absolute flash of loading indicator. Let me just say this. Let me flip back to no throttling. Sinking. Damn, that's cool. Okay, let's try this. Looks like it is loading from convex and from dexie and seems faster. But I do still see a flash of load spinner. Here's log. I don't want to see it. I'm going to try to edit the other thing again. Well, once we've launched this and make sure that the basics are working, then I'll go through and add the little optimizations to make things like this go faster using surgical perrry sites. Yeah, it's there it's gone. Let's see if that did anything. We're still seeing the fucking spinner. So while we're waiting for that one, this is vibe coding everybody. I'm not doing a greenfield app. I'm doing a targeted feature. But I think in the next video, we'll do more of a full demonstration of vibe coding from scratch. But this is kind of like the tweet that apparently coined the term vibe coding. You fully give into the vibes, embrace exponentials. Forget that the code even exists because the LLMs are getting too good. That's not done yet. I will just see if it made any difference. It's good. But is he good enough? I'm like trying to see if there's like a teeny tiny flash. Okay, so I'm just gonna call that good enough. Okay. Okay, thanks. It works fine. Remove the logging. All right, so I'm gonna push that and merge this PR. What, that took 10, 15 minutes to do a library that I've never used before, what otherwise spend an hour reading the docs and implementing everything manually. Well, thanks for checking out OpenAgence.com. Please give it a try. Is it still, we're calling it kind of open beta because there's still probably some bugs and stuff to to work out. But we have a deal now. If you sign up for Pro, you'll get 30% off of the price, which is gonna be 19 a month, undercut cursor by $1. And for now, there's a 30% discount for early adopters and that's a lifetime thing. So, you know, locking the price at $13 instead of 19 or 105 instead of 154 annual. This will get better rapidly and just to give an idea of where we're going with this. You see the models down here? We're gonna always have like the top models. So you'll get all just coming here and chat with models if you want to. The agents. So the basic one is assistant. Coder is more like the thing that you've seen me use with a bunch of the right access tools. Researcher, we're gonna do our version of deep research. And our approach, you know, it's not to do kind of wild garden thing using your own models. It's literally to just take whatever is the best that the open source community comes up with and throw it into an ISUI. And pretty soon here, we're gonna dust off our old agent store idea where people get paid Bitcoin revenue sharing to contribute to workflows. So we want to have anyone who contributes plugins or prompts or something that's used by code or used by researcher. Whenever anyone is using that in some sort of paid usage, I'll be wanting you to get some some revenue share. And that'll be paid out probably to your Onyx Bitcoin wallet, our open source wallet, which I think we'll do some hacking on together pretty soon. So yeah, check it out. Go sign up if you're cool. Otherwise, even feel free to kick the tires on the free plan. And please give us some feedback. See you soon.

## Timestamped Transcript

[0:00 - 0:06] And we're back. Let's take a tour of the new OpenAgents.com. So we've got an

[0:06 - 0:13] Agentechat UI here. You can try it by clicking a prompt. In this case we're

[0:13 - 0:19] analyzing an issue from the Bitcoin Core repo and it fetches all the open

[0:19 - 0:23] issues. One of them had a bunch of good information in the body so I was able to

[0:23 - 0:27] kind of give a proposed solution. I don't know how I hear this is but you can

[0:27 - 0:32] get some good stuff here. You can chain multiple tools together. The tools that

[0:32 - 0:35] you have access to even in free mode are the basic read-only ones

[0:35 - 0:40] interacting with GitHub. You don't have to interact with GitHub if you don't want

[0:40 - 0:46] to but this is kind of intended to have really good Coder Agents. The Coder

[0:46 - 0:53] agent itself for like right access to make code changes is in our pro plan which

[0:53 - 0:59] also gets you actually to son it 3.7.35. Gemini Flash, GBT Mini, we'll be adding a

[0:59 - 1:04] bunch more models there and all of the right access tools. So let me flip over to

[1:04 - 1:13] my pro account where I've got sonnet, I've got Coder, I've got all these tools.

[1:13 - 1:23] And let's do a little demo of what I do, how I code. Let's see this could fail

[1:23 - 1:27] miserably but so here's one thing that I do not like. We want to improve this

[1:27 - 1:32] chat history thing. So right now every time I open this there's a load spinner

[1:32 - 1:38] because every time I open this it's doing this kind of query from our database

[1:38 - 1:47] convex and it's not being persistent. And what I want to do is use a indexed DB

[1:49 - 1:59] library to connect my repo there to save that locally. This could be very complex

[1:59 - 2:03] but you know my hope is that in the next 10 minutes or so we can get it working.

[2:03 - 2:10] So let's just do how I would actually do. There's a library I know and he

[2:10 - 2:26] used called Dexie. Okay so here's what I'm gonna do. On the V4 repo we want to

[2:28 - 2:37] change the history recent threads component to not always fetch from convex.

[2:38 - 2:51] I want to store locally in indexed DB, V4 Dexie. Look around to find relevant

[2:51 - 3:01] files and scrape this and then open a GitHub issue. So this is one thing that I

[3:01 - 3:08] like a lot that I haven't seen any other cursor or anything do is be able to open

[3:08 - 3:13] issues for you. Okay this is cool. It's also using the Grap tool which is new and

[3:14 - 3:20] while most of these tools use the GitHub API the Grap tool calls a service that

[3:20 - 3:25] we're running that will clone the repo down so we can run bash commands on it. And then

[3:25 - 3:30] we'll be extending that to do other things like you know being able to run scripts and

[3:30 - 3:35] other kind of bash things so that you don't always have to like have your own environment.

[3:36 - 3:41] Let's see what it's doing here. So checking this component. Now it's scraping to get information

[3:41 - 3:46] about Dexie. Now it's creating a GitHub issue outlining the implementation plan and I'll

[3:46 - 3:56] pop over to V4 and check out issue 80. Ampliment local storage for chat history using

[3:56 - 4:01] doxie.js. Currently the reason threads component fetches chat history directly from convex

[4:01 - 4:13] blah blah blah. Install Dexie and Dexie React hooks. Create the database schema. Do you think

[4:13 - 4:21] this will work? So what I'll do is I'll run this command. See if I can avoid even going into

[4:21 - 4:35] cursor at all. Let's do this. All right so let's it. I'll have it create a branch. Create a branch.

[4:37 - 4:48] I'll do the installation of packages myself. I want you to make all other needed code changes.

[4:51 - 5:00] And whatever. Then open a pull request when ready.

[5:05 - 5:07] Do you think that'll work?

[5:13 - 5:15] Feature Dexie local storage.

[5:18 - 5:24] So I think the GitHub integration will even though I like using it for my own personal workflows,

[5:24 - 5:28] I think this will be especially helpful if you're on a team of people.

[5:29 - 5:33] Not just solo coding or figuring out the GitHub integration yourself but

[5:36 - 5:41] having things documented on GitHub. So one of the things we're going to be pulling over from our

[5:41 - 5:51] previous like internal business facing product is teams and projects. So if you have a team of

[5:51 - 5:58] engineers that you'd like to get using shared conversations through this, I'd love to onboard you to

[5:58 - 6:04] do that. So I created a branch. It's creating the database schema file. That's a modify,

[6:04 - 6:07] recent threads and create a pull request.

[6:17 - 6:21] Let's see implement local storage for chat history using Dexie.

[6:23 - 6:29] Okay, now I'm going to do some stuff locally which in the future we want to remove the need to do this.

[6:31 - 6:38] Oops, get pull. Check it out. First thing I'll do is I'll install this stuff.

[6:40 - 6:41] We're using PNPM.

[6:47 - 6:48] What the hell?

[7:00 - 7:02] Also new to PNPM. I don't know what that error is about.

[7:08 - 7:11] Okay, PNPM. This stuff.

[7:15 - 7:17] Okay, so I will

[7:19 - 7:20] come in that.

[7:22 - 7:24] And then I'll run type check.

[7:27 - 7:35] Another thing that will be done in a loop. So CN that just needs to be imported.

[7:36 - 7:39] And I can fix that easily myself but fix this.

[7:42 - 7:44] You do it. You do it.

[7:48 - 7:53] Okay, there's a couple of things here where the cursor workflow is better than ours because it

[7:53 - 8:02] already has that local demon running. And we'll pull over patterns like that. But the point is

[8:02 - 8:06] that even what we have now is already competitive to the point where there's workflows,

[8:06 - 8:10] particularly if I'm trying to interact with the GitHub APIs where the open agent's flow is

[8:10 - 8:18] better. Sometimes I'll use it kind of as a supplement to cursor or cloud coder.

[8:20 - 8:26] But I've been able to use more and more of this. Let me fix that in the recent

[8:26 - 8:28] thread. Is this getting tired? Why is it not working yet?

[8:31 - 8:35] Oh, so one other thing that we need to pull over is surgical edits.

[8:35 - 8:39] Correct. So right now it's doing full file edits which is not so good.

[8:46 - 8:47] Oh, looks like that might have worked.

[8:51 - 8:53] Okay, get pulled.

[9:01 - 9:10] Okay, we have no type errors. Now shall I test what's going to happen? What's going to happen?

[9:11 - 9:13] There's no way it would just work one shot. There's no way, right?

[9:15 - 9:20] PMP and run. There's no way this would work one shot.

[9:20 - 9:24] It's like an advanced library with a thing and a stuff and I don't know.

[9:30 - 9:35] Let me look up console log. Let's do what happens here. All right, I'm going to open chat history.

[9:39 - 9:43] I saw the loading. I'm still seeing the loading.

[9:47 - 9:50] Is it faster?

[10:03 - 10:12] I run it and it works with no errors. But I still see the load spinner thing

[10:13 - 10:18] which I don't want to see. It shouldn't re-render each time.

[10:19 - 10:25] And I can't tell if it's loading from Dexie or if there's any speed up.

[10:26 - 10:32] Add logging so I can tell what's going on.

[10:33 - 10:35] Kind of seems faster, but I don't know.

[10:55 - 10:58] Test offline functionality. That's something I could probably do.

[11:03 - 11:05] How would I test that?

[11:07 - 11:12] I can't take my computer offline because I'm recording.

[11:19 - 11:24] Why do you go to throttle stuff here?

[11:26 - 11:27] Oh, here we go. Offline.

[11:28 - 11:32] I guess that's the real test. If I refresh, well, first of all,

[11:36 - 11:37] that may not work.

[11:49 - 11:52] So now I'm emulating a slow network.

[11:59 - 12:06] Well, I guess if it loads, wow, 3G out.

[12:24 - 12:25] That's it.

[12:25 - 12:29] Now out of curiosity, what happens if I now go and turn it offline?

[12:31 - 12:33] I'm now offline. What happens if I click this?

[12:34 - 12:37] Well, then, okay, that confirms it worked.

[12:43 - 12:45] This ordinarily would always pull from the

[12:48 - 12:50] nice, nice, nice. Okay.

[12:52 - 12:56] So let's just pull the latest and we'll get the logging.

[12:57 - 12:59] I don't think I'll be able to refresh.

[13:04 - 13:06] So I'm going to have to refresh.

[13:07 - 13:12] I guess there's other parts of the app that don't work offline.

[13:12 - 13:15] So let's get full offline support. There's more I'll need to do.

[13:25 - 13:26] Offline.

[13:27 - 13:28] Let's take a look at the history.

[13:31 - 13:35] Fetching from index DB got 275 threads from index DB.

[13:42 - 13:47] I think it just takes a fraction of a second to get us.

[13:48 - 13:53] So I still see that absolute flash of loading indicator.

[13:57 - 13:58] Let me just say this.

[14:06 - 14:09] Let me flip back to no throttling.

[14:15 - 14:16] Sinking.

[14:16 - 14:18] Damn, that's cool.

[14:22 - 14:23] Okay, let's try this.

[14:26 - 14:35] Looks like it is loading from convex and from dexie and seems faster.

[14:35 - 14:42] But I do still see a flash of load spinner.

[14:43 - 14:44] Here's log.

[14:44 - 14:46] I don't want to see it.

[15:40 - 15:45] I'm going to try to edit the other thing again.

[15:51 - 15:54] Well, once we've launched this and make sure that the basics are working,

[15:54 - 15:56] then I'll go through and add the little

[15:57 - 16:04] optimizations to make things like this go faster using surgical perrry sites.

[16:06 - 16:09] Yeah, it's there it's gone.

[16:29 - 16:31] Let's see if that did anything.

[16:40 - 16:42] We're still seeing the fucking spinner.

[18:26 - 18:33] So while we're waiting for that one, this is vibe coding everybody.

[18:34 - 18:37] I'm not doing a greenfield app.

[18:37 - 18:39] I'm doing a targeted feature.

[18:40 - 18:48] But I think in the next video, we'll do more of a full demonstration of vibe coding from scratch.

[18:48 - 18:52] But this is kind of like the tweet that apparently coined the term vibe coding.

[18:53 - 18:56] You fully give into the vibes, embrace exponentials.

[18:56 - 18:59] Forget that the code even exists because the LLMs are getting too good.

[19:15 - 19:15] That's not done yet.

[19:16 - 19:17] I will just see if it made any difference.

[19:32 - 19:33] It's good.

[19:39 - 19:40] But is he good enough?

[19:48 - 19:54] I'm like trying to see if there's like a teeny tiny flash.

[19:55 - 19:57] Okay, so I'm just gonna call that good enough.

[20:00 - 20:01] Okay.

[20:09 - 20:10] Okay, thanks.

[20:10 - 20:12] It works fine.

[20:13 - 20:14] Remove the logging.

[20:17 - 20:22] All right, so I'm gonna push that and merge this PR.

[20:22 - 20:28] What, that took 10, 15 minutes to do a library that I've never used before,

[20:28 - 20:32] what otherwise spend an hour reading the docs and implementing everything manually.

[20:37 - 20:41] Well, thanks for checking out OpenAgence.com.

[20:42 - 20:43] Please give it a try.

[20:43 - 20:48] Is it still, we're calling it kind of open beta because there's still probably some bugs and stuff to

[20:50 - 20:51] to work out.

[20:52 - 20:55] But we have a deal now.

[20:55 - 21:03] If you sign up for Pro, you'll get 30% off of the price, which is gonna be 19 a month,

[21:03 - 21:04] undercut cursor by $1.

[21:07 - 21:13] And for now, there's a 30% discount for early adopters and that's a lifetime thing.

[21:13 - 21:26] So, you know, locking the price at $13 instead of 19 or 105 instead of 154 annual.

[21:27 - 21:31] This will get better rapidly and just to give an idea of where we're going with this.

[21:32 - 21:36] You see the models down here? We're gonna always have like the top models.

[21:36 - 21:38] So you'll get all just coming here and chat with models if you want to.

[21:39 - 21:43] The agents. So the basic one is assistant.

[21:44 - 21:49] Coder is more like the thing that you've seen me use with a bunch of the right access tools.

[21:50 - 21:52] Researcher, we're gonna do our version of deep research.

[21:53 - 21:57] And our approach, you know, it's not to do kind of wild garden thing using your own models.

[21:57 - 22:01] It's literally to just take whatever is the best that the open source community comes up with

[22:02 - 22:03] and throw it into an ISUI.

[22:03 - 22:11] And pretty soon here, we're gonna dust off our old agent store idea where people get paid

[22:11 - 22:18] Bitcoin revenue sharing to contribute to workflows. So we want to have anyone who contributes

[22:18 - 22:22] plugins or prompts or something that's used by code or used by researcher.

[22:22 - 22:26] Whenever anyone is using that in some sort of paid usage,

[22:26 - 22:28] I'll be wanting you to get some some revenue share.

[22:29 - 22:33] And that'll be paid out probably to your Onyx Bitcoin wallet,

[22:33 - 22:37] our open source wallet, which I think we'll do some hacking on together pretty soon.

[22:38 - 22:41] So yeah, check it out. Go sign up if you're cool.

[22:42 - 22:45] Otherwise, even feel free to kick the tires on the free plan.

[22:46 - 22:47] And please give us some feedback.

[22:49 - 22:49] See you soon.


## Notes

This transcript was automatically generated using OpenAI Whisper (base model) from the video at https://x.com/OpenAgentsInc/status/1900279256606794235.

Transcription accuracy may vary. Please refer to the original video for definitive content.
