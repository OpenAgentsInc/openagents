# Solver Agent

This document explains the Solver agent, a specialized autonomous agent designed to analyze, plan, and implement solutions for OpenAgents Projects issues.

## Overview

Solver runs in the OpenAgents Projects dashboard and is implemented as a Cloudflare Durable Object using their [Agents SDK](https://developers.cloudflare.com/agents/api-reference/). The agent interacts with users through a chat interface and works within the OpenAgents Projects data model:

- Teams have Projects
- Projects have Issues
- Issues have Implementation Steps and Comments

Each Solver is a unique instance with an ID format of `solver/{uuid-of-issue}`, with one Solver per issue.

## Architecture

The Solver architecture consists of several key components working together:

### Issue Page Interface

The main user interface is located at `/issues/{uuid-of-issue}` and provides:

- A chat interface for communicating with the agent
- Complete action history with tool usage
- Right sidebar with connection status, issue details, and agent controls

### State Management

The Solver extends BaseAgentState with specialized properties to track and maintain issue context:

```typescript
export interface SolverState extends BaseAgentState {
  messages: UIMessage[];
  currentIssue?: BaseIssue;
  currentProject?: BaseProject;
  currentTeam?: BaseTeam;
  implementationSteps?: ImplementationStep[];
  issueComments?: IssueComment[];
}
```

The state management system handles:

- Message history persistence
- Issue/project/team context tracking
- Implementation step progress (storing the plan generated by `createImplementationPlan`)
- Deep cloning of state objects using `JSON.parse(JSON.stringify())` to prevent reference issues *(Note: This method has limitations, e.g., it doesn't preserve Dates, functions, Sets, Maps, etc.)*
- State recovery from message history when context is lost (e.g., after DO eviction or deployment)
- Race condition handling: Primarily relies on the Cloudflare Durable Object platform ensuring single-threaded execution per agent instance, mitigating typical concurrency issues within the agent's state updates

### Message Handling

The WebSocket message handler processes various operation types:

- `get_system_prompt`: Retrieves the current system prompt with context
- `set_context`: Updates the issue/project/team context
- `observation`: Records agent observations during operation
- `status_update`: Handles status change notifications
- `command`: Processes specific agent commands
- `shared_infer`: Handles inference requests using Llama 4

The handler includes robust security features like redacting sensitive information in logs and validation of message data before processing.

## Core Files

The Solver implementation is distributed across several key files:

### Main Agent Implementation

`packages/agents/src/agents/solver/index.ts` defines the main Solver class that extends OpenAgent and implements:

- WebSocket message handling with type safety
- Context management and recovery mechanisms
- Inference request processing
- State serialization and deep cloning
- Comprehensive error handling and logging

### Base Agent Functionality

`packages/agents/src/common/open-agent.ts` provides the foundation for all agents in the system:

- Common functionality shared across different agent types
- Repository context and GitHub integration
- Shared inference using Cloudflare Workers AI
- Base WebSocket handling and state management
- Secure token management

### Dynamic Prompt Generation

`packages/agents/src/agents/solver/prompts.ts` handles system prompt generation:

- Creates dynamic prompts based on current state and context
- Configures behavior through temperature settings
- Incorporates comprehensive context including issue details, project information, team context, and implementation status
- Adapts behavior based on temperature:
  - Low temperature (<0.3): Focuses on precision and correctness
  - High temperature (>0.7): Provides more creative solutions

### Type Definitions

`packages/agents/src/agents/solver/types.ts` provides TypeScript interfaces:

- SolverState extending BaseAgentState with solver-specific properties
- SolverIssue extending BaseIssue for specialized functionality
- Type safety throughout the implementation

### Tool Implementation

`packages/agents/src/agents/solver/tools.ts` defines specialized tools:

- `getIssueDetails`: Fetches comprehensive issue information
- `updateIssueStatus`: Updates issue status with optional comments
- `createImplementationPlan`: Generates step-by-step solution plans

### Web Interface Components

The user interacts with the Solver agent via components within the main issue page (`/issues/{uuid-of-issue}`). The core interaction model involves:

1. **Parent Issue Page (`apps/website/app/routes/issues/$id.tsx`):** Initializes the agent connection using the `useOpenAgent` hook for the specific issue (`solver/{uuid-of-issue}`). It fetches necessary data (issue, project, team) and manages the overall layout.
2. **Shared Agent Instance:** The `agent` instance created by the parent page hook is passed down as a prop to both the chat interface and the controls sidebar, ensuring they interact with the same agent state and WebSocket connection.
3. **Chat Interface (`apps/website/app/components/agent/solver-connector.tsx`):** Receives the shared `agent` instance. Displays the message history (`agent.messages`) and provides the input field for users. When a message is sent, it updates the local message list, ensures the agent has context (sending a `set_context` message if needed), and then sends the user's input to the agent via a `shared_infer` message.
4. **Sidebar Controls (`apps/website/app/components/agent/solver-controls.tsx`):** Also receives the shared `agent` instance. Provides buttons to connect/disconnect. The connection logic (`connectToSolver`) orchestrates setting the GitHub token, repository context, sending the initial `set_context` message, submitting an initial prompt (`handleSubmit`), and triggering inference (`infer`) via methods on the shared `agent` instance. It also includes debugging/testing buttons that interact with the agent (e.g., viewing the system prompt, sending test messages).

#### 1. Main Issue Page (`apps/website/app/routes/issues/$id.tsx`)

This component serves as the container for the entire issue view and is responsible for:

- Loading issue data from the database using the `loader` function
- Initializing the shared agent instance via the `useOpenAgent` hook
- Managing the overall layout with a two-column design
- Setting up the issue context and GitHub token handling
- Handling issue status updates and other metadata changes
- Synchronizing the Solver agent's state when the page loads

The main page automatically retrieves the GitHub token from local storage and passes it to child components, ensuring secure token handling without exposing credentials in the UI state.

```typescript
// Example of agent initialization in the issue page
const agent = useOpenAgent(issue.id, "solver");

// Passing the agent instance to child components
<SolverConnector
  issue={issue}
  agent={agent}
  githubToken={getGithubToken()}
  className="w-full h-full"
/>
```

#### 2. Chat Interface (`apps/website/app/components/agent/solver-connector.tsx`)

The chat interface handles direct user interaction with the agent and:

- Displays message history with proper formatting
- Provides a message input for user queries
- Handles message submission and response rendering
- Manages auto-scrolling behavior for new messages
- Shows appropriate UI states (connecting, disconnected, error)
- Implements context recovery if the agent loses state

The component includes special handling for agent communication:

```typescript
// Example of message handling in the chat interface
const handleSendMessage = async (message) => {
  // Add message to UI immediately
  agent.setMessages([...agent.messages, userMessage]);

  // Ensure context is available before sending
  if (contextMissing) {
    const contextMessage = {
      type: "set_context",
      issue: formattedIssue,
      project: formattedProject,
      team: formattedTeam
    };
    agent.sendRawMessage(contextMessage);
    await new Promise(resolve => setTimeout(resolve, 300));
  }

  // Send the inference request
  await agent.sendRawMessage({
    type: "shared_infer",
    params: { /* message parameters */ },
    context: { issue, project, team }
  });
};
```

#### 3. Sidebar Controls (`apps/website/app/components/agent/solver-controls.tsx`)

The sidebar controls provide user management of the agent and:

- Display connection status with visual indicators
- Offer connection/disconnection buttons
- Provide debugging tools for agent operations
- Allow viewing the system prompt
- Include testing buttons for agent features
- Handle error states and retries
- Show contextual information based on agent state

The component implements several agent control functions:

```typescript
// Example of agent connection in the controls component
const connectToSolver = async () => {
  setConnectionState('connecting');

  // First set the GitHub token
  await agent.setGithubToken(githubToken);

  // Set repository context
  await agent.setRepositoryContext(owner, repo, branch);

  // Send context message with issue data
  agent.sendRawMessage({
    type: "set_context",
    issue: formattedIssue,
    project: formattedProject,
    team: formattedTeam
  });

  // Send initial prompt and start inference
  await agent.handleSubmit(initialPrompt);
  await agent.infer(githubToken);

  setConnectionState('connected');
};
```

These components work together to create a cohesive user experience, with the parent issue page coordinating the agent instance that is shared between the chat interface and control sidebar.

## Agent Capabilities

The Solver agent provides capabilities focused on issue understanding, planning, and management within OpenAgents Projects:

1. **Analysis and Planning**
   - Analyzes issue descriptions and requirements provided in the context
   - Can generate a *predefined* step-by-step plan using the `createImplementationPlan` tool, which outlines typical stages like analysis, research, implementation, testing, and documentation *(Note: Currently, this tool generates a static plan template stored in the agent's state, not a dynamically reasoned plan based on the issue content)*
   - Can research issue details using the `getIssueDetails` tool (currently supports fetching from GitHub API or retrieving the current issue from agent state)
   - Researches existing codebase context *(Note: Relies on LLM's general knowledge or context provided in prompts; no specific code browsing tools are implemented yet)*

2. **Implementation Assistance** *(Clarified Title)*
   - Facilitates implementation by providing analysis and planning steps
   - *Future Capability:* Aspirational goals include tools to directly suggest or implement code modifications and test changes, but these are not yet implemented

3. **Progress Management**
   - Updates issue status using the `updateIssueStatus` tool (currently supports updating GitHub issues or the issue within agent state)
   - Adds comments during status updates (supported by `updateIssueStatus` for GitHub)
   - Maintains context across user interactions within a session
   - Recovers essential context (issue, project, team) from message history if the agent instance restarts (e.g., due to eviction or deployment)

4. **Technical Robustness**
   - Handles temperature-based behavior adjustments in LLM responses (via `sharedInfer` parameters)
   - Implements error recovery for tool execution and message handling
   - Maintains security best practices (GitHub token handling, log redaction)
   - Leverages Durable Objects for state persistence and concurrency handling (single thread per instance)
   - Provides comprehensive logging via `console.log` for debugging

## Implementation Details

The Solver uses several advanced techniques:

- **Inference**: Cloudflare Workers AI with Llama 4
- **State**: Persistent state through Durable Objects
- **TypeScript**: Strong typing for code quality and maintainability
- **WebSockets**: Real-time communication with clients
- **State Cloning**: Deep cloning to prevent reference issues
- **Context Recovery**: Rebuilding state from message history when needed
- **Security**: Proper token handling and sensitive data redaction

## Usage

When using the Solver agent through the issue chatbox:

1. Messages are appended to the agent's `messages` array as UIMessages
2. The agent processes these through its WebSocket handler
3. Responses are generated with appropriate context and tools

The agent maintains context across interactions and adapts behavior based on:
- Current issue state
- Project and team context
- Implementation progress
- Temperature settings
- Available tools
- Recent observations

### Interaction Guidelines

The Solver follows a methodical approach to issue resolution:

1. **Understanding Phase**
   - Analyzes issue requirements
   - Researches context and background
   - Plans implementation approach

2. **Implementation Phase**
   - Develops solution step-by-step
   - Tests changes thoroughly
   - Documents reasoning and decisions

3. **Context Recovery**
   - If state is lost, the agent can rebuild context from message history
   - Validates recovered state for consistency
   - Maintains continuity of operation

4. **Temperature Adaptation**
   - Lower temperature (<0.3): More precise and careful analysis
   - Higher temperature (>0.7): More creative problem-solving
   - Consistent validation regardless of temperature setting

## Tool Architecture and GitHub Token Handling

The Solver agent leverages a robust tool architecture that spans both base functionality and solver-specific capabilities. Tools are implemented using the Vercel AI SDK's `tool()` function, which provides type-safe parameter validation using Zod schemas.

### Tool Hierarchy

Tools are organized into two main categories:

1. **Base Agent Tools** (`packages/agents/src/common/tools/index.ts`)
   - Scheduling tools: `scheduleTask`, `listSystemSchedules`, `deleteSystemSchedule`
   - Repository context: `setRepositoryContext`
   - Utility tools: `getLocalTime`, `getWeatherInformation`

   These tools are available to all agent types and handle cross-cutting concerns that aren't specific to any particular agent use case.

2. **Solver-Specific Tools** (`packages/agents/src/agents/solver/tools.ts`)
   - Issue management: `getIssueDetails`, `updateIssueStatus`
   - Planning: `createImplementationPlan`

   These tools implement functionality specific to the Solver agent's role in managing issues and implementing solutions.

### GitHub Token Flow

The secure handling of GitHub tokens is a critical aspect of the Solver agent architecture. Here's how GitHub token authentication works:

1. **Token Origin**
   - The GitHub token originates from the user through the web interface
   - It's sent to the agent via a secure WebSocket connection when establishing a connection

2. **Token Storage**
   - The token is stored in the agent's state using the `setGithubToken` method
   - It's kept in the `githubToken` property of the `BaseAgentState` interface
   - This state is maintained by Cloudflare's Durable Object storage mechanism
   - The token is never persisted to disk or any external storage system
   - State is specific to the individual agent instance (one per issue)

3. **Token Usage in Tools**
   - When a GitHub API tool is invoked, it retrieves the token from the agent's state:
     ```typescript
     const agent = solverContext.getStore();
     const token = agent.state.githubToken;
     ```
   - The token is used in API requests with proper authorization headers:
     ```typescript
     const response = await fetch(url, {
       headers: {
         'Authorization': `Bearer ${token}`,
         'Accept': 'application/vnd.github.v3+json',
         'User-Agent': 'OpenAgents'
       }
     });
     ```
   - All token usage is scoped to the specific operations required by the tool

4. **Security Measures**
   - Tokens are redacted in all logs to prevent exposure:
     ```typescript
     const safeMessageForLogging = { ...parsedMessage };
     if (safeMessageForLogging.githubToken) {
       safeMessageForLogging.githubToken = "[REDACTED]";
     }
     ```
   - WebSocket connections use TLS encryption for secure transit
   - Tokens are never exposed to other agents or users
   - Token validation happens at the time of API requests
   - No token persistence beyond the agent's runtime state

5. **Token Lifecycle**
   - Tokens live only for the duration of the agent session
   - When an agent instance is stopped or evicted from memory, the token is discarded
   - New connections require re-authentication with a fresh token
   - No automatic token refresh is implemented; expired tokens must be replaced manually

The token flow ensures that GitHub credentials are securely handled while enabling the agent to perform authenticated operations against the GitHub API on behalf of the user.

## Development and Architecture Details

This section provides additional technical details for developers working with the Solver agent.

### OpenAgents Projects Ecosystem

The Solver agent is part of the broader OpenAgents Projects ecosystem, which includes:

- **Web Interface**: The primary user-facing application at `apps/website/`
- **Core Package**: Shared types and utilities at `packages/core/`
- **Agents Package**: This package, containing the implementation of all agents
- **Chat Server**: For handling more general chat interactions

Solver is one of several agent types, each designed for specific purposes:

1. **Solver Agent**: For analyzing and implementing solutions to project issues
2. **PR Reviewer Agent**: For code review (planned)
3. **Document Agent**: For knowledge management (planned)

These agents don't directly interact with each other, but share the same UI components, core functionality, and infrastructure.

### Durable Object Lifecycle

Solver agents are implemented as Cloudflare Durable Objects with these characteristics:

- **Lifetime**: Instances persist as long as they're actively used
- **Eviction**: After a period of inactivity (typically 24-72 hours), Cloudflare may evict the DO
- **State Persistence**: State stored in the DO survives until eviction
- **Context Loss**: Occurs primarily during code deployments or when instances are evicted and recreated

Context recovery is handled by:
1. Storing rich context in message history
2. Implementing logic to extract context from historical messages
3. Reconstructing state when context is detected as missing

In practice, context loss is relatively infrequent but can happen during deployments or after extended periods of inactivity.

### State Recovery Details

When context loss is detected (usually when key state properties are missing), the agent:

1. Searches the message history for context indicators like "issue" and "context"
2. Extracts structured information about the issue, project, and team
3. Rebuilds the state object with the extracted information
4. Validates the reconstructed state for consistency
5. Uses the recovered state for the current operation

This process happens automatically and is triggered by missing context in critical operations like inference or system prompt generation.

### Cloudflare Workers AI Configuration

The Solver agent uses Cloudflare Workers AI with these specifics:

- **Default Model**: `@cf/meta/llama-4-scout-17b-16e-instruct`
- **Context Window**: 32K tokens
- **Cost**: Free tier through Cloudflare Workers
- **Typical Latency**: 2-5 seconds for responses (varies by message complexity)
- **Temperature Control**: Set by the caller when needed, default 0.7
  - Can be dynamically adjusted per operation for different behavior characteristics

### Tool Extension

To add a new tool to the Solver agent:

1. Define the tool using the Vercel AI SDK's `tool()` function:
   ```typescript
   export const myNewTool = tool({
     description: "Description of what the tool does",
     parameters: z.object({
       param1: z.string().describe("Parameter description"),
       param2: z.number().optional().describe("Optional parameter")
     }),
     execute: async ({ param1, param2 }) => {
       // Implementation
       return { result: "Success", data: {...} };
     }
   });
   ```

2. Add the tool to the exported tools in either:
   - `packages/agents/src/common/tools/index.ts` for base tools
   - `packages/agents/src/agents/solver/tools.ts` for solver-specific tools

3. Reference the new tool in prompt generation if needed:
   ```typescript
   // In prompts.ts
   `You have access to the following tools:
   - existingTool: Description
   - myNewTool: Description of what the tool does`
   ```

Tools defined in `solver/tools.ts` can access the current Solver agent instance and its state using the `solverContext` AsyncLocalStorage:
```typescript
import { solverContext } from "./index";

// Inside the tool's execute function:
const agent = solverContext.getStore();
if (agent) {
  const token = agent.state.githubToken;
  // Access other state properties or agent methods...
}
```

### GitHub Token Scope Requirements

For the Solver agent to function properly with GitHub, the token provided by the user needs these scopes:

- `repo`: Full control of private repositories
  - `repo:status`: Access commit status
  - `repo_deployment`: Access deployment status
  - `public_repo`: Access public repositories
  - `repo:invite`: Access repository invitations

For issue-only operations (no code changes), a more limited scope is possible:
- `repo`: (Limited to issues, pull requests)
  - `read:packages`: Read packages
  - `read:user`: Read user information

The token is used for:
1. Fetching issue details from GitHub repositories
2. Updating issue statuses
3. Adding comments to issues
4. Accessing repository content for code context
5. (If configured) Making code changes via Pull Requests

### Development Environment

To develop and test the Solver agent locally:

1. **Setup Cloudflare Environment**:
   - Clone the repository
   - Install dependencies with `npm install`
   - Set up Cloudflare Wrangler with `npm i -g wrangler`
   - Login to Cloudflare with `wrangler login`

2. **Local Development**:
   - Start the development server with `wrangler dev`
   - Run the website with `npm run dev`
   - Connect to the local agent instance using the development tools

3. **Testing WebSocket Communication**:
   - Use the browser's developer tools to monitor WebSocket traffic
   - Develop against the mock agent in development mode before deploying

4. **Deployment**:
   - Publish to Cloudflare with `wrangler publish`
   - Note that deploying new code causes existing DO instances to restart, potentially losing in-memory state

### Monitoring and Debugging

Solver has extensive logging:

- **Console Logs**: Detailed debug information is sent to Cloudflare logs
- **State Snapshots**: Critical state is logged at key decision points
- **Error Tracking**: All errors are logged with stack traces
- **Message Tracing**: Agent messages include timestamps and request IDs

All GitHub tokens and sensitive information are redacted from logs automatically.

For local debugging, use:
- Browser developer tools to inspect WebSocket traffic
- Cloudflare dashboard for Worker logs
- `wrangler tail` for real-time log streaming

## Shared Inference System

The Solver agent utilizes a shared inference system implemented in the base `OpenAgent` class (`packages/agents/src/common/open-agent.ts`). This system provides a unified way for agents to access AI models.

### Current Implementation (Cloudflare Workers AI)

The `sharedInfer` method in `OpenAgent` currently uses the native Cloudflare Workers AI binding (`this.env.AI.run`) to interact with language models.

```typescript
// Simplified snippet from open-agent.ts (Current Implementation)
async sharedInfer(props: InferProps): Promise<InferResponse> {
  const { model = "@cf/meta/llama-4-scout-17b-16e-instruct", ... } = props;
  let systemPrompt = system || this.getSystemPrompt(); // Uses agent's prompt if none provided

  // Format messages...

  // Call Cloudflare Workers AI
  // @ts-expect-error Env type might be generic
  const result = await this.env.AI.run(model, {
    messages: formattedMessages,
    temperature, max_tokens, top_p
  });

  // Process and return response...
  const responseText = /* Logic to extract text from result */;
  return { /* standardized response */ };
}
```

### Key Features (Current)

1. **Model Access**: Provides access to Cloudflare's AI models available via the `env.AI` binding, defaulting to Llama 4 Scout.
2. **Dynamic System Prompts**: Automatically generates and includes the agent-specific system prompt (via `this.getSystemPrompt()`) if no `system` prompt is passed in the `InferProps`.
3. **Flexible Parameters**: Supports standard LLM parameters like `temperature`, `max_tokens`, and `top_p`.
4. **Context Awareness**: Incorporates agent state into the system prompt for contextual responses.
5. **Consistent Response Format**: Returns a standardized `InferResponse` object.
6. **Error Handling**: Includes try/catch blocks to handle inference errors.

### Usage in Solver Agent

The Solver agent leverages `sharedInfer` when processing `shared_infer` messages received via the WebSocket, typically triggered by user input in the chat interface (`solver-connector.tsx`). The frontend often passes the full message history and sometimes the system prompt explicitly to ensure context.

When the user sends a message through the chat interface, this triggers a message flow that:

1. Formats the conversation history and user message
2. Ensures context is available for the system prompt
3. Calls `sharedInfer` with appropriate parameters
4. Processes the response and adds it to the message history
5. Displays the response to the user

### Advanced Configuration

For specialized use cases, the inference system can be configured with:

- Different model selections (e.g., smaller models for faster responses)
- Custom temperature settings (lower for precise answers, higher for creative ones)
- Token limits to control response length
- Custom system prompts for specific behaviors

For more implementation details, see the full documentation in `packages/agents/docs/shared-inference-implementation.md`.

### Future Work: Vercel AI SDK Integration

A future refactoring is planned to migrate the shared inference system to use the Vercel AI SDK. This aims to provide greater flexibility, easier access to a wider range of models (e.g., via OpenRouter), and a more standardized provider interface.

# Vercel AI SDK Integration Plan

This section outlines the plan for refactoring the shared inference system to use the Vercel AI SDK, providing more flexibility and model options.

## Current Implementation

Currently, the shared inference system directly uses Cloudflare Workers AI through the environment binding:

```typescript
// Direct usage of Cloudflare Workers AI
const result = await this.env.AI.run(model, {
  messages: formattedMessages,
  temperature,
  max_tokens,
  top_p
});
```

## Proposed Refactoring

The refactoring will implement two key providers:

1. **OpenRouter Provider** for accessing multiple models
2. **Cloudflare Workers AI Provider** for continued access to Cloudflare models

### Files to Modify

1. **`/packages/agents/src/common/open-agent.ts`**
   - Replace direct Cloudflare Workers AI calls with Vercel AI SDK
   - Update the `sharedInfer` method to use provider abstraction

2. **`/packages/agents/package.json`**
   - Add dependencies:
     ```json
     {
       "dependencies": {
         "ai": "^2.2.0",
         "@openrouter/ai-sdk-provider": "^0.1.0",
         "workers-ai-provider": "^0.1.0"
       }
     }
     ```

### New Files to Create

1. **`/packages/agents/src/common/providers/index.ts`**
   - Create a provider factory module:
   ```typescript
   import { createOpenRouter } from '@openrouter/ai-sdk-provider';
   import { createWorkersAI } from 'workers-ai-provider';
   import { generateText, generateObject } from 'ai';
   import { modelMap } from './models';

   // Provider factory
   export function createProviders(env: any) {
     // OpenRouter provider
     const openrouter = createOpenRouter({
       apiKey: env.OPENROUTER_API_KEY || process.env.OPENROUTER_API_KEY
     });

     // Cloudflare Workers AI provider
     const workersai = createWorkersAI({
       binding: env.AI
     });

     return {
       openrouter,
       workersai,
       getProviderForModel(modelId: string) {
         const modelConfig = modelMap[modelId];
         if (!modelConfig) {
           throw new Error(`Unknown model ID: ${modelId}`);
         }

         const { provider, model } = modelConfig;
         return {
           provider: provider === 'openrouter' ? openrouter : workersai,
           modelId: model,
           config: modelConfig
         };
       },
       generateText,
       generateObject
     };
   }

   export type Providers = ReturnType<typeof createProviders>;
   ```

2. **`/packages/agents/src/common/providers/models.ts`**
   - Create model mappings:
   ```typescript
   export interface ModelConfig {
     provider: 'openrouter' | 'workersai';
     model: string;
     contextSize: number;
     defaultTemp: number;
     maxTokens: number;
   }

   export const modelMap: Record<string, ModelConfig> = {
     // Default Llama 4 from Cloudflare
     "@cf/meta/llama-4-scout-17b-16e-instruct": {
       provider: 'workersai',
       model: '@cf/meta/llama-4-scout-17b-16e-instruct',
       contextSize: 32000,
       defaultTemp: 0.7,
       maxTokens: 1024
     },

     // Claude models via OpenRouter
     "claude-3.5-sonnet": {
       provider: 'openrouter',
       model: 'anthropic/claude-3.5-sonnet',
       contextSize: 200000,
       defaultTemp: 0.7,
       maxTokens: 4096
     },

     "claude-3-opus": {
       provider: 'openrouter',
       model: 'anthropic/claude-3-opus',
       contextSize: 200000,
       defaultTemp: 0.7,
       maxTokens: 4096
     },

     // Other Cloudflare models
     "mistral-large": {
       provider: 'workersai',
       model: '@cf/mistral/mistral-large-2402',
       contextSize: 32000,
       defaultTemp: 0.7,
       maxTokens: 1024
     }
   };
   ```

### Refactored Implementation in `open-agent.ts`

Update the `sharedInfer` method in `/packages/agents/src/common/open-agent.ts`:

```typescript
import { generateId } from 'ai';
import { createProviders, type Providers } from './providers';
import type { InferProps, InferResponse } from './types';

export class OpenAgent<T extends BaseAgentState> extends Agent<Env, T> {
  // Add providers property
  private providers: Providers;

  constructor(ctx: any, env: Env) {
    super(ctx, env);
    // Initialize providers in constructor
    this.providers = createProviders(env);
  }

  // Other methods...

  /**
   * Shared inference method for all agents
   * Uses Vercel AI SDK to access models through OpenRouter and Cloudflare Workers AI
   */
  async sharedInfer(props: InferProps): Promise<InferResponse> {
    try {
      // Extract properties with defaults
      const {
        model = "@cf/meta/llama-4-scout-17b-16e-instruct",
        messages,
        system,
        temperature = 0.7,
        max_tokens = 1024,
        top_p = 0.95
      } = props;

      // Get the appropriate provider for this model
      const { provider, modelId, config } = this.providers.getProviderForModel(model);

      // Format messages for the AI model
      const formattedMessages = [];

      // Add system message
      const systemPrompt = system || this.getSystemPrompt();
      formattedMessages.push({ role: "system", content: systemPrompt });

      // Add conversation messages
      messages.forEach(msg => {
        formattedMessages.push({
          role: msg.role,
          content: msg.content
        });
      });

      // Use Vercel AI SDK's generateText
      const { text } = await this.providers.generateText({
        model: provider.chatModel(modelId),
        messages: formattedMessages,
        temperature,
        maxTokens: max_tokens,
        topP: top_p
      });

      // Return standardized response
      return {
        id: generateId(),
        content: text,
        role: "assistant",
        timestamp: new Date().toISOString(),
        model: model
      };
    } catch (error) {
      console.error("Error during AI inference:", error);

      // Return a standardized error response
      return {
        id: generateId(),
        content: `Error generating response: ${error instanceof Error ? error.message : String(error)}`,
        role: "assistant",
        timestamp: new Date().toISOString(),
        model: model
      };
    }
  }
}
```

## Implementation Benefits

This refactoring will provide:
- Access to hundreds of AI models through OpenRouter
- Continued access to Cloudflare Workers AI models
- Standardized interface across all providers
- More flexible configuration options
- Improved error handling and fallback capabilities
- Easier addition of new models and providers in the future

## Implementation Timeline

1. **Phase 1: Setup and Library Integration**
   - Add required packages to package.json
   - Create provider models and mappings
   - Implement provider factory

2. **Phase 2: OpenAgent Refactoring**
   - Update the OpenAgent constructor to initialize providers
   - Refactor sharedInfer method to use Vercel AI SDK
   - Add fallback mechanisms for backward compatibility

3. **Phase 3: Testing and Validation**
   - Create tests for different providers and models
   - Validate behavior matches current implementation
   - Test error handling and edge cases

4. **Phase 4: Frontend Integration**
   - Update any frontend code that depends on specific model IDs
   - Add UI components for model selection if needed

## Architectural Considerations: Exploring the Effect Framework

While the current Solver Agent implementation effectively utilizes Cloudflare Durable Objects, WebSockets, and standard TypeScript `async/await` patterns, we are exploring the potential benefits of incrementally adopting the [Effect framework](https://effect.website/) for future development and refactoring.

**Why Consider Effect?**

The Solver Agent deals with complex asynchronous operations (LLM inference, tool execution, external API calls), potential failures across these operations, state management within a distributed context (Durable Objects), and managing dependencies like GitHub tokens and API clients. Effect offers a functional programming paradigm within TypeScript designed to address these challenges more robustly and explicitly:

1.  **Explicit Error Handling:** Effect tracks potential errors (`E` in `Effect<A, E, R>`) in the type system, moving beyond `try/catch` to make failure paths explicit and force developers to handle them, potentially increasing reliability.
2.  **Structured Concurrency:** Provides powerful, composable tools for managing concurrent operations (like multiple tool calls or background tasks) safely, including built-in interruption handling.
3.  **Dependency Management:** Formalizes the management of dependencies (`R` in `Effect<A, E, R>`) like API clients or configuration using `Context` and `Layer`, which can improve testability and modularity.
4.  **Rich Ecosystem:** Offers built-in, composable solutions for common patterns like retries (`Schedule`), resource management (`Scope`), caching (`Cache`), and more, potentially reducing boilerplate code.

**Current Status & Plan**

We believe adopting Effect could lead to a more reliable, maintainable, and testable Solver Agent over time. However, it represents a significant paradigm shift requiring careful consideration and learning.

An **incremental adoption plan** is being evaluated, starting with potentially refactoring the internal logic of specific tools before considering broader changes to core agent logic or dependency management.

**For a detailed analysis of Effect's relevance to this project and the proposed incremental adoption strategy, please refer to the dedicated document:**

[**Effect Framework Considerations for Solver Agent (`packages/agents/docs/solver-effect-considerations.md`)**](/packages/agents/docs/SOLVER-README.md)

## Resources

- [Vercel AI SDK Documentation](https://sdk.vercel.ai/docs)
- [OpenRouter Provider Documentation](https://sdk.vercel.ai/providers/community-providers/openrouter)
- [Workers AI Provider Documentation](https://sdk.vercel.ai/providers/community-providers/cloudflare-workers-ai)

## Additional Resources

- [Vercel AI SDK Documentation](https://sdk.vercel.ai/docs)
- [OpenRouter Provider Documentation](https://sdk.vercel.ai/providers/community-providers/openrouter)
- [Workers AI Provider Documentation](https://sdk.vercel.ai/providers/community-providers/cloudflare-workers-ai)
- [Cloudflare Workers AI Documentation](https://developers.cloudflare.com/workers-ai/)
- [Llama 4 Model Information](https://ai.meta.com/llama/)
